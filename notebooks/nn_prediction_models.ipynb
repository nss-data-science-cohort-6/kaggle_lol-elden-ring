{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle League of Legends competition - Neural Network Models\n",
    "\n",
    "## Team: Elden Ring"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://eldenring.wiki.fextralife.com/file/Elden-Ring/mirel_pastor_of_vow.jpg\" alt=\"PRAISE DOG\" style=\"width:806px;height:600px;\"/>\n",
    "\n",
    "#### PRAISE THE DOG!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Win at League of Legends?\n",
    "\n",
    "### Uninstall LoL and [install Dota 2](https://store.steampowered.com/app/570/Dota_2/), EZ. (just kidding, both games are great. Volvo pls gib patch.)\n",
    "\n",
    "<img src = \"https://static.wikia.nocookie.net/dota2_gamepedia/images/7/78/Keyart_phoenix.jpg/revision/latest/\" alt=\"SKREE CAW CAW IM A BIRD\" style=\"width:800px;height:497px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_original = pd.read_csv('../data/participants_train.csv')\n",
    "X_test_original = pd.read_csv('../data/participants_test.csv')\n",
    "y_train_original = pd.read_csv('../data/train_winners.csv')\n",
    "\n",
    "champion_mastery = pd.read_csv('../data/champion_mastery.csv')\n",
    "champion = pd.read_json('../data/champion.json')\n",
    "\n",
    "team_positions = pd.read_csv('../data/teamPositions.csv')\n",
    "\n",
    "train_last_frame_values = pd.read_csv('../data/train_last_frame_values.csv')\n",
    "test_last_frame_values = pd.read_csv('../data/test_last_frame_values.csv')\n",
    "\n",
    "training_events = pd.read_csv('../data/training_events.csv')\n",
    "testing_events = pd.read_csv('../data/testing_events.csv')\n",
    "\n",
    "submission = pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that converts values to negative (for the second team, teamId 200)\n",
    "# it leaves the first team values, teamId intact\n",
    "\n",
    "def convert_team_values(df, col_names):\n",
    "    \n",
    "    \n",
    "    for col in col_names:\n",
    "        df[col] = np.where(df['teamId'] == 200,\n",
    "                            -1* df[col],\n",
    "                                df[col])\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation needed on the dataframes (see the file prediction_models)\n",
    "vars = ['wards_placed', 'wards_killed', 'turretplates_destroyed', 'elite_monsters_killed']\n",
    "\n",
    "convert_team_values(training_events, vars)\n",
    "convert_team_values(testing_events, vars)\n",
    "\n",
    "training_events = training_events.groupby('matchId')[vars].sum()\n",
    "testing_events = testing_events.groupby('matchId')[vars].sum()\n",
    "\n",
    "champion_data = pd.json_normalize(champion['data'])\n",
    "champion_data['key'] = champion_data['key'].astype(int)\n",
    "\n",
    "champion_types= champion_data.explode('tags').pivot_table(values='id', index='key', columns='tags', aggfunc='count').fillna(0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be used later to measure the accuracy!\n",
    "kfold = KFold(n_splits = 10, shuffle = True, random_state = 42)\n",
    "\n",
    "# this is to extract the column that is needed for training\n",
    "y_train = y_train_original['winner']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Neural Networks now to try and improve the logreg predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['summonerLevel', 'championLevel','championPoints', \n",
    "             'Assassin', 'Fighter', 'Mage', 'Marksman', 'Support', 'Tank',\n",
    "            'info.attack', 'info.defense', 'info.magic', 'info.difficulty',\n",
    "            'stats.hpregenperlevel',\t'stats.mpregen', 'stats.mpregenperlevel',\t'stats.crit',\t'stats.critperlevel',\n",
    "            'stats.attackdamage', 'stats.attackdamageperlevel', 'stats.attackspeedperlevel',\t'stats.attackspeed',\n",
    "            'final_gold', 'final_xp', 'final_abilityhaste', 'final_abilitypower', 'final_armor', 'final_armorpen',\n",
    "            'final_armorpenpercent', 'final_atkdmg', 'final_bns_armorpenpercent', 'final_bns_magicpenpercent', 'final_ccreduction',\n",
    "            'final_cdreduction', 'final_remaining_health', 'final_health', 'final_healthrgn', 'final_lifesteal', 'final_mppen',\n",
    "            'final_mgpenpercent', 'final_mgres', 'final_ms', 'final_omnivamp', 'final_physicalvamp', 'final_power', 'final_powermax',\n",
    "            'final_powerregen', 'final_spellvamp', 'final_currentgold', 'final_magicdmgdone', 'final_magicdmgdonetochamps', 'final_magicdmgtaken',\n",
    "            'final_physdmgdone', 'final_physdmgdonetochamps', 'final_physdmgtaken', 'final_dmgdone', 'final_dmgdonetochamps', 'final_dmgtaken', \n",
    "            'final_truedmgdone', 'final_truedmgdonetochamps', 'final_truedmgtaken', 'final_goldpersec', 'final_jungleminionskilled', 'final_lvl',\n",
    "            'final_minionskilled', 'final_jungleminionskilled', 'final_jungleminionskilled', 'final_jungleminionskilled', 'final_enemycontrolled'                  \n",
    "             ]\n",
    "\n",
    "X_train = pd.merge(X_train_original, team_positions, how='inner', on=['matchId', 'participantId'])\n",
    "X_train = pd.merge(X_train, champion_data, how='inner', left_on='championId', right_on='key')\n",
    "X_train = pd.merge(X_train, champion_types, how='inner', left_on='championId', right_on='key')\n",
    "X_train = pd.merge(X_train, train_last_frame_values, how='inner', on=['matchId', 'participantId'])\n",
    "X_train = pd.merge(X_train, champion_mastery, how='left', on=['summonerId', 'championId']).fillna(0)\n",
    "\n",
    "X_train = X_train.sort_values(['matchId', 'participantId'], ascending = [True, True]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "convert_team_values(X_train, variables)\n",
    "\n",
    "X_train_perlane = X_train.groupby(['matchId', 'teamPosition'])[['final_gold']].sum().pivot_table(values='final_gold', index='matchId', columns='teamPosition').reset_index().drop(columns=0)\n",
    "\n",
    "for lane in ['BOTTOM', 'JUNGLE', 'MIDDLE', 'TOP', 'UTILITY']:\n",
    "  X_train_perlane[f'{lane}'] = np.where(X_train_perlane[f'{lane}'] >= 0, 1, -1)\n",
    "\n",
    "X_train = (\n",
    "    X_train\n",
    "    .groupby(['matchId'])[variables]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "X_train = pd.merge(X_train, X_train_perlane, how='inner', on='matchId').reset_index(drop = True)\n",
    "X_train = pd.merge(X_train, training_events, how='inner', on='matchId').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matchId</th>\n",
       "      <th>summonerLevel</th>\n",
       "      <th>championLevel</th>\n",
       "      <th>championPoints</th>\n",
       "      <th>Assassin</th>\n",
       "      <th>Fighter</th>\n",
       "      <th>Mage</th>\n",
       "      <th>Marksman</th>\n",
       "      <th>Support</th>\n",
       "      <th>Tank</th>\n",
       "      <th>...</th>\n",
       "      <th>final_enemycontrolled</th>\n",
       "      <th>BOTTOM</th>\n",
       "      <th>JUNGLE</th>\n",
       "      <th>MIDDLE</th>\n",
       "      <th>TOP</th>\n",
       "      <th>UTILITY</th>\n",
       "      <th>wards_placed</th>\n",
       "      <th>wards_killed</th>\n",
       "      <th>turretplates_destroyed</th>\n",
       "      <th>elite_monsters_killed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>682</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-605428.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>67664</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-64</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>628</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1356027.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-61783</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1049</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-273911.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-132630</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-1027</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-287667.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-39616</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1612</td>\n",
       "      <td>7.0</td>\n",
       "      <td>503668.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16629</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   matchId  summonerLevel  championLevel  championPoints  Assassin  Fighter  \\\n",
       "0        0            682            0.0       -605428.0      -1.0     -1.0   \n",
       "1        1            628            8.0       1356027.0       3.0      0.0   \n",
       "2        2           1049            1.0       -273911.0      -2.0      1.0   \n",
       "3        3          -1027           -3.0       -287667.0       1.0      1.0   \n",
       "4        4           1612            7.0        503668.0       0.0      0.0   \n",
       "\n",
       "   Mage  Marksman  Support  Tank  ...  final_enemycontrolled  BOTTOM  JUNGLE  \\\n",
       "0   2.0       0.0      0.0  -2.0  ...                  67664      -1      -1   \n",
       "1   0.0       0.0     -1.0  -1.0  ...                 -61783       1       1   \n",
       "2   0.0       0.0      0.0   0.0  ...                -132630       1      -1   \n",
       "3   1.0      -1.0     -1.0   0.0  ...                 -39616       1       1   \n",
       "4   2.0      -1.0      0.0  -1.0  ...                  16629      -1       1   \n",
       "\n",
       "   MIDDLE  TOP  UTILITY  wards_placed  wards_killed  turretplates_destroyed  \\\n",
       "0       1   -1       -1           -64            -1                      -2   \n",
       "1       1   -1        1             1             0                      -1   \n",
       "2      -1   -1       -1             4             1                      -2   \n",
       "3      -1   -1        1             4             0                       2   \n",
       "4      -1    1        1             4             0                      -1   \n",
       "\n",
       "   elite_monsters_killed  \n",
       "0                     -1  \n",
       "1                     -1  \n",
       "2                      1  \n",
       "3                      0  \n",
       "4                     -1  \n",
       "\n",
       "[5 rows x 79 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_neuralnetwork = Pipeline(\n",
    "    steps = [\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('nn', MLPClassifier(verbose = True,\n",
    "                             hidden_layer_sizes = (100, 100, 100, 100),\n",
    "                             activation = 'tanh',\n",
    "                             max_iter = 10000,\n",
    "                             alpha=0.05))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.66849572\n",
      "Iteration 2, loss = 0.62492647\n",
      "Iteration 3, loss = 0.62073670\n",
      "Iteration 4, loss = 0.60617163\n",
      "Iteration 5, loss = 0.61613392\n",
      "Iteration 6, loss = 0.60443803\n",
      "Iteration 7, loss = 0.60497042\n",
      "Iteration 8, loss = 0.59647355\n",
      "Iteration 9, loss = 0.59316923\n",
      "Iteration 10, loss = 0.59304150\n",
      "Iteration 11, loss = 0.59657358\n",
      "Iteration 12, loss = 0.59080561\n",
      "Iteration 13, loss = 0.59035417\n",
      "Iteration 14, loss = 0.58673575\n",
      "Iteration 15, loss = 0.59459825\n",
      "Iteration 16, loss = 0.58465865\n",
      "Iteration 17, loss = 0.58895721\n",
      "Iteration 18, loss = 0.58755091\n",
      "Iteration 19, loss = 0.58328436\n",
      "Iteration 20, loss = 0.57939311\n",
      "Iteration 21, loss = 0.58285486\n",
      "Iteration 22, loss = 0.58145488\n",
      "Iteration 23, loss = 0.58454298\n",
      "Iteration 24, loss = 0.57697932\n",
      "Iteration 25, loss = 0.57648520\n",
      "Iteration 26, loss = 0.57901413\n",
      "Iteration 27, loss = 0.57501599\n",
      "Iteration 28, loss = 0.57388428\n",
      "Iteration 29, loss = 0.57613449\n",
      "Iteration 30, loss = 0.57234724\n",
      "Iteration 31, loss = 0.57186986\n",
      "Iteration 32, loss = 0.58050383\n",
      "Iteration 33, loss = 0.57550793\n",
      "Iteration 34, loss = 0.56966141\n",
      "Iteration 35, loss = 0.56882603\n",
      "Iteration 36, loss = 0.56733082\n",
      "Iteration 37, loss = 0.57411954\n",
      "Iteration 38, loss = 0.56840232\n",
      "Iteration 39, loss = 0.56783649\n",
      "Iteration 40, loss = 0.57131622\n",
      "Iteration 41, loss = 0.57453707\n",
      "Iteration 42, loss = 0.57462179\n",
      "Iteration 43, loss = 0.56621848\n",
      "Iteration 44, loss = 0.56846693\n",
      "Iteration 45, loss = 0.56540597\n",
      "Iteration 46, loss = 0.57146096\n",
      "Iteration 47, loss = 0.56517774\n",
      "Iteration 48, loss = 0.56401185\n",
      "Iteration 49, loss = 0.56484509\n",
      "Iteration 50, loss = 0.56781441\n",
      "Iteration 51, loss = 0.56488390\n",
      "Iteration 52, loss = 0.56231408\n",
      "Iteration 53, loss = 0.56288628\n",
      "Iteration 54, loss = 0.56609445\n",
      "Iteration 55, loss = 0.56234969\n",
      "Iteration 56, loss = 0.56159092\n",
      "Iteration 57, loss = 0.56047736\n",
      "Iteration 58, loss = 0.56036538\n",
      "Iteration 59, loss = 0.56521083\n",
      "Iteration 60, loss = 0.56035620\n",
      "Iteration 61, loss = 0.56215695\n",
      "Iteration 62, loss = 0.55848766\n",
      "Iteration 63, loss = 0.56269287\n",
      "Iteration 64, loss = 0.56372778\n",
      "Iteration 65, loss = 0.55809719\n",
      "Iteration 66, loss = 0.56400730\n",
      "Iteration 67, loss = 0.55795094\n",
      "Iteration 68, loss = 0.55653989\n",
      "Iteration 69, loss = 0.56066231\n",
      "Iteration 70, loss = 0.56171358\n",
      "Iteration 71, loss = 0.55893512\n",
      "Iteration 72, loss = 0.55949949\n",
      "Iteration 73, loss = 0.55949651\n",
      "Iteration 74, loss = 0.56239187\n",
      "Iteration 75, loss = 0.56527048\n",
      "Iteration 76, loss = 0.55607239\n",
      "Iteration 77, loss = 0.55917004\n",
      "Iteration 78, loss = 0.55456729\n",
      "Iteration 79, loss = 0.55172986\n",
      "Iteration 80, loss = 0.55718587\n",
      "Iteration 81, loss = 0.55429627\n",
      "Iteration 82, loss = 0.55514032\n",
      "Iteration 83, loss = 0.55646821\n",
      "Iteration 84, loss = 0.55998991\n",
      "Iteration 85, loss = 0.55842822\n",
      "Iteration 86, loss = 0.55491300\n",
      "Iteration 87, loss = 0.56033969\n",
      "Iteration 88, loss = 0.55907703\n",
      "Iteration 89, loss = 0.55146856\n",
      "Iteration 90, loss = 0.55126558\n",
      "Iteration 91, loss = 0.55119967\n",
      "Iteration 92, loss = 0.55353205\n",
      "Iteration 93, loss = 0.55184282\n",
      "Iteration 94, loss = 0.55746230\n",
      "Iteration 95, loss = 0.55564990\n",
      "Iteration 96, loss = 0.55317478\n",
      "Iteration 97, loss = 0.55531401\n",
      "Iteration 98, loss = 0.55559724\n",
      "Iteration 99, loss = 0.55458600\n",
      "Iteration 100, loss = 0.55165511\n",
      "Iteration 101, loss = 0.54978957\n",
      "Iteration 102, loss = 0.55157412\n",
      "Iteration 103, loss = 0.55009012\n",
      "Iteration 104, loss = 0.54986113\n",
      "Iteration 105, loss = 0.55242629\n",
      "Iteration 106, loss = 0.55378078\n",
      "Iteration 107, loss = 0.54939382\n",
      "Iteration 108, loss = 0.54909448\n",
      "Iteration 109, loss = 0.54940812\n",
      "Iteration 110, loss = 0.54753460\n",
      "Iteration 111, loss = 0.54794408\n",
      "Iteration 112, loss = 0.55027745\n",
      "Iteration 113, loss = 0.54846959\n",
      "Iteration 114, loss = 0.55316385\n",
      "Iteration 115, loss = 0.55192590\n",
      "Iteration 116, loss = 0.55384131\n",
      "Iteration 117, loss = 0.55086159\n",
      "Iteration 118, loss = 0.54696546\n",
      "Iteration 119, loss = 0.54718677\n",
      "Iteration 120, loss = 0.54825068\n",
      "Iteration 121, loss = 0.54785914\n",
      "Iteration 122, loss = 0.54696617\n",
      "Iteration 123, loss = 0.54962020\n",
      "Iteration 124, loss = 0.54676841\n",
      "Iteration 125, loss = 0.54919926\n",
      "Iteration 126, loss = 0.54890879\n",
      "Iteration 127, loss = 0.54718753\n",
      "Iteration 128, loss = 0.54670306\n",
      "Iteration 129, loss = 0.54621910\n",
      "Iteration 130, loss = 0.54619053\n",
      "Iteration 131, loss = 0.54630104\n",
      "Iteration 132, loss = 0.54779389\n",
      "Iteration 133, loss = 0.54738922\n",
      "Iteration 134, loss = 0.54528649\n",
      "Iteration 135, loss = 0.54702849\n",
      "Iteration 136, loss = 0.54867643\n",
      "Iteration 137, loss = 0.54421719\n",
      "Iteration 138, loss = 0.54635198\n",
      "Iteration 139, loss = 0.54662326\n",
      "Iteration 140, loss = 0.54643876\n",
      "Iteration 141, loss = 0.54391960\n",
      "Iteration 142, loss = 0.54638628\n",
      "Iteration 143, loss = 0.54834621\n",
      "Iteration 144, loss = 0.54468620\n",
      "Iteration 145, loss = 0.55091636\n",
      "Iteration 146, loss = 0.54505587\n",
      "Iteration 147, loss = 0.54373977\n",
      "Iteration 148, loss = 0.54863402\n",
      "Iteration 149, loss = 0.55792654\n",
      "Iteration 150, loss = 0.54645934\n",
      "Iteration 151, loss = 0.54390911\n",
      "Iteration 152, loss = 0.54643530\n",
      "Iteration 153, loss = 0.54608328\n",
      "Iteration 154, loss = 0.54337402\n",
      "Iteration 155, loss = 0.54419662\n",
      "Iteration 156, loss = 0.54351510\n",
      "Iteration 157, loss = 0.54226613\n",
      "Iteration 158, loss = 0.54783903\n",
      "Iteration 159, loss = 0.54211806\n",
      "Iteration 160, loss = 0.54395117\n",
      "Iteration 161, loss = 0.54436328\n",
      "Iteration 162, loss = 0.54364394\n",
      "Iteration 163, loss = 0.54726762\n",
      "Iteration 164, loss = 0.54387386\n",
      "Iteration 165, loss = 0.55268561\n",
      "Iteration 166, loss = 0.54458674\n",
      "Iteration 167, loss = 0.54352766\n",
      "Iteration 168, loss = 0.54427319\n",
      "Iteration 169, loss = 0.54160831\n",
      "Iteration 170, loss = 0.54699710\n",
      "Iteration 171, loss = 0.54486706\n",
      "Iteration 172, loss = 0.54416180\n",
      "Iteration 173, loss = 0.54428398\n",
      "Iteration 174, loss = 0.54131305\n",
      "Iteration 175, loss = 0.54193954\n",
      "Iteration 176, loss = 0.54717634\n",
      "Iteration 177, loss = 0.54435203\n",
      "Iteration 178, loss = 0.54131048\n",
      "Iteration 179, loss = 0.54032829\n",
      "Iteration 180, loss = 0.54922128\n",
      "Iteration 181, loss = 0.54702055\n",
      "Iteration 182, loss = 0.54387534\n",
      "Iteration 183, loss = 0.54655040\n",
      "Iteration 184, loss = 0.53935307\n",
      "Iteration 185, loss = 0.54498905\n",
      "Iteration 186, loss = 0.53959648\n",
      "Iteration 187, loss = 0.54285821\n",
      "Iteration 188, loss = 0.54728792\n",
      "Iteration 189, loss = 0.54398654\n",
      "Iteration 190, loss = 0.54352269\n",
      "Iteration 191, loss = 0.54292734\n",
      "Iteration 192, loss = 0.54247899\n",
      "Iteration 193, loss = 0.54319002\n",
      "Iteration 194, loss = 0.54308919\n",
      "Iteration 195, loss = 0.54085455\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-13 {color: black;background-color: white;}#sk-container-id-13 pre{padding: 0;}#sk-container-id-13 div.sk-toggleable {background-color: white;}#sk-container-id-13 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-13 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-13 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-13 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-13 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-13 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-13 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-13 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-13 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-13 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-13 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-13 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-13 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-13 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-13 div.sk-item {position: relative;z-index: 1;}#sk-container-id-13 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-13 div.sk-item::before, #sk-container-id-13 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-13 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-13 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-13 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-13 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-13 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-13 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-13 div.sk-label-container {text-align: center;}#sk-container-id-13 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-13 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-13\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler()),\n",
       "                (&#x27;nn&#x27;,\n",
       "                 MLPClassifier(activation=&#x27;tanh&#x27;, alpha=0.05,\n",
       "                               hidden_layer_sizes=(100, 100, 100, 100),\n",
       "                               max_iter=10000, verbose=True))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-37\" type=\"checkbox\" ><label for=\"sk-estimator-id-37\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler()),\n",
       "                (&#x27;nn&#x27;,\n",
       "                 MLPClassifier(activation=&#x27;tanh&#x27;, alpha=0.05,\n",
       "                               hidden_layer_sizes=(100, 100, 100, 100),\n",
       "                               max_iter=10000, verbose=True))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-38\" type=\"checkbox\" ><label for=\"sk-estimator-id-38\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-39\" type=\"checkbox\" ><label for=\"sk-estimator-id-39\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, alpha=0.05,\n",
       "              hidden_layer_sizes=(100, 100, 100, 100), max_iter=10000,\n",
       "              verbose=True)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('scaler', MinMaxScaler()),\n",
       "                ('nn',\n",
       "                 MLPClassifier(activation='tanh', alpha=0.05,\n",
       "                               hidden_layer_sizes=(100, 100, 100, 100),\n",
       "                               max_iter=10000, verbose=True))])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_neuralnetwork.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.717625"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(\n",
    "    y_true = y_train,\n",
    "    y_pred = pipeline_neuralnetwork.predict(X_train)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.65800709\n",
      "Iteration 2, loss = 0.61456134\n",
      "Iteration 3, loss = 0.61665560\n",
      "Iteration 4, loss = 0.60812177\n",
      "Iteration 5, loss = 0.60439405\n",
      "Iteration 6, loss = 0.60015250\n",
      "Iteration 7, loss = 0.59699050\n",
      "Iteration 8, loss = 0.59816644\n",
      "Iteration 9, loss = 0.59363554\n",
      "Iteration 10, loss = 0.58751001\n",
      "Iteration 11, loss = 0.59588893\n",
      "Iteration 12, loss = 0.59765861\n",
      "Iteration 13, loss = 0.58942774\n",
      "Iteration 14, loss = 0.59035947\n",
      "Iteration 15, loss = 0.58772058\n",
      "Iteration 16, loss = 0.58151265\n",
      "Iteration 17, loss = 0.59271009\n",
      "Iteration 18, loss = 0.58689395\n",
      "Iteration 19, loss = 0.57984969\n",
      "Iteration 20, loss = 0.58531268\n",
      "Iteration 21, loss = 0.58286370\n",
      "Iteration 22, loss = 0.58269038\n",
      "Iteration 23, loss = 0.57683059\n",
      "Iteration 24, loss = 0.57704025\n",
      "Iteration 25, loss = 0.57622863\n",
      "Iteration 26, loss = 0.57861961\n",
      "Iteration 27, loss = 0.57732753\n",
      "Iteration 28, loss = 0.57530933\n",
      "Iteration 29, loss = 0.57812743\n",
      "Iteration 30, loss = 0.57262509\n",
      "Iteration 31, loss = 0.57406335\n",
      "Iteration 32, loss = 0.57473141\n",
      "Iteration 33, loss = 0.57571333\n",
      "Iteration 34, loss = 0.57031204\n",
      "Iteration 35, loss = 0.57510292\n",
      "Iteration 36, loss = 0.56748979\n",
      "Iteration 37, loss = 0.56829831\n",
      "Iteration 38, loss = 0.56711376\n",
      "Iteration 39, loss = 0.57278861\n",
      "Iteration 40, loss = 0.56677555\n",
      "Iteration 41, loss = 0.56545113\n",
      "Iteration 42, loss = 0.56511726\n",
      "Iteration 43, loss = 0.57070075\n",
      "Iteration 44, loss = 0.56827776\n",
      "Iteration 45, loss = 0.56826268\n",
      "Iteration 46, loss = 0.56422448\n",
      "Iteration 47, loss = 0.56908553\n",
      "Iteration 48, loss = 0.56590720\n",
      "Iteration 49, loss = 0.56443648\n",
      "Iteration 50, loss = 0.56981335\n",
      "Iteration 51, loss = 0.56828048\n",
      "Iteration 52, loss = 0.56247558\n",
      "Iteration 53, loss = 0.56731280\n",
      "Iteration 54, loss = 0.56278327\n",
      "Iteration 55, loss = 0.56294080\n",
      "Iteration 56, loss = 0.56476728\n",
      "Iteration 57, loss = 0.56192884\n",
      "Iteration 58, loss = 0.56116495\n",
      "Iteration 59, loss = 0.55883819\n",
      "Iteration 60, loss = 0.55931495\n",
      "Iteration 61, loss = 0.55955910\n",
      "Iteration 62, loss = 0.55867397\n",
      "Iteration 63, loss = 0.55979258\n",
      "Iteration 64, loss = 0.56000084\n",
      "Iteration 65, loss = 0.55904504\n",
      "Iteration 66, loss = 0.55803000\n",
      "Iteration 67, loss = 0.55887144\n",
      "Iteration 68, loss = 0.55638190\n",
      "Iteration 69, loss = 0.56664164\n",
      "Iteration 70, loss = 0.55692401\n",
      "Iteration 71, loss = 0.55863541\n",
      "Iteration 72, loss = 0.56102239\n",
      "Iteration 73, loss = 0.55591865\n",
      "Iteration 74, loss = 0.55687414\n",
      "Iteration 75, loss = 0.55379768\n",
      "Iteration 76, loss = 0.55467706\n",
      "Iteration 77, loss = 0.55460146\n",
      "Iteration 78, loss = 0.55478195\n",
      "Iteration 79, loss = 0.55952999\n",
      "Iteration 80, loss = 0.55483610\n",
      "Iteration 81, loss = 0.55421154\n",
      "Iteration 82, loss = 0.55495895\n",
      "Iteration 83, loss = 0.55686373\n",
      "Iteration 84, loss = 0.55125468\n",
      "Iteration 85, loss = 0.55386824\n",
      "Iteration 86, loss = 0.56019222\n",
      "Iteration 87, loss = 0.55336476\n",
      "Iteration 88, loss = 0.55148840\n",
      "Iteration 89, loss = 0.55182080\n",
      "Iteration 90, loss = 0.55137815\n",
      "Iteration 91, loss = 0.54958002\n",
      "Iteration 92, loss = 0.55331882\n",
      "Iteration 93, loss = 0.55445725\n",
      "Iteration 94, loss = 0.55263791\n",
      "Iteration 95, loss = 0.55351660\n",
      "Iteration 96, loss = 0.55061028\n",
      "Iteration 97, loss = 0.55470916\n",
      "Iteration 98, loss = 0.55335576\n",
      "Iteration 99, loss = 0.55006981\n",
      "Iteration 100, loss = 0.55336116\n",
      "Iteration 101, loss = 0.55011931\n",
      "Iteration 102, loss = 0.55079320\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66252392\n",
      "Iteration 2, loss = 0.61787714\n",
      "Iteration 3, loss = 0.61776136\n",
      "Iteration 4, loss = 0.60392944\n",
      "Iteration 5, loss = 0.60083081\n",
      "Iteration 6, loss = 0.59682924\n",
      "Iteration 7, loss = 0.59544790\n",
      "Iteration 8, loss = 0.59219677\n",
      "Iteration 9, loss = 0.59734860\n",
      "Iteration 10, loss = 0.58906483\n",
      "Iteration 11, loss = 0.59060548\n",
      "Iteration 12, loss = 0.58939930\n",
      "Iteration 13, loss = 0.58311591\n",
      "Iteration 14, loss = 0.58440637\n",
      "Iteration 15, loss = 0.58208968\n",
      "Iteration 16, loss = 0.58094987\n",
      "Iteration 17, loss = 0.59255190\n",
      "Iteration 18, loss = 0.58131829\n",
      "Iteration 19, loss = 0.57850456\n",
      "Iteration 20, loss = 0.57836738\n",
      "Iteration 21, loss = 0.57690638\n",
      "Iteration 22, loss = 0.58059710\n",
      "Iteration 23, loss = 0.57583589\n",
      "Iteration 24, loss = 0.57595200\n",
      "Iteration 25, loss = 0.57441902\n",
      "Iteration 26, loss = 0.57164675\n",
      "Iteration 27, loss = 0.57261459\n",
      "Iteration 28, loss = 0.57225436\n",
      "Iteration 29, loss = 0.57159398\n",
      "Iteration 30, loss = 0.57356348\n",
      "Iteration 31, loss = 0.57373591\n",
      "Iteration 32, loss = 0.57162727\n",
      "Iteration 33, loss = 0.56944550\n",
      "Iteration 34, loss = 0.57945761\n",
      "Iteration 35, loss = 0.56837514\n",
      "Iteration 36, loss = 0.57025369\n",
      "Iteration 37, loss = 0.56729376\n",
      "Iteration 38, loss = 0.56621529\n",
      "Iteration 39, loss = 0.56559175\n",
      "Iteration 40, loss = 0.57079743\n",
      "Iteration 41, loss = 0.56930014\n",
      "Iteration 42, loss = 0.57229101\n",
      "Iteration 43, loss = 0.57832442\n",
      "Iteration 44, loss = 0.56635194\n",
      "Iteration 45, loss = 0.56849426\n",
      "Iteration 46, loss = 0.56391945\n",
      "Iteration 47, loss = 0.56361147\n",
      "Iteration 48, loss = 0.56524440\n",
      "Iteration 49, loss = 0.56460847\n",
      "Iteration 50, loss = 0.56664694\n",
      "Iteration 51, loss = 0.56001887\n",
      "Iteration 52, loss = 0.56554948\n",
      "Iteration 53, loss = 0.56478617\n",
      "Iteration 54, loss = 0.56285337\n",
      "Iteration 55, loss = 0.56180935\n",
      "Iteration 56, loss = 0.56213576\n",
      "Iteration 57, loss = 0.55914353\n",
      "Iteration 58, loss = 0.55929134\n",
      "Iteration 59, loss = 0.55792919\n",
      "Iteration 60, loss = 0.56187717\n",
      "Iteration 61, loss = 0.55624172\n",
      "Iteration 62, loss = 0.55937526\n",
      "Iteration 63, loss = 0.55744396\n",
      "Iteration 64, loss = 0.56784600\n",
      "Iteration 65, loss = 0.56570116\n",
      "Iteration 66, loss = 0.55581501\n",
      "Iteration 67, loss = 0.56358368\n",
      "Iteration 68, loss = 0.56631040\n",
      "Iteration 69, loss = 0.56192088\n",
      "Iteration 70, loss = 0.55512834\n",
      "Iteration 71, loss = 0.55429727\n",
      "Iteration 72, loss = 0.55441452\n",
      "Iteration 73, loss = 0.55502307\n",
      "Iteration 74, loss = 0.55218428\n",
      "Iteration 75, loss = 0.55966422\n",
      "Iteration 76, loss = 0.55491465\n",
      "Iteration 77, loss = 0.55228759\n",
      "Iteration 78, loss = 0.55505735\n",
      "Iteration 79, loss = 0.55412886\n",
      "Iteration 80, loss = 0.55114166\n",
      "Iteration 81, loss = 0.55398430\n",
      "Iteration 82, loss = 0.55459530\n",
      "Iteration 83, loss = 0.55657899\n",
      "Iteration 84, loss = 0.55221462\n",
      "Iteration 85, loss = 0.55148664\n",
      "Iteration 86, loss = 0.55257434\n",
      "Iteration 87, loss = 0.55367178\n",
      "Iteration 88, loss = 0.55663948\n",
      "Iteration 89, loss = 0.55651466\n",
      "Iteration 90, loss = 0.55196705\n",
      "Iteration 91, loss = 0.54963809\n",
      "Iteration 92, loss = 0.55395010\n",
      "Iteration 93, loss = 0.54989509\n",
      "Iteration 94, loss = 0.54748585\n",
      "Iteration 95, loss = 0.54609413\n",
      "Iteration 96, loss = 0.54708985\n",
      "Iteration 97, loss = 0.55038647\n",
      "Iteration 98, loss = 0.55255881\n",
      "Iteration 99, loss = 0.54753506\n",
      "Iteration 100, loss = 0.54810190\n",
      "Iteration 101, loss = 0.55301804\n",
      "Iteration 102, loss = 0.55039971\n",
      "Iteration 103, loss = 0.54786003\n",
      "Iteration 104, loss = 0.54515525\n",
      "Iteration 105, loss = 0.54810170\n",
      "Iteration 106, loss = 0.55049077\n",
      "Iteration 107, loss = 0.55052364\n",
      "Iteration 108, loss = 0.54430955\n",
      "Iteration 109, loss = 0.54363568\n",
      "Iteration 110, loss = 0.55365893\n",
      "Iteration 111, loss = 0.54738478\n",
      "Iteration 112, loss = 0.54848156\n",
      "Iteration 113, loss = 0.54587228\n",
      "Iteration 114, loss = 0.54396747\n",
      "Iteration 115, loss = 0.54229421\n",
      "Iteration 116, loss = 0.54315275\n",
      "Iteration 117, loss = 0.55266654\n",
      "Iteration 118, loss = 0.54695933\n",
      "Iteration 119, loss = 0.54807171\n",
      "Iteration 120, loss = 0.54491801\n",
      "Iteration 121, loss = 0.54831774\n",
      "Iteration 122, loss = 0.54808459\n",
      "Iteration 123, loss = 0.54384223\n",
      "Iteration 124, loss = 0.54875040\n",
      "Iteration 125, loss = 0.54691195\n",
      "Iteration 126, loss = 0.54519185\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67111624\n",
      "Iteration 2, loss = 0.62216477\n",
      "Iteration 3, loss = 0.61715856\n",
      "Iteration 4, loss = 0.61298500\n",
      "Iteration 5, loss = 0.60321198\n",
      "Iteration 6, loss = 0.60708783\n",
      "Iteration 7, loss = 0.60203710\n",
      "Iteration 8, loss = 0.59739561\n",
      "Iteration 9, loss = 0.59544046\n",
      "Iteration 10, loss = 0.59762085\n",
      "Iteration 11, loss = 0.59023978\n",
      "Iteration 12, loss = 0.59744360\n",
      "Iteration 13, loss = 0.59401400\n",
      "Iteration 14, loss = 0.59423525\n",
      "Iteration 15, loss = 0.59235785\n",
      "Iteration 16, loss = 0.59277419\n",
      "Iteration 17, loss = 0.59399933\n",
      "Iteration 18, loss = 0.58494488\n",
      "Iteration 19, loss = 0.58513980\n",
      "Iteration 20, loss = 0.58654330\n",
      "Iteration 21, loss = 0.59010028\n",
      "Iteration 22, loss = 0.57950300\n",
      "Iteration 23, loss = 0.58467650\n",
      "Iteration 24, loss = 0.58550413\n",
      "Iteration 25, loss = 0.58173441\n",
      "Iteration 26, loss = 0.58172894\n",
      "Iteration 27, loss = 0.57547933\n",
      "Iteration 28, loss = 0.58412413\n",
      "Iteration 29, loss = 0.57933191\n",
      "Iteration 30, loss = 0.57337156\n",
      "Iteration 31, loss = 0.57338298\n",
      "Iteration 32, loss = 0.57705375\n",
      "Iteration 33, loss = 0.57332353\n",
      "Iteration 34, loss = 0.57379505\n",
      "Iteration 35, loss = 0.57664255\n",
      "Iteration 36, loss = 0.57251799\n",
      "Iteration 37, loss = 0.57099868\n",
      "Iteration 38, loss = 0.57563048\n",
      "Iteration 39, loss = 0.57436239\n",
      "Iteration 40, loss = 0.57112520\n",
      "Iteration 41, loss = 0.56877307\n",
      "Iteration 42, loss = 0.57355354\n",
      "Iteration 43, loss = 0.57394334\n",
      "Iteration 44, loss = 0.57065822\n",
      "Iteration 45, loss = 0.57143791\n",
      "Iteration 46, loss = 0.57035432\n",
      "Iteration 47, loss = 0.57168481\n",
      "Iteration 48, loss = 0.56499173\n",
      "Iteration 49, loss = 0.56954273\n",
      "Iteration 50, loss = 0.56614033\n",
      "Iteration 51, loss = 0.56625125\n",
      "Iteration 52, loss = 0.56465231\n",
      "Iteration 53, loss = 0.56576325\n",
      "Iteration 54, loss = 0.56519934\n",
      "Iteration 55, loss = 0.56299808\n",
      "Iteration 56, loss = 0.56297102\n",
      "Iteration 57, loss = 0.56246135\n",
      "Iteration 58, loss = 0.56192460\n",
      "Iteration 59, loss = 0.56160676\n",
      "Iteration 60, loss = 0.56376832\n",
      "Iteration 61, loss = 0.56206194\n",
      "Iteration 62, loss = 0.56007067\n",
      "Iteration 63, loss = 0.55898675\n",
      "Iteration 64, loss = 0.56029087\n",
      "Iteration 65, loss = 0.55904312\n",
      "Iteration 66, loss = 0.56409961\n",
      "Iteration 67, loss = 0.56379308\n",
      "Iteration 68, loss = 0.55826961\n",
      "Iteration 69, loss = 0.55944519\n",
      "Iteration 70, loss = 0.56106868\n",
      "Iteration 71, loss = 0.55794327\n",
      "Iteration 72, loss = 0.56194809\n",
      "Iteration 73, loss = 0.55595518\n",
      "Iteration 74, loss = 0.55742775\n",
      "Iteration 75, loss = 0.56016407\n",
      "Iteration 76, loss = 0.56471545\n",
      "Iteration 77, loss = 0.55994598\n",
      "Iteration 78, loss = 0.55607830\n",
      "Iteration 79, loss = 0.55888460\n",
      "Iteration 80, loss = 0.55397068\n",
      "Iteration 81, loss = 0.55587279\n",
      "Iteration 82, loss = 0.56063841\n",
      "Iteration 83, loss = 0.55503864\n",
      "Iteration 84, loss = 0.55340185\n",
      "Iteration 85, loss = 0.55384425\n",
      "Iteration 86, loss = 0.55475509\n",
      "Iteration 87, loss = 0.55658885\n",
      "Iteration 88, loss = 0.55420488\n",
      "Iteration 89, loss = 0.55557491\n",
      "Iteration 90, loss = 0.55678063\n",
      "Iteration 91, loss = 0.55481794\n",
      "Iteration 92, loss = 0.56279709\n",
      "Iteration 93, loss = 0.55425781\n",
      "Iteration 94, loss = 0.55068244\n",
      "Iteration 95, loss = 0.55534350\n",
      "Iteration 96, loss = 0.55273931\n",
      "Iteration 97, loss = 0.55745464\n",
      "Iteration 98, loss = 0.55225607\n",
      "Iteration 99, loss = 0.55742569\n",
      "Iteration 100, loss = 0.55059436\n",
      "Iteration 101, loss = 0.55082047\n",
      "Iteration 102, loss = 0.54962430\n",
      "Iteration 103, loss = 0.55310159\n",
      "Iteration 104, loss = 0.55196079\n",
      "Iteration 105, loss = 0.55004406\n",
      "Iteration 106, loss = 0.54930754\n",
      "Iteration 107, loss = 0.54877740\n",
      "Iteration 108, loss = 0.54817713\n",
      "Iteration 109, loss = 0.55303146\n",
      "Iteration 110, loss = 0.55012837\n",
      "Iteration 111, loss = 0.54842168\n",
      "Iteration 112, loss = 0.55040987\n",
      "Iteration 113, loss = 0.54736061\n",
      "Iteration 114, loss = 0.54749029\n",
      "Iteration 115, loss = 0.54697821\n",
      "Iteration 116, loss = 0.54835096\n",
      "Iteration 117, loss = 0.54566383\n",
      "Iteration 118, loss = 0.55013408\n",
      "Iteration 119, loss = 0.54484389\n",
      "Iteration 120, loss = 0.54537390\n",
      "Iteration 121, loss = 0.54809420\n",
      "Iteration 122, loss = 0.54614015\n",
      "Iteration 123, loss = 0.54672540\n",
      "Iteration 124, loss = 0.54645036\n",
      "Iteration 125, loss = 0.55319174\n",
      "Iteration 126, loss = 0.55291153\n",
      "Iteration 127, loss = 0.54595083\n",
      "Iteration 128, loss = 0.54581784\n",
      "Iteration 129, loss = 0.54864987\n",
      "Iteration 130, loss = 0.54716843\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68513084\n",
      "Iteration 2, loss = 0.62752830\n",
      "Iteration 3, loss = 0.61488731\n",
      "Iteration 4, loss = 0.61186210\n",
      "Iteration 5, loss = 0.60643982\n",
      "Iteration 6, loss = 0.60545841\n",
      "Iteration 7, loss = 0.60660064\n",
      "Iteration 8, loss = 0.59501352\n",
      "Iteration 9, loss = 0.60443847\n",
      "Iteration 10, loss = 0.60121763\n",
      "Iteration 11, loss = 0.59077071\n",
      "Iteration 12, loss = 0.59706310\n",
      "Iteration 13, loss = 0.59853965\n",
      "Iteration 14, loss = 0.58981301\n",
      "Iteration 15, loss = 0.58612136\n",
      "Iteration 16, loss = 0.59139193\n",
      "Iteration 17, loss = 0.58631619\n",
      "Iteration 18, loss = 0.59314177\n",
      "Iteration 19, loss = 0.58407665\n",
      "Iteration 20, loss = 0.58937179\n",
      "Iteration 21, loss = 0.58584208\n",
      "Iteration 22, loss = 0.59163023\n",
      "Iteration 23, loss = 0.58863140\n",
      "Iteration 24, loss = 0.58723763\n",
      "Iteration 25, loss = 0.58829854\n",
      "Iteration 26, loss = 0.57978501\n",
      "Iteration 27, loss = 0.58706640\n",
      "Iteration 28, loss = 0.58231439\n",
      "Iteration 29, loss = 0.57743400\n",
      "Iteration 30, loss = 0.58325233\n",
      "Iteration 31, loss = 0.57702613\n",
      "Iteration 32, loss = 0.57839546\n",
      "Iteration 33, loss = 0.58100247\n",
      "Iteration 34, loss = 0.57681412\n",
      "Iteration 35, loss = 0.57431274\n",
      "Iteration 36, loss = 0.57737774\n",
      "Iteration 37, loss = 0.57368611\n",
      "Iteration 38, loss = 0.57072013\n",
      "Iteration 39, loss = 0.57532492\n",
      "Iteration 40, loss = 0.58251501\n",
      "Iteration 41, loss = 0.57703721\n",
      "Iteration 42, loss = 0.57383037\n",
      "Iteration 43, loss = 0.57299297\n",
      "Iteration 44, loss = 0.57635829\n",
      "Iteration 45, loss = 0.57477899\n",
      "Iteration 46, loss = 0.57254164\n",
      "Iteration 47, loss = 0.56911988\n",
      "Iteration 48, loss = 0.57044229\n",
      "Iteration 49, loss = 0.56764789\n",
      "Iteration 50, loss = 0.57006148\n",
      "Iteration 51, loss = 0.57253129\n",
      "Iteration 52, loss = 0.57073082\n",
      "Iteration 53, loss = 0.56387028\n",
      "Iteration 54, loss = 0.56947282\n",
      "Iteration 55, loss = 0.57082638\n",
      "Iteration 56, loss = 0.56542451\n",
      "Iteration 57, loss = 0.56430009\n",
      "Iteration 58, loss = 0.56746559\n",
      "Iteration 59, loss = 0.56458127\n",
      "Iteration 60, loss = 0.56534611\n",
      "Iteration 61, loss = 0.56575892\n",
      "Iteration 62, loss = 0.56138160\n",
      "Iteration 63, loss = 0.56584158\n",
      "Iteration 64, loss = 0.56198305\n",
      "Iteration 65, loss = 0.56998739\n",
      "Iteration 66, loss = 0.56270273\n",
      "Iteration 67, loss = 0.56775213\n",
      "Iteration 68, loss = 0.56307808\n",
      "Iteration 69, loss = 0.55868595\n",
      "Iteration 70, loss = 0.56165376\n",
      "Iteration 71, loss = 0.55946490\n",
      "Iteration 72, loss = 0.55955460\n",
      "Iteration 73, loss = 0.56083893\n",
      "Iteration 74, loss = 0.56376894\n",
      "Iteration 75, loss = 0.56221269\n",
      "Iteration 76, loss = 0.56551237\n",
      "Iteration 77, loss = 0.56278408\n",
      "Iteration 78, loss = 0.55921935\n",
      "Iteration 79, loss = 0.55876972\n",
      "Iteration 80, loss = 0.55894084\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66289451\n",
      "Iteration 2, loss = 0.62061708\n",
      "Iteration 3, loss = 0.61121178\n",
      "Iteration 4, loss = 0.60319378\n",
      "Iteration 5, loss = 0.60058172\n",
      "Iteration 6, loss = 0.61066745\n",
      "Iteration 7, loss = 0.60332142\n",
      "Iteration 8, loss = 0.60033473\n",
      "Iteration 9, loss = 0.59648279\n",
      "Iteration 10, loss = 0.59151660\n",
      "Iteration 11, loss = 0.60031049\n",
      "Iteration 12, loss = 0.59465786\n",
      "Iteration 13, loss = 0.59619614\n",
      "Iteration 14, loss = 0.58682830\n",
      "Iteration 15, loss = 0.58596510\n",
      "Iteration 16, loss = 0.58933754\n",
      "Iteration 17, loss = 0.59089860\n",
      "Iteration 18, loss = 0.58394114\n",
      "Iteration 19, loss = 0.58896420\n",
      "Iteration 20, loss = 0.58309550\n",
      "Iteration 21, loss = 0.58101592\n",
      "Iteration 22, loss = 0.58793451\n",
      "Iteration 23, loss = 0.58420312\n",
      "Iteration 24, loss = 0.58284361\n",
      "Iteration 25, loss = 0.57906681\n",
      "Iteration 26, loss = 0.57949920\n",
      "Iteration 27, loss = 0.57865624\n",
      "Iteration 28, loss = 0.57793625\n",
      "Iteration 29, loss = 0.57527880\n",
      "Iteration 30, loss = 0.58029062\n",
      "Iteration 31, loss = 0.57723793\n",
      "Iteration 32, loss = 0.57354300\n",
      "Iteration 33, loss = 0.57708590\n",
      "Iteration 34, loss = 0.57840921\n",
      "Iteration 35, loss = 0.57389326\n",
      "Iteration 36, loss = 0.57406600\n",
      "Iteration 37, loss = 0.57243657\n",
      "Iteration 38, loss = 0.57196278\n",
      "Iteration 39, loss = 0.57385800\n",
      "Iteration 40, loss = 0.57377139\n",
      "Iteration 41, loss = 0.57614255\n",
      "Iteration 42, loss = 0.57246975\n",
      "Iteration 43, loss = 0.57047775\n",
      "Iteration 44, loss = 0.57224316\n",
      "Iteration 45, loss = 0.56935207\n",
      "Iteration 46, loss = 0.56916938\n",
      "Iteration 47, loss = 0.57296810\n",
      "Iteration 48, loss = 0.56954816\n",
      "Iteration 49, loss = 0.57275681\n",
      "Iteration 50, loss = 0.57351955\n",
      "Iteration 51, loss = 0.56866717\n",
      "Iteration 52, loss = 0.56569001\n",
      "Iteration 53, loss = 0.56560074\n",
      "Iteration 54, loss = 0.56486968\n",
      "Iteration 55, loss = 0.56657702\n",
      "Iteration 56, loss = 0.56343925\n",
      "Iteration 57, loss = 0.56278630\n",
      "Iteration 58, loss = 0.56374714\n",
      "Iteration 59, loss = 0.56239685\n",
      "Iteration 60, loss = 0.56374648\n",
      "Iteration 61, loss = 0.56870895\n",
      "Iteration 62, loss = 0.56543212\n",
      "Iteration 63, loss = 0.56946367\n",
      "Iteration 64, loss = 0.56991797\n",
      "Iteration 65, loss = 0.56684937\n",
      "Iteration 66, loss = 0.56412553\n",
      "Iteration 67, loss = 0.56743260\n",
      "Iteration 68, loss = 0.56019884\n",
      "Iteration 69, loss = 0.56111731\n",
      "Iteration 70, loss = 0.56095920\n",
      "Iteration 71, loss = 0.55809765\n",
      "Iteration 72, loss = 0.56355401\n",
      "Iteration 73, loss = 0.56616705\n",
      "Iteration 74, loss = 0.56353426\n",
      "Iteration 75, loss = 0.56256154\n",
      "Iteration 76, loss = 0.56061591\n",
      "Iteration 77, loss = 0.55965140\n",
      "Iteration 78, loss = 0.56050566\n",
      "Iteration 79, loss = 0.55918466\n",
      "Iteration 80, loss = 0.56210175\n",
      "Iteration 81, loss = 0.56011011\n",
      "Iteration 82, loss = 0.55841687\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66785660\n",
      "Iteration 2, loss = 0.63494331\n",
      "Iteration 3, loss = 0.62505791\n",
      "Iteration 4, loss = 0.61431444\n",
      "Iteration 5, loss = 0.60774770\n",
      "Iteration 6, loss = 0.60004634\n",
      "Iteration 7, loss = 0.60132191\n",
      "Iteration 8, loss = 0.59758210\n",
      "Iteration 9, loss = 0.59855099\n",
      "Iteration 10, loss = 0.59792749\n",
      "Iteration 11, loss = 0.59268568\n",
      "Iteration 12, loss = 0.58965809\n",
      "Iteration 13, loss = 0.59733712\n",
      "Iteration 14, loss = 0.59278668\n",
      "Iteration 15, loss = 0.58596844\n",
      "Iteration 16, loss = 0.59162994\n",
      "Iteration 17, loss = 0.58532669\n",
      "Iteration 18, loss = 0.59250004\n",
      "Iteration 19, loss = 0.58324587\n",
      "Iteration 20, loss = 0.58525700\n",
      "Iteration 21, loss = 0.58343629\n",
      "Iteration 22, loss = 0.58396151\n",
      "Iteration 23, loss = 0.58153458\n",
      "Iteration 24, loss = 0.57934889\n",
      "Iteration 25, loss = 0.57828229\n",
      "Iteration 26, loss = 0.58154254\n",
      "Iteration 27, loss = 0.58306910\n",
      "Iteration 28, loss = 0.57745405\n",
      "Iteration 29, loss = 0.57733794\n",
      "Iteration 30, loss = 0.58441623\n",
      "Iteration 31, loss = 0.57686923\n",
      "Iteration 32, loss = 0.57859781\n",
      "Iteration 33, loss = 0.57646448\n",
      "Iteration 34, loss = 0.58167728\n",
      "Iteration 35, loss = 0.57402652\n",
      "Iteration 36, loss = 0.57639448\n",
      "Iteration 37, loss = 0.57517322\n",
      "Iteration 38, loss = 0.57269967\n",
      "Iteration 39, loss = 0.57032034\n",
      "Iteration 40, loss = 0.57421882\n",
      "Iteration 41, loss = 0.56867416\n",
      "Iteration 42, loss = 0.57206064\n",
      "Iteration 43, loss = 0.57094528\n",
      "Iteration 44, loss = 0.57538745\n",
      "Iteration 45, loss = 0.57045796\n",
      "Iteration 46, loss = 0.56569505\n",
      "Iteration 47, loss = 0.56894613\n",
      "Iteration 48, loss = 0.56815908\n",
      "Iteration 49, loss = 0.56529382\n",
      "Iteration 50, loss = 0.56784758\n",
      "Iteration 51, loss = 0.57708967\n",
      "Iteration 52, loss = 0.56985978\n",
      "Iteration 53, loss = 0.56284126\n",
      "Iteration 54, loss = 0.56512065\n",
      "Iteration 55, loss = 0.56747467\n",
      "Iteration 56, loss = 0.56817617\n",
      "Iteration 57, loss = 0.56640604\n",
      "Iteration 58, loss = 0.56063758\n",
      "Iteration 59, loss = 0.55949286\n",
      "Iteration 60, loss = 0.56418466\n",
      "Iteration 61, loss = 0.56422191\n",
      "Iteration 62, loss = 0.56053820\n",
      "Iteration 63, loss = 0.56042575\n",
      "Iteration 64, loss = 0.56134118\n",
      "Iteration 65, loss = 0.56246917\n",
      "Iteration 66, loss = 0.56810028\n",
      "Iteration 67, loss = 0.55801363\n",
      "Iteration 68, loss = 0.55889224\n",
      "Iteration 69, loss = 0.55839779\n",
      "Iteration 70, loss = 0.55682249\n",
      "Iteration 71, loss = 0.55836898\n",
      "Iteration 72, loss = 0.56118212\n",
      "Iteration 73, loss = 0.56978603\n",
      "Iteration 74, loss = 0.56257612\n",
      "Iteration 75, loss = 0.56324192\n",
      "Iteration 76, loss = 0.56184391\n",
      "Iteration 77, loss = 0.55884017\n",
      "Iteration 78, loss = 0.55666507\n",
      "Iteration 79, loss = 0.56400755\n",
      "Iteration 80, loss = 0.55605874\n",
      "Iteration 81, loss = 0.55523830\n",
      "Iteration 82, loss = 0.55686996\n",
      "Iteration 83, loss = 0.55852596\n",
      "Iteration 84, loss = 0.55408240\n",
      "Iteration 85, loss = 0.55387149\n",
      "Iteration 86, loss = 0.55577869\n",
      "Iteration 87, loss = 0.55525690\n",
      "Iteration 88, loss = 0.55629101\n",
      "Iteration 89, loss = 0.56121602\n",
      "Iteration 90, loss = 0.55344295\n",
      "Iteration 91, loss = 0.56210778\n",
      "Iteration 92, loss = 0.55416367\n",
      "Iteration 93, loss = 0.55359251\n",
      "Iteration 94, loss = 0.55462800\n",
      "Iteration 95, loss = 0.55332881\n",
      "Iteration 96, loss = 0.55024275\n",
      "Iteration 97, loss = 0.55679961\n",
      "Iteration 98, loss = 0.55211696\n",
      "Iteration 99, loss = 0.55272669\n",
      "Iteration 100, loss = 0.55427243\n",
      "Iteration 101, loss = 0.55464822\n",
      "Iteration 102, loss = 0.55262846\n",
      "Iteration 103, loss = 0.55193728\n",
      "Iteration 104, loss = 0.55059990\n",
      "Iteration 105, loss = 0.55100632\n",
      "Iteration 106, loss = 0.55170316\n",
      "Iteration 107, loss = 0.55347567\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66324343\n",
      "Iteration 2, loss = 0.62271678\n",
      "Iteration 3, loss = 0.61905358\n",
      "Iteration 4, loss = 0.60628062\n",
      "Iteration 5, loss = 0.60576015\n",
      "Iteration 6, loss = 0.60085173\n",
      "Iteration 7, loss = 0.59766621\n",
      "Iteration 8, loss = 0.59953054\n",
      "Iteration 9, loss = 0.59378009\n",
      "Iteration 10, loss = 0.59155095\n",
      "Iteration 11, loss = 0.59512559\n",
      "Iteration 12, loss = 0.58810152\n",
      "Iteration 13, loss = 0.58999823\n",
      "Iteration 14, loss = 0.58772133\n",
      "Iteration 15, loss = 0.58800638\n",
      "Iteration 16, loss = 0.58602171\n",
      "Iteration 17, loss = 0.58698796\n",
      "Iteration 18, loss = 0.58808130\n",
      "Iteration 19, loss = 0.58641461\n",
      "Iteration 20, loss = 0.57915840\n",
      "Iteration 21, loss = 0.58155291\n",
      "Iteration 22, loss = 0.57787222\n",
      "Iteration 23, loss = 0.58417259\n",
      "Iteration 24, loss = 0.57928543\n",
      "Iteration 25, loss = 0.57821375\n",
      "Iteration 26, loss = 0.57510094\n",
      "Iteration 27, loss = 0.58026199\n",
      "Iteration 28, loss = 0.57687969\n",
      "Iteration 29, loss = 0.57828772\n",
      "Iteration 30, loss = 0.58837225\n",
      "Iteration 31, loss = 0.58636702\n",
      "Iteration 32, loss = 0.57689293\n",
      "Iteration 33, loss = 0.57251089\n",
      "Iteration 34, loss = 0.57654063\n",
      "Iteration 35, loss = 0.57191626\n",
      "Iteration 36, loss = 0.57738520\n",
      "Iteration 37, loss = 0.57288627\n",
      "Iteration 38, loss = 0.57056854\n",
      "Iteration 39, loss = 0.57240074\n",
      "Iteration 40, loss = 0.57092606\n",
      "Iteration 41, loss = 0.56942114\n",
      "Iteration 42, loss = 0.56725790\n",
      "Iteration 43, loss = 0.56732568\n",
      "Iteration 44, loss = 0.57901062\n",
      "Iteration 45, loss = 0.56721534\n",
      "Iteration 46, loss = 0.56579660\n",
      "Iteration 47, loss = 0.56552326\n",
      "Iteration 48, loss = 0.56610287\n",
      "Iteration 49, loss = 0.57050002\n",
      "Iteration 50, loss = 0.56798534\n",
      "Iteration 51, loss = 0.56609281\n",
      "Iteration 52, loss = 0.56323998\n",
      "Iteration 53, loss = 0.57132087\n",
      "Iteration 54, loss = 0.56543135\n",
      "Iteration 55, loss = 0.57324364\n",
      "Iteration 56, loss = 0.56219940\n",
      "Iteration 57, loss = 0.56505374\n",
      "Iteration 58, loss = 0.56120522\n",
      "Iteration 59, loss = 0.56437926\n",
      "Iteration 60, loss = 0.56464299\n",
      "Iteration 61, loss = 0.56428014\n",
      "Iteration 62, loss = 0.56891548\n",
      "Iteration 63, loss = 0.56101379\n",
      "Iteration 64, loss = 0.56249772\n",
      "Iteration 65, loss = 0.56061229\n",
      "Iteration 66, loss = 0.55927012\n",
      "Iteration 67, loss = 0.56231215\n",
      "Iteration 68, loss = 0.56064415\n",
      "Iteration 69, loss = 0.55833417\n",
      "Iteration 70, loss = 0.56086143\n",
      "Iteration 71, loss = 0.55658472\n",
      "Iteration 72, loss = 0.56016179\n",
      "Iteration 73, loss = 0.56629578\n",
      "Iteration 74, loss = 0.55856092\n",
      "Iteration 75, loss = 0.55498926\n",
      "Iteration 76, loss = 0.55676829\n",
      "Iteration 77, loss = 0.56186019\n",
      "Iteration 78, loss = 0.55858569\n",
      "Iteration 79, loss = 0.55513498\n",
      "Iteration 80, loss = 0.55596813\n",
      "Iteration 81, loss = 0.55677008\n",
      "Iteration 82, loss = 0.55200910\n",
      "Iteration 83, loss = 0.55408216\n",
      "Iteration 84, loss = 0.55309730\n",
      "Iteration 85, loss = 0.55385113\n",
      "Iteration 86, loss = 0.55287263\n",
      "Iteration 87, loss = 0.55161780\n",
      "Iteration 88, loss = 0.55369724\n",
      "Iteration 89, loss = 0.55422060\n",
      "Iteration 90, loss = 0.55578744\n",
      "Iteration 91, loss = 0.54987551\n",
      "Iteration 92, loss = 0.55181960\n",
      "Iteration 93, loss = 0.55014346\n",
      "Iteration 94, loss = 0.55266139\n",
      "Iteration 95, loss = 0.55503076\n",
      "Iteration 96, loss = 0.55381952\n",
      "Iteration 97, loss = 0.55188144\n",
      "Iteration 98, loss = 0.55344309\n",
      "Iteration 99, loss = 0.54986794\n",
      "Iteration 100, loss = 0.55209547\n",
      "Iteration 101, loss = 0.54915690\n",
      "Iteration 102, loss = 0.55334107\n",
      "Iteration 103, loss = 0.55131891\n",
      "Iteration 104, loss = 0.55763550\n",
      "Iteration 105, loss = 0.54974441\n",
      "Iteration 106, loss = 0.54598258\n",
      "Iteration 107, loss = 0.54912389\n",
      "Iteration 108, loss = 0.54749643\n",
      "Iteration 109, loss = 0.54834786\n",
      "Iteration 110, loss = 0.54972357\n",
      "Iteration 111, loss = 0.55435231\n",
      "Iteration 112, loss = 0.54879924\n",
      "Iteration 113, loss = 0.54754308\n",
      "Iteration 114, loss = 0.55162808\n",
      "Iteration 115, loss = 0.54747493\n",
      "Iteration 116, loss = 0.54803442\n",
      "Iteration 117, loss = 0.54564426\n",
      "Iteration 118, loss = 0.54623541\n",
      "Iteration 119, loss = 0.54557293\n",
      "Iteration 120, loss = 0.54553169\n",
      "Iteration 121, loss = 0.54492802\n",
      "Iteration 122, loss = 0.54626082\n",
      "Iteration 123, loss = 0.54713670\n",
      "Iteration 124, loss = 0.54461660\n",
      "Iteration 125, loss = 0.54261905\n",
      "Iteration 126, loss = 0.54151118\n",
      "Iteration 127, loss = 0.54635832\n",
      "Iteration 128, loss = 0.54695284\n",
      "Iteration 129, loss = 0.55006925\n",
      "Iteration 130, loss = 0.54809980\n",
      "Iteration 131, loss = 0.54411012\n",
      "Iteration 132, loss = 0.54653849\n",
      "Iteration 133, loss = 0.54311129\n",
      "Iteration 134, loss = 0.54361855\n",
      "Iteration 135, loss = 0.54384423\n",
      "Iteration 136, loss = 0.54386703\n",
      "Iteration 137, loss = 0.54119851\n",
      "Iteration 138, loss = 0.54426560\n",
      "Iteration 139, loss = 0.54627510\n",
      "Iteration 140, loss = 0.54269085\n",
      "Iteration 141, loss = 0.54743814\n",
      "Iteration 142, loss = 0.54997266\n",
      "Iteration 143, loss = 0.54472514\n",
      "Iteration 144, loss = 0.54438368\n",
      "Iteration 145, loss = 0.54180406\n",
      "Iteration 146, loss = 0.54183963\n",
      "Iteration 147, loss = 0.54384622\n",
      "Iteration 148, loss = 0.54556847\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66624452\n",
      "Iteration 2, loss = 0.62873895\n",
      "Iteration 3, loss = 0.61810573\n",
      "Iteration 4, loss = 0.61398785\n",
      "Iteration 5, loss = 0.60514522\n",
      "Iteration 6, loss = 0.59726312\n",
      "Iteration 7, loss = 0.59708178\n",
      "Iteration 8, loss = 0.59722401\n",
      "Iteration 9, loss = 0.60379480\n",
      "Iteration 10, loss = 0.59782563\n",
      "Iteration 11, loss = 0.59044056\n",
      "Iteration 12, loss = 0.59052440\n",
      "Iteration 13, loss = 0.58685989\n",
      "Iteration 14, loss = 0.58846261\n",
      "Iteration 15, loss = 0.58566894\n",
      "Iteration 16, loss = 0.59430561\n",
      "Iteration 17, loss = 0.58899992\n",
      "Iteration 18, loss = 0.58339711\n",
      "Iteration 19, loss = 0.58620865\n",
      "Iteration 20, loss = 0.58106244\n",
      "Iteration 21, loss = 0.58006730\n",
      "Iteration 22, loss = 0.58281964\n",
      "Iteration 23, loss = 0.58011499\n",
      "Iteration 24, loss = 0.57977257\n",
      "Iteration 25, loss = 0.57953392\n",
      "Iteration 26, loss = 0.57559238\n",
      "Iteration 27, loss = 0.58054971\n",
      "Iteration 28, loss = 0.58336586\n",
      "Iteration 29, loss = 0.57569127\n",
      "Iteration 30, loss = 0.57554167\n",
      "Iteration 31, loss = 0.57369685\n",
      "Iteration 32, loss = 0.57014123\n",
      "Iteration 33, loss = 0.57244449\n",
      "Iteration 34, loss = 0.57124827\n",
      "Iteration 35, loss = 0.57110073\n",
      "Iteration 36, loss = 0.56838994\n",
      "Iteration 37, loss = 0.57100869\n",
      "Iteration 38, loss = 0.57345236\n",
      "Iteration 39, loss = 0.57061680\n",
      "Iteration 40, loss = 0.56862322\n",
      "Iteration 41, loss = 0.56769080\n",
      "Iteration 42, loss = 0.56957277\n",
      "Iteration 43, loss = 0.56802661\n",
      "Iteration 44, loss = 0.56695449\n",
      "Iteration 45, loss = 0.56567634\n",
      "Iteration 46, loss = 0.56442228\n",
      "Iteration 47, loss = 0.56817418\n",
      "Iteration 48, loss = 0.56638464\n",
      "Iteration 49, loss = 0.56294044\n",
      "Iteration 50, loss = 0.56696965\n",
      "Iteration 51, loss = 0.56815078\n",
      "Iteration 52, loss = 0.56379932\n",
      "Iteration 53, loss = 0.56293810\n",
      "Iteration 54, loss = 0.56262271\n",
      "Iteration 55, loss = 0.56329045\n",
      "Iteration 56, loss = 0.56332819\n",
      "Iteration 57, loss = 0.56542563\n",
      "Iteration 58, loss = 0.55902738\n",
      "Iteration 59, loss = 0.56064608\n",
      "Iteration 60, loss = 0.56110142\n",
      "Iteration 61, loss = 0.56303521\n",
      "Iteration 62, loss = 0.56048163\n",
      "Iteration 63, loss = 0.55848421\n",
      "Iteration 64, loss = 0.55725295\n",
      "Iteration 65, loss = 0.55860104\n",
      "Iteration 66, loss = 0.55966026\n",
      "Iteration 67, loss = 0.55885687\n",
      "Iteration 68, loss = 0.55912476\n",
      "Iteration 69, loss = 0.56269972\n",
      "Iteration 70, loss = 0.55905093\n",
      "Iteration 71, loss = 0.55634533\n",
      "Iteration 72, loss = 0.56212060\n",
      "Iteration 73, loss = 0.56193513\n",
      "Iteration 74, loss = 0.55973770\n",
      "Iteration 75, loss = 0.55710637\n",
      "Iteration 76, loss = 0.55273685\n",
      "Iteration 77, loss = 0.55565944\n",
      "Iteration 78, loss = 0.55824201\n",
      "Iteration 79, loss = 0.55336645\n",
      "Iteration 80, loss = 0.55771435\n",
      "Iteration 81, loss = 0.56946560\n",
      "Iteration 82, loss = 0.55713450\n",
      "Iteration 83, loss = 0.55589989\n",
      "Iteration 84, loss = 0.55424722\n",
      "Iteration 85, loss = 0.55122860\n",
      "Iteration 86, loss = 0.55321407\n",
      "Iteration 87, loss = 0.55115956\n",
      "Iteration 88, loss = 0.55123706\n",
      "Iteration 89, loss = 0.55328161\n",
      "Iteration 90, loss = 0.55321659\n",
      "Iteration 91, loss = 0.55318418\n",
      "Iteration 92, loss = 0.55345499\n",
      "Iteration 93, loss = 0.55197137\n",
      "Iteration 94, loss = 0.55226308\n",
      "Iteration 95, loss = 0.54936006\n",
      "Iteration 96, loss = 0.55240498\n",
      "Iteration 97, loss = 0.55172407\n",
      "Iteration 98, loss = 0.55031933\n",
      "Iteration 99, loss = 0.55020732\n",
      "Iteration 100, loss = 0.55024387\n",
      "Iteration 101, loss = 0.55108434\n",
      "Iteration 102, loss = 0.54803056\n",
      "Iteration 103, loss = 0.55348769\n",
      "Iteration 104, loss = 0.55374256\n",
      "Iteration 105, loss = 0.55123695\n",
      "Iteration 106, loss = 0.55999908\n",
      "Iteration 107, loss = 0.55090945\n",
      "Iteration 108, loss = 0.55022163\n",
      "Iteration 109, loss = 0.54845929\n",
      "Iteration 110, loss = 0.55075755\n",
      "Iteration 111, loss = 0.54653350\n",
      "Iteration 112, loss = 0.55080136\n",
      "Iteration 113, loss = 0.54854337\n",
      "Iteration 114, loss = 0.54917585\n",
      "Iteration 115, loss = 0.54756977\n",
      "Iteration 116, loss = 0.54941453\n",
      "Iteration 117, loss = 0.54822045\n",
      "Iteration 118, loss = 0.54887082\n",
      "Iteration 119, loss = 0.54941943\n",
      "Iteration 120, loss = 0.54997807\n",
      "Iteration 121, loss = 0.54655358\n",
      "Iteration 122, loss = 0.54468057\n",
      "Iteration 123, loss = 0.55037038\n",
      "Iteration 124, loss = 0.55160694\n",
      "Iteration 125, loss = 0.54862095\n",
      "Iteration 126, loss = 0.54535620\n",
      "Iteration 127, loss = 0.54610485\n",
      "Iteration 128, loss = 0.54484881\n",
      "Iteration 129, loss = 0.54532690\n",
      "Iteration 130, loss = 0.54411340\n",
      "Iteration 131, loss = 0.54597728\n",
      "Iteration 132, loss = 0.54710600\n",
      "Iteration 133, loss = 0.54765079\n",
      "Iteration 134, loss = 0.55812787\n",
      "Iteration 135, loss = 0.55314156\n",
      "Iteration 136, loss = 0.54584541\n",
      "Iteration 137, loss = 0.54390418\n",
      "Iteration 138, loss = 0.54558178\n",
      "Iteration 139, loss = 0.54266210\n",
      "Iteration 140, loss = 0.54776663\n",
      "Iteration 141, loss = 0.54870593\n",
      "Iteration 142, loss = 0.54368906\n",
      "Iteration 143, loss = 0.54419418\n",
      "Iteration 144, loss = 0.54387763\n",
      "Iteration 145, loss = 0.54591654\n",
      "Iteration 146, loss = 0.54653650\n",
      "Iteration 147, loss = 0.54819931\n",
      "Iteration 148, loss = 0.54631792\n",
      "Iteration 149, loss = 0.54715845\n",
      "Iteration 150, loss = 0.54338081\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68336710\n",
      "Iteration 2, loss = 0.62654720\n",
      "Iteration 3, loss = 0.61727693\n",
      "Iteration 4, loss = 0.61589881\n",
      "Iteration 5, loss = 0.61246383\n",
      "Iteration 6, loss = 0.60685825\n",
      "Iteration 7, loss = 0.59846127\n",
      "Iteration 8, loss = 0.61109384\n",
      "Iteration 9, loss = 0.60024029\n",
      "Iteration 10, loss = 0.60363524\n",
      "Iteration 11, loss = 0.60074118\n",
      "Iteration 12, loss = 0.59613581\n",
      "Iteration 13, loss = 0.59388666\n",
      "Iteration 14, loss = 0.60199523\n",
      "Iteration 15, loss = 0.59505715\n",
      "Iteration 16, loss = 0.60150091\n",
      "Iteration 17, loss = 0.60029231\n",
      "Iteration 18, loss = 0.59032381\n",
      "Iteration 19, loss = 0.58968393\n",
      "Iteration 20, loss = 0.58751512\n",
      "Iteration 21, loss = 0.58551736\n",
      "Iteration 22, loss = 0.58297992\n",
      "Iteration 23, loss = 0.58146631\n",
      "Iteration 24, loss = 0.58432890\n",
      "Iteration 25, loss = 0.58212088\n",
      "Iteration 26, loss = 0.58568770\n",
      "Iteration 27, loss = 0.58572546\n",
      "Iteration 28, loss = 0.57979915\n",
      "Iteration 29, loss = 0.58016284\n",
      "Iteration 30, loss = 0.57639583\n",
      "Iteration 31, loss = 0.58320883\n",
      "Iteration 32, loss = 0.57720089\n",
      "Iteration 33, loss = 0.57676212\n",
      "Iteration 34, loss = 0.57591184\n",
      "Iteration 35, loss = 0.57338631\n",
      "Iteration 36, loss = 0.57455583\n",
      "Iteration 37, loss = 0.57457676\n",
      "Iteration 38, loss = 0.57225077\n",
      "Iteration 39, loss = 0.57218096\n",
      "Iteration 40, loss = 0.57437885\n",
      "Iteration 41, loss = 0.57665029\n",
      "Iteration 42, loss = 0.57713818\n",
      "Iteration 43, loss = 0.56904187\n",
      "Iteration 44, loss = 0.57114420\n",
      "Iteration 45, loss = 0.57622596\n",
      "Iteration 46, loss = 0.56980248\n",
      "Iteration 47, loss = 0.57001475\n",
      "Iteration 48, loss = 0.56788245\n",
      "Iteration 49, loss = 0.56795962\n",
      "Iteration 50, loss = 0.56686669\n",
      "Iteration 51, loss = 0.56685586\n",
      "Iteration 52, loss = 0.56749520\n",
      "Iteration 53, loss = 0.56627508\n",
      "Iteration 54, loss = 0.56365056\n",
      "Iteration 55, loss = 0.56603709\n",
      "Iteration 56, loss = 0.56595171\n",
      "Iteration 57, loss = 0.56926126\n",
      "Iteration 58, loss = 0.57002559\n",
      "Iteration 59, loss = 0.56735959\n",
      "Iteration 60, loss = 0.58229721\n",
      "Iteration 61, loss = 0.56928934\n",
      "Iteration 62, loss = 0.56362527\n",
      "Iteration 63, loss = 0.56711521\n",
      "Iteration 64, loss = 0.56569923\n",
      "Iteration 65, loss = 0.56332653\n",
      "Iteration 66, loss = 0.56199265\n",
      "Iteration 67, loss = 0.56608509\n",
      "Iteration 68, loss = 0.56575201\n",
      "Iteration 69, loss = 0.56423798\n",
      "Iteration 70, loss = 0.56454803\n",
      "Iteration 71, loss = 0.56323656\n",
      "Iteration 72, loss = 0.56209942\n",
      "Iteration 73, loss = 0.56165508\n",
      "Iteration 74, loss = 0.56504641\n",
      "Iteration 75, loss = 0.56114822\n",
      "Iteration 76, loss = 0.55891871\n",
      "Iteration 77, loss = 0.55743515\n",
      "Iteration 78, loss = 0.56181103\n",
      "Iteration 79, loss = 0.56125999\n",
      "Iteration 80, loss = 0.56007083\n",
      "Iteration 81, loss = 0.55769704\n",
      "Iteration 82, loss = 0.55692897\n",
      "Iteration 83, loss = 0.55924172\n",
      "Iteration 84, loss = 0.55828478\n",
      "Iteration 85, loss = 0.55529422\n",
      "Iteration 86, loss = 0.55486049\n",
      "Iteration 87, loss = 0.55897840\n",
      "Iteration 88, loss = 0.55545529\n",
      "Iteration 89, loss = 0.55837489\n",
      "Iteration 90, loss = 0.55492088\n",
      "Iteration 91, loss = 0.55581162\n",
      "Iteration 92, loss = 0.55476085\n",
      "Iteration 93, loss = 0.55294454\n",
      "Iteration 94, loss = 0.55423839\n",
      "Iteration 95, loss = 0.55379539\n",
      "Iteration 96, loss = 0.56127458\n",
      "Iteration 97, loss = 0.55380307\n",
      "Iteration 98, loss = 0.56248764\n",
      "Iteration 99, loss = 0.56083022\n",
      "Iteration 100, loss = 0.55414488\n",
      "Iteration 101, loss = 0.55334696\n",
      "Iteration 102, loss = 0.55274990\n",
      "Iteration 103, loss = 0.55117803\n",
      "Iteration 104, loss = 0.55000484\n",
      "Iteration 105, loss = 0.55388946\n",
      "Iteration 106, loss = 0.55123801\n",
      "Iteration 107, loss = 0.55400150\n",
      "Iteration 108, loss = 0.54988758\n",
      "Iteration 109, loss = 0.55343761\n",
      "Iteration 110, loss = 0.55094700\n",
      "Iteration 111, loss = 0.55346086\n",
      "Iteration 112, loss = 0.55713287\n",
      "Iteration 113, loss = 0.54943272\n",
      "Iteration 114, loss = 0.54806418\n",
      "Iteration 115, loss = 0.54903517\n",
      "Iteration 116, loss = 0.55124690\n",
      "Iteration 117, loss = 0.55933010\n",
      "Iteration 118, loss = 0.55415931\n",
      "Iteration 119, loss = 0.55005662\n",
      "Iteration 120, loss = 0.55177863\n",
      "Iteration 121, loss = 0.55403822\n",
      "Iteration 122, loss = 0.54952620\n",
      "Iteration 123, loss = 0.54953124\n",
      "Iteration 124, loss = 0.54971184\n",
      "Iteration 125, loss = 0.54867912\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68038868\n",
      "Iteration 2, loss = 0.62598327\n",
      "Iteration 3, loss = 0.61523653\n",
      "Iteration 4, loss = 0.60541103\n",
      "Iteration 5, loss = 0.60625187\n",
      "Iteration 6, loss = 0.59964168\n",
      "Iteration 7, loss = 0.60123497\n",
      "Iteration 8, loss = 0.59371802\n",
      "Iteration 9, loss = 0.58951845\n",
      "Iteration 10, loss = 0.58822478\n",
      "Iteration 11, loss = 0.59405433\n",
      "Iteration 12, loss = 0.58772391\n",
      "Iteration 13, loss = 0.58620128\n",
      "Iteration 14, loss = 0.58549089\n",
      "Iteration 15, loss = 0.58209567\n",
      "Iteration 16, loss = 0.58378811\n",
      "Iteration 17, loss = 0.58558400\n",
      "Iteration 18, loss = 0.58317518\n",
      "Iteration 19, loss = 0.57936049\n",
      "Iteration 20, loss = 0.57608422\n",
      "Iteration 21, loss = 0.57638299\n",
      "Iteration 22, loss = 0.57883150\n",
      "Iteration 23, loss = 0.57560942\n",
      "Iteration 24, loss = 0.57273815\n",
      "Iteration 25, loss = 0.57153558\n",
      "Iteration 26, loss = 0.57174703\n",
      "Iteration 27, loss = 0.56889558\n",
      "Iteration 28, loss = 0.57267470\n",
      "Iteration 29, loss = 0.57161915\n",
      "Iteration 30, loss = 0.56836787\n",
      "Iteration 31, loss = 0.56916989\n",
      "Iteration 32, loss = 0.57055476\n",
      "Iteration 33, loss = 0.56905530\n",
      "Iteration 34, loss = 0.56708016\n",
      "Iteration 35, loss = 0.57301022\n",
      "Iteration 36, loss = 0.56580547\n",
      "Iteration 37, loss = 0.56900634\n",
      "Iteration 38, loss = 0.56606741\n",
      "Iteration 39, loss = 0.56561014\n",
      "Iteration 40, loss = 0.56713592\n",
      "Iteration 41, loss = 0.56752217\n",
      "Iteration 42, loss = 0.57390115\n",
      "Iteration 43, loss = 0.56447975\n",
      "Iteration 44, loss = 0.56203726\n",
      "Iteration 45, loss = 0.56338449\n",
      "Iteration 46, loss = 0.56182160\n",
      "Iteration 47, loss = 0.56115875\n",
      "Iteration 48, loss = 0.56544549\n",
      "Iteration 49, loss = 0.56904694\n",
      "Iteration 50, loss = 0.56684026\n",
      "Iteration 51, loss = 0.55977401\n",
      "Iteration 52, loss = 0.55947350\n",
      "Iteration 53, loss = 0.56418127\n",
      "Iteration 54, loss = 0.56043660\n",
      "Iteration 55, loss = 0.55899628\n",
      "Iteration 56, loss = 0.56309976\n",
      "Iteration 57, loss = 0.55924309\n",
      "Iteration 58, loss = 0.55648987\n",
      "Iteration 59, loss = 0.55656103\n",
      "Iteration 60, loss = 0.55590433\n",
      "Iteration 61, loss = 0.55507388\n",
      "Iteration 62, loss = 0.55687284\n",
      "Iteration 63, loss = 0.55536325\n",
      "Iteration 64, loss = 0.55842029\n",
      "Iteration 65, loss = 0.55808740\n",
      "Iteration 66, loss = 0.55674084\n",
      "Iteration 67, loss = 0.55258589\n",
      "Iteration 68, loss = 0.55269440\n",
      "Iteration 69, loss = 0.55816907\n",
      "Iteration 70, loss = 0.55787140\n",
      "Iteration 71, loss = 0.55052725\n",
      "Iteration 72, loss = 0.55165549\n",
      "Iteration 73, loss = 0.54980871\n",
      "Iteration 74, loss = 0.55307329\n",
      "Iteration 75, loss = 0.54868729\n",
      "Iteration 76, loss = 0.55201917\n",
      "Iteration 77, loss = 0.55689182\n",
      "Iteration 78, loss = 0.54959316\n",
      "Iteration 79, loss = 0.55130930\n",
      "Iteration 80, loss = 0.55470139\n",
      "Iteration 81, loss = 0.55523690\n",
      "Iteration 82, loss = 0.55301106\n",
      "Iteration 83, loss = 0.55355230\n",
      "Iteration 84, loss = 0.54918527\n",
      "Iteration 85, loss = 0.54882907\n",
      "Iteration 86, loss = 0.54942874\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[0.71625 0.71375 0.705   0.70125 0.72125 0.745   0.73    0.71125 0.73125\n",
      " 0.71   ]\n",
      "0.7185\n"
     ]
    }
   ],
   "source": [
    "summoner_cv_scores = cross_val_score(\n",
    "    estimator = pipeline_neuralnetwork,\n",
    "    X = X_train,\n",
    "    y = y_train,\n",
    "    cv = kfold\n",
    ")\n",
    "\n",
    "print(summoner_cv_scores)\n",
    "print(np.mean(summoner_cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['summonerLevel', 'championLevel','championPoints', \n",
    "             'Assassin', 'Fighter', 'Mage', 'Marksman', 'Support', 'Tank',\n",
    "            'info.attack', 'info.defense', 'info.magic', 'info.difficulty',\n",
    "            'stats.hpregenperlevel',\t'stats.mpregen', 'stats.mpregenperlevel',\t'stats.crit',\t'stats.critperlevel',\n",
    "            'stats.attackdamage', 'stats.attackdamageperlevel', 'stats.attackspeedperlevel',\t'stats.attackspeed',\n",
    "            'final_gold', 'final_xp', 'final_abilityhaste', 'final_abilitypower', 'final_armor', 'final_armorpen',\n",
    "            'final_armorpenpercent', 'final_atkdmg', 'final_bns_armorpenpercent', 'final_bns_magicpenpercent', 'final_ccreduction',\n",
    "            'final_cdreduction', 'final_remaining_health', 'final_health', 'final_healthrgn', 'final_lifesteal', 'final_mppen',\n",
    "            'final_mgpenpercent', 'final_mgres', 'final_ms', 'final_omnivamp', 'final_physicalvamp', 'final_power', 'final_powermax',\n",
    "            'final_powerregen', 'final_spellvamp', 'final_currentgold', 'final_magicdmgdone', 'final_magicdmgdonetochamps', 'final_magicdmgtaken',\n",
    "            'final_physdmgdone', 'final_physdmgdonetochamps', 'final_physdmgtaken', 'final_dmgdone', 'final_dmgdonetochamps', 'final_dmgtaken', \n",
    "            'final_truedmgdone', 'final_truedmgdonetochamps', 'final_truedmgtaken', 'final_goldpersec', 'final_jungleminionskilled', 'final_lvl',\n",
    "            'final_minionskilled', 'final_jungleminionskilled', 'final_jungleminionskilled', 'final_jungleminionskilled', 'final_enemycontrolled'                  \n",
    "             ]\n",
    "\n",
    "X_test = pd.merge(X_test_original, team_positions, how='inner', on=['matchId', 'participantId'])\n",
    "X_test = pd.merge(X_test, champion_data, how='inner', left_on='championId', right_on='key')\n",
    "X_test = pd.merge(X_test, champion_types, how='inner', left_on='championId', right_on='key')\n",
    "X_test = pd.merge(X_test, test_last_frame_values, how='inner', on=['matchId', 'participantId'])\n",
    "X_test = pd.merge(X_test, champion_mastery, how='left', on=['summonerId', 'championId']).fillna(0)\n",
    "\n",
    "X_test = X_test.sort_values(['matchId', 'participantId'], ascending = [True, True]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "convert_team_values(X_test, variables)\n",
    "\n",
    "X_test_perlane = X_test.groupby(['matchId', 'teamPosition'])[['final_gold']].sum().pivot_table(values='final_gold', index='matchId', columns='teamPosition').reset_index().drop(columns=0)\n",
    "\n",
    "for lane in ['BOTTOM', 'JUNGLE', 'MIDDLE', 'TOP', 'UTILITY']:\n",
    "  X_test_perlane[f'{lane}'] = np.where(X_test_perlane[f'{lane}'] >= 0, 1, -1)\n",
    "\n",
    "X_test = (\n",
    "    X_test\n",
    "    .groupby(['matchId'])[variables]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "X_test = pd.merge(X_test, X_test_perlane, how='inner', on='matchId').reset_index(drop = True)\n",
    "X_test = pd.merge(X_test, testing_events, how='inner', on='matchId').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline_neuralnetwork.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matchId</th>\n",
       "      <th>winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8000</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8001</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8002</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8003</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8004</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   matchId  winner\n",
       "0     8000     100\n",
       "1     8001     100\n",
       "2     8002     100\n",
       "3     8003     100\n",
       "4     8004     200"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['winner'] = y_pred\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('../data/submission_neural_network_hidden_layers4x100_alpha005_activtahn_2023_03_30.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
