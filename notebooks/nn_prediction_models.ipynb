{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle League of Legends competition - Neural Network Models\n",
    "\n",
    "## Team: Elden Ring"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://eldenring.wiki.fextralife.com/file/Elden-Ring/mirel_pastor_of_vow.jpg\" alt=\"PRAISE DOG\" style=\"width:806px;height:600px;\"/>\n",
    "\n",
    "#### PRAISE THE DOG!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Win at League of Legends?\n",
    "\n",
    "### Uninstall LoL and [install Dota 2](https://store.steampowered.com/app/570/Dota_2/), EZ. (just kidding, both games are great. Volvo pls gib patch.)\n",
    "\n",
    "<img src = \"https://static.wikia.nocookie.net/dota2_gamepedia/images/7/78/Keyart_phoenix.jpg/revision/latest/\" alt=\"SKREE CAW CAW IM A BIRD\" style=\"width:800px;height:497px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_original = pd.read_csv('../data/participants_train.csv')\n",
    "X_test_original = pd.read_csv('../data/participants_test.csv')\n",
    "y_train_original = pd.read_csv('../data/train_winners.csv')\n",
    "\n",
    "champion_mastery = pd.read_csv('../data/champion_mastery.csv')\n",
    "champion = pd.read_json('../data/champion.json')\n",
    "\n",
    "team_positions = pd.read_csv('../data/teamPositions.csv')\n",
    "\n",
    "train_last_frame_values = pd.read_csv('../data/train_last_frame_values.csv')\n",
    "test_last_frame_values = pd.read_csv('../data/test_last_frame_values.csv')\n",
    "\n",
    "training_events = pd.read_csv('../data/training_events.csv')\n",
    "testing_events = pd.read_csv('../data/testing_events.csv')\n",
    "\n",
    "submission = pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that converts values to negative (for the second team, teamId 200)\n",
    "# it leaves the first team values, teamId intact\n",
    "\n",
    "def convert_team_values(df, col_names):\n",
    "    \n",
    "    \n",
    "    for col in col_names:\n",
    "        df[col] = np.where(df['teamId'] == 200,\n",
    "                            -1* df[col],\n",
    "                                df[col])\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation needed on the dataframes (see the file prediction_models)\n",
    "vars = ['wards_placed', 'wards_killed', 'turretplates_destroyed', 'elite_monsters_killed']\n",
    "\n",
    "convert_team_values(training_events, vars)\n",
    "convert_team_values(testing_events, vars)\n",
    "\n",
    "training_events = training_events.groupby('matchId')[vars].sum()\n",
    "testing_events = testing_events.groupby('matchId')[vars].sum()\n",
    "\n",
    "champion_data = pd.json_normalize(champion['data'])\n",
    "champion_data['key'] = champion_data['key'].astype(int)\n",
    "\n",
    "champion_types= champion_data.explode('tags').pivot_table(values='id', index='key', columns='tags', aggfunc='count').fillna(0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be used later to measure the accuracy!\n",
    "kfold = KFold(n_splits = 10, shuffle = True, random_state = 42)\n",
    "\n",
    "# this is to extract the column that is needed for training\n",
    "y_train = y_train_original['winner']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Neural Networks now to try and improve the logreg predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['summonerLevel', 'championLevel','championPoints', \n",
    "             'Assassin', 'Fighter', 'Mage', 'Marksman', 'Support', 'Tank',\n",
    "            'info.attack', 'info.defense', 'info.magic', 'info.difficulty',\n",
    "            'stats.hpregenperlevel',\t'stats.mpregen', 'stats.mpregenperlevel',\t'stats.crit',\t'stats.critperlevel',\n",
    "            'stats.attackdamage', 'stats.attackdamageperlevel', 'stats.attackspeedperlevel',\t'stats.attackspeed',\n",
    "            'final_gold', 'final_xp', 'final_abilityhaste', 'final_abilitypower', 'final_armor', 'final_armorpen',\n",
    "            'final_armorpenpercent', 'final_atkdmg', 'final_bns_armorpenpercent', 'final_bns_magicpenpercent', 'final_ccreduction',\n",
    "            'final_cdreduction', 'final_remaining_health', 'final_health', 'final_healthrgn', 'final_lifesteal', 'final_mppen',\n",
    "            'final_mgpenpercent', 'final_mgres', 'final_ms', 'final_omnivamp', 'final_physicalvamp', 'final_power', 'final_powermax',\n",
    "            'final_powerregen', 'final_spellvamp', 'final_currentgold', 'final_magicdmgdone', 'final_magicdmgdonetochamps', 'final_magicdmgtaken',\n",
    "            'final_physdmgdone', 'final_physdmgdonetochamps', 'final_physdmgtaken', 'final_dmgdone', 'final_dmgdonetochamps', 'final_dmgtaken', \n",
    "            'final_truedmgdone', 'final_truedmgdonetochamps', 'final_truedmgtaken', 'final_goldpersec', 'final_jungleminionskilled', 'final_lvl',\n",
    "            'final_minionskilled', 'final_jungleminionskilled', 'final_jungleminionskilled', 'final_jungleminionskilled', 'final_enemycontrolled'                  \n",
    "             ]\n",
    "\n",
    "X_train = pd.merge(X_train_original, team_positions, how='inner', on=['matchId', 'participantId'])\n",
    "X_train = pd.merge(X_train, champion_data, how='inner', left_on='championId', right_on='key')\n",
    "X_train = pd.merge(X_train, champion_types, how='inner', left_on='championId', right_on='key')\n",
    "X_train = pd.merge(X_train, train_last_frame_values, how='inner', on=['matchId', 'participantId'])\n",
    "X_train = pd.merge(X_train, champion_mastery, how='left', on=['summonerId', 'championId']).fillna(0)\n",
    "\n",
    "X_train = X_train.sort_values(['matchId', 'participantId'], ascending = [True, True]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "convert_team_values(X_train, variables)\n",
    "\n",
    "X_train_perlane = X_train.groupby(['matchId', 'teamPosition'])[['final_gold']].sum().pivot_table(values='final_gold', index='matchId', columns='teamPosition').reset_index().drop(columns=0)\n",
    "\n",
    "for lane in ['BOTTOM', 'JUNGLE', 'MIDDLE', 'TOP', 'UTILITY']:\n",
    "  X_train_perlane[f'{lane}'] = np.where(X_train_perlane[f'{lane}'] >= 0, 1, -1)\n",
    "\n",
    "X_train = (\n",
    "    X_train\n",
    "    .groupby(['matchId'])[variables]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "X_train = pd.merge(X_train, X_train_perlane, how='inner', on='matchId').reset_index(drop = True)\n",
    "X_train = pd.merge(X_train, training_events, how='inner', on='matchId').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matchId</th>\n",
       "      <th>summonerLevel</th>\n",
       "      <th>championLevel</th>\n",
       "      <th>championPoints</th>\n",
       "      <th>Assassin</th>\n",
       "      <th>Fighter</th>\n",
       "      <th>Mage</th>\n",
       "      <th>Marksman</th>\n",
       "      <th>Support</th>\n",
       "      <th>Tank</th>\n",
       "      <th>...</th>\n",
       "      <th>final_enemycontrolled</th>\n",
       "      <th>BOTTOM</th>\n",
       "      <th>JUNGLE</th>\n",
       "      <th>MIDDLE</th>\n",
       "      <th>TOP</th>\n",
       "      <th>UTILITY</th>\n",
       "      <th>wards_placed</th>\n",
       "      <th>wards_killed</th>\n",
       "      <th>turretplates_destroyed</th>\n",
       "      <th>elite_monsters_killed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>682</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-605428.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>67664</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-64</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>628</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1356027.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-61783</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1049</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-273911.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-132630</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-1027</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-287667.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-39616</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1612</td>\n",
       "      <td>7.0</td>\n",
       "      <td>503668.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16629</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   matchId  summonerLevel  championLevel  championPoints  Assassin  Fighter  \\\n",
       "0        0            682            0.0       -605428.0      -1.0     -1.0   \n",
       "1        1            628            8.0       1356027.0       3.0      0.0   \n",
       "2        2           1049            1.0       -273911.0      -2.0      1.0   \n",
       "3        3          -1027           -3.0       -287667.0       1.0      1.0   \n",
       "4        4           1612            7.0        503668.0       0.0      0.0   \n",
       "\n",
       "   Mage  Marksman  Support  Tank  ...  final_enemycontrolled  BOTTOM  JUNGLE  \\\n",
       "0   2.0       0.0      0.0  -2.0  ...                  67664      -1      -1   \n",
       "1   0.0       0.0     -1.0  -1.0  ...                 -61783       1       1   \n",
       "2   0.0       0.0      0.0   0.0  ...                -132630       1      -1   \n",
       "3   1.0      -1.0     -1.0   0.0  ...                 -39616       1       1   \n",
       "4   2.0      -1.0      0.0  -1.0  ...                  16629      -1       1   \n",
       "\n",
       "   MIDDLE  TOP  UTILITY  wards_placed  wards_killed  turretplates_destroyed  \\\n",
       "0       1   -1       -1           -64            -1                      -2   \n",
       "1       1   -1        1             1             0                      -1   \n",
       "2      -1   -1       -1             4             1                      -2   \n",
       "3      -1   -1        1             4             0                       2   \n",
       "4      -1    1        1             4             0                      -1   \n",
       "\n",
       "   elite_monsters_killed  \n",
       "0                     -1  \n",
       "1                     -1  \n",
       "2                      1  \n",
       "3                      0  \n",
       "4                     -1  \n",
       "\n",
       "[5 rows x 79 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_neuralnetwork = Pipeline(\n",
    "    steps = [\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('nn', MLPClassifier(verbose = True,\n",
    "                             hidden_layer_sizes = (100, 100, 100, 100),\n",
    "                             activation = 'tanh',\n",
    "                             max_iter = 10000,\n",
    "                             alpha=0.05))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.67293528\n",
      "Iteration 2, loss = 0.62760996\n",
      "Iteration 3, loss = 0.61507694\n",
      "Iteration 4, loss = 0.60792764\n",
      "Iteration 5, loss = 0.60942378\n",
      "Iteration 6, loss = 0.60098059\n",
      "Iteration 7, loss = 0.59904177\n",
      "Iteration 8, loss = 0.59567799\n",
      "Iteration 9, loss = 0.59229353\n",
      "Iteration 10, loss = 0.59068142\n",
      "Iteration 11, loss = 0.60015370\n",
      "Iteration 12, loss = 0.60409990\n",
      "Iteration 13, loss = 0.59377210\n",
      "Iteration 14, loss = 0.59273640\n",
      "Iteration 15, loss = 0.58455900\n",
      "Iteration 16, loss = 0.58548125\n",
      "Iteration 17, loss = 0.58130663\n",
      "Iteration 18, loss = 0.58291796\n",
      "Iteration 19, loss = 0.59040405\n",
      "Iteration 20, loss = 0.58682680\n",
      "Iteration 21, loss = 0.58339420\n",
      "Iteration 22, loss = 0.58146511\n",
      "Iteration 23, loss = 0.58046965\n",
      "Iteration 24, loss = 0.57775138\n",
      "Iteration 25, loss = 0.57732240\n",
      "Iteration 26, loss = 0.57702880\n",
      "Iteration 27, loss = 0.57357318\n",
      "Iteration 28, loss = 0.57858250\n",
      "Iteration 29, loss = 0.57253075\n",
      "Iteration 30, loss = 0.57304390\n",
      "Iteration 31, loss = 0.57745479\n",
      "Iteration 32, loss = 0.57961947\n",
      "Iteration 33, loss = 0.57223664\n",
      "Iteration 34, loss = 0.56922262\n",
      "Iteration 35, loss = 0.56902986\n",
      "Iteration 36, loss = 0.56914336\n",
      "Iteration 37, loss = 0.56985931\n",
      "Iteration 38, loss = 0.56845327\n",
      "Iteration 39, loss = 0.57218695\n",
      "Iteration 40, loss = 0.56821361\n",
      "Iteration 41, loss = 0.56518019\n",
      "Iteration 42, loss = 0.56675755\n",
      "Iteration 43, loss = 0.56561004\n",
      "Iteration 44, loss = 0.56872677\n",
      "Iteration 45, loss = 0.56430572\n",
      "Iteration 46, loss = 0.56559055\n",
      "Iteration 47, loss = 0.56311310\n",
      "Iteration 48, loss = 0.56579131\n",
      "Iteration 49, loss = 0.56234052\n",
      "Iteration 50, loss = 0.56514553\n",
      "Iteration 51, loss = 0.56319263\n",
      "Iteration 52, loss = 0.56643181\n",
      "Iteration 53, loss = 0.56220227\n",
      "Iteration 54, loss = 0.56012157\n",
      "Iteration 55, loss = 0.56757127\n",
      "Iteration 56, loss = 0.56383916\n",
      "Iteration 57, loss = 0.55998723\n",
      "Iteration 58, loss = 0.56289828\n",
      "Iteration 59, loss = 0.56043752\n",
      "Iteration 60, loss = 0.55922468\n",
      "Iteration 61, loss = 0.55901761\n",
      "Iteration 62, loss = 0.56435207\n",
      "Iteration 63, loss = 0.56101457\n",
      "Iteration 64, loss = 0.56234680\n",
      "Iteration 65, loss = 0.55772745\n",
      "Iteration 66, loss = 0.56025685\n",
      "Iteration 67, loss = 0.55798780\n",
      "Iteration 68, loss = 0.56007296\n",
      "Iteration 69, loss = 0.56195231\n",
      "Iteration 70, loss = 0.56257074\n",
      "Iteration 71, loss = 0.55869264\n",
      "Iteration 72, loss = 0.55840970\n",
      "Iteration 73, loss = 0.55925574\n",
      "Iteration 74, loss = 0.56227111\n",
      "Iteration 75, loss = 0.55801326\n",
      "Iteration 76, loss = 0.55641385\n",
      "Iteration 77, loss = 0.56558774\n",
      "Iteration 78, loss = 0.55816495\n",
      "Iteration 79, loss = 0.55464586\n",
      "Iteration 80, loss = 0.55900453\n",
      "Iteration 81, loss = 0.55631971\n",
      "Iteration 82, loss = 0.55569176\n",
      "Iteration 83, loss = 0.55783539\n",
      "Iteration 84, loss = 0.55485364\n",
      "Iteration 85, loss = 0.55554357\n",
      "Iteration 86, loss = 0.55281457\n",
      "Iteration 87, loss = 0.55730980\n",
      "Iteration 88, loss = 0.56024985\n",
      "Iteration 89, loss = 0.55183243\n",
      "Iteration 90, loss = 0.55167752\n",
      "Iteration 91, loss = 0.55304369\n",
      "Iteration 92, loss = 0.55136798\n",
      "Iteration 93, loss = 0.55410610\n",
      "Iteration 94, loss = 0.55202762\n",
      "Iteration 95, loss = 0.55676999\n",
      "Iteration 96, loss = 0.55162332\n",
      "Iteration 97, loss = 0.55187211\n",
      "Iteration 98, loss = 0.55207061\n",
      "Iteration 99, loss = 0.55350413\n",
      "Iteration 100, loss = 0.55165905\n",
      "Iteration 101, loss = 0.55526055\n",
      "Iteration 102, loss = 0.55477065\n",
      "Iteration 103, loss = 0.55173901\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler()),\n",
       "                (&#x27;nn&#x27;,\n",
       "                 MLPClassifier(activation=&#x27;tanh&#x27;, alpha=0.05,\n",
       "                               hidden_layer_sizes=(100, 100, 100, 100),\n",
       "                               max_iter=10000, verbose=True))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler()),\n",
       "                (&#x27;nn&#x27;,\n",
       "                 MLPClassifier(activation=&#x27;tanh&#x27;, alpha=0.05,\n",
       "                               hidden_layer_sizes=(100, 100, 100, 100),\n",
       "                               max_iter=10000, verbose=True))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, alpha=0.05,\n",
       "              hidden_layer_sizes=(100, 100, 100, 100), max_iter=10000,\n",
       "              verbose=True)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('scaler', MinMaxScaler()),\n",
       "                ('nn',\n",
       "                 MLPClassifier(activation='tanh', alpha=0.05,\n",
       "                               hidden_layer_sizes=(100, 100, 100, 100),\n",
       "                               max_iter=10000, verbose=True))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_neuralnetwork.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7315"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(\n",
    "    y_true = y_train,\n",
    "    y_pred = pipeline_neuralnetwork.predict(X_train)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.68012126\n",
      "Iteration 2, loss = 0.63318414\n",
      "Iteration 3, loss = 0.62492725\n",
      "Iteration 4, loss = 0.61127346\n",
      "Iteration 5, loss = 0.60608691\n",
      "Iteration 6, loss = 0.60779001\n",
      "Iteration 7, loss = 0.59552473\n",
      "Iteration 8, loss = 0.59335532\n",
      "Iteration 9, loss = 0.60281433\n",
      "Iteration 10, loss = 0.59165388\n",
      "Iteration 11, loss = 0.58819519\n",
      "Iteration 12, loss = 0.59135862\n",
      "Iteration 13, loss = 0.60295727\n",
      "Iteration 14, loss = 0.58716376\n",
      "Iteration 15, loss = 0.58519290\n",
      "Iteration 16, loss = 0.58470119\n",
      "Iteration 17, loss = 0.58602775\n",
      "Iteration 18, loss = 0.58489872\n",
      "Iteration 19, loss = 0.58357934\n",
      "Iteration 20, loss = 0.58700730\n",
      "Iteration 21, loss = 0.58553890\n",
      "Iteration 22, loss = 0.57925451\n",
      "Iteration 23, loss = 0.57869636\n",
      "Iteration 24, loss = 0.57895957\n",
      "Iteration 25, loss = 0.58066444\n",
      "Iteration 26, loss = 0.58006968\n",
      "Iteration 27, loss = 0.57487156\n",
      "Iteration 28, loss = 0.57452987\n",
      "Iteration 29, loss = 0.57986518\n",
      "Iteration 30, loss = 0.57165764\n",
      "Iteration 31, loss = 0.57520205\n",
      "Iteration 32, loss = 0.57515729\n",
      "Iteration 33, loss = 0.57203338\n",
      "Iteration 34, loss = 0.57395240\n",
      "Iteration 35, loss = 0.57125477\n",
      "Iteration 36, loss = 0.57074094\n",
      "Iteration 37, loss = 0.57194621\n",
      "Iteration 38, loss = 0.56993591\n",
      "Iteration 39, loss = 0.56791397\n",
      "Iteration 40, loss = 0.57109032\n",
      "Iteration 41, loss = 0.56717821\n",
      "Iteration 42, loss = 0.57266415\n",
      "Iteration 43, loss = 0.56641059\n",
      "Iteration 44, loss = 0.57301882\n",
      "Iteration 45, loss = 0.56658529\n",
      "Iteration 46, loss = 0.56442914\n",
      "Iteration 47, loss = 0.56563024\n",
      "Iteration 48, loss = 0.56652321\n",
      "Iteration 49, loss = 0.56269655\n",
      "Iteration 50, loss = 0.56248697\n",
      "Iteration 51, loss = 0.56328743\n",
      "Iteration 52, loss = 0.56007007\n",
      "Iteration 53, loss = 0.56408597\n",
      "Iteration 54, loss = 0.56167168\n",
      "Iteration 55, loss = 0.56204765\n",
      "Iteration 56, loss = 0.56088690\n",
      "Iteration 57, loss = 0.56240522\n",
      "Iteration 58, loss = 0.56333635\n",
      "Iteration 59, loss = 0.56368228\n",
      "Iteration 60, loss = 0.55863168\n",
      "Iteration 61, loss = 0.55606787\n",
      "Iteration 62, loss = 0.56313247\n",
      "Iteration 63, loss = 0.55504253\n",
      "Iteration 64, loss = 0.56052932\n",
      "Iteration 65, loss = 0.55900639\n",
      "Iteration 66, loss = 0.55628476\n",
      "Iteration 67, loss = 0.55634603\n",
      "Iteration 68, loss = 0.55361535\n",
      "Iteration 69, loss = 0.55603599\n",
      "Iteration 70, loss = 0.55945560\n",
      "Iteration 71, loss = 0.55544245\n",
      "Iteration 72, loss = 0.55779279\n",
      "Iteration 73, loss = 0.55634423\n",
      "Iteration 74, loss = 0.55497838\n",
      "Iteration 75, loss = 0.55281583\n",
      "Iteration 76, loss = 0.55063427\n",
      "Iteration 77, loss = 0.55083478\n",
      "Iteration 78, loss = 0.55372776\n",
      "Iteration 79, loss = 0.55894782\n",
      "Iteration 80, loss = 0.55261747\n",
      "Iteration 81, loss = 0.55342186\n",
      "Iteration 82, loss = 0.55002473\n",
      "Iteration 83, loss = 0.54952199\n",
      "Iteration 84, loss = 0.55455347\n",
      "Iteration 85, loss = 0.54867377\n",
      "Iteration 86, loss = 0.55156763\n",
      "Iteration 87, loss = 0.55260255\n",
      "Iteration 88, loss = 0.56141561\n",
      "Iteration 89, loss = 0.55163462\n",
      "Iteration 90, loss = 0.54923605\n",
      "Iteration 91, loss = 0.54956232\n",
      "Iteration 92, loss = 0.54804213\n",
      "Iteration 93, loss = 0.55705074\n",
      "Iteration 94, loss = 0.55069854\n",
      "Iteration 95, loss = 0.54781803\n",
      "Iteration 96, loss = 0.55025244\n",
      "Iteration 97, loss = 0.54466545\n",
      "Iteration 98, loss = 0.54751401\n",
      "Iteration 99, loss = 0.54565726\n",
      "Iteration 100, loss = 0.54606571\n",
      "Iteration 101, loss = 0.54757658\n",
      "Iteration 102, loss = 0.54741349\n",
      "Iteration 103, loss = 0.55439702\n",
      "Iteration 104, loss = 0.54898279\n",
      "Iteration 105, loss = 0.55121885\n",
      "Iteration 106, loss = 0.54827203\n",
      "Iteration 107, loss = 0.54794780\n",
      "Iteration 108, loss = 0.54824075\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65816714\n",
      "Iteration 2, loss = 0.63193919\n",
      "Iteration 3, loss = 0.61635670\n",
      "Iteration 4, loss = 0.61060121\n",
      "Iteration 5, loss = 0.60845049\n",
      "Iteration 6, loss = 0.60780892\n",
      "Iteration 7, loss = 0.60299638\n",
      "Iteration 8, loss = 0.59456975\n",
      "Iteration 9, loss = 0.59516261\n",
      "Iteration 10, loss = 0.59095326\n",
      "Iteration 11, loss = 0.59096693\n",
      "Iteration 12, loss = 0.59445529\n",
      "Iteration 13, loss = 0.58872446\n",
      "Iteration 14, loss = 0.58929271\n",
      "Iteration 15, loss = 0.58913620\n",
      "Iteration 16, loss = 0.58467660\n",
      "Iteration 17, loss = 0.58466702\n",
      "Iteration 18, loss = 0.58207589\n",
      "Iteration 19, loss = 0.57986654\n",
      "Iteration 20, loss = 0.58309814\n",
      "Iteration 21, loss = 0.58836591\n",
      "Iteration 22, loss = 0.58097539\n",
      "Iteration 23, loss = 0.58027613\n",
      "Iteration 24, loss = 0.57599417\n",
      "Iteration 25, loss = 0.58073569\n",
      "Iteration 26, loss = 0.57710456\n",
      "Iteration 27, loss = 0.57433997\n",
      "Iteration 28, loss = 0.57788486\n",
      "Iteration 29, loss = 0.57225173\n",
      "Iteration 30, loss = 0.57189773\n",
      "Iteration 31, loss = 0.57398829\n",
      "Iteration 32, loss = 0.57146749\n",
      "Iteration 33, loss = 0.57250703\n",
      "Iteration 34, loss = 0.57107498\n",
      "Iteration 35, loss = 0.56997766\n",
      "Iteration 36, loss = 0.56786251\n",
      "Iteration 37, loss = 0.56858606\n",
      "Iteration 38, loss = 0.57049480\n",
      "Iteration 39, loss = 0.57663200\n",
      "Iteration 40, loss = 0.57225254\n",
      "Iteration 41, loss = 0.56702168\n",
      "Iteration 42, loss = 0.57570866\n",
      "Iteration 43, loss = 0.56636776\n",
      "Iteration 44, loss = 0.57142619\n",
      "Iteration 45, loss = 0.56587164\n",
      "Iteration 46, loss = 0.57108447\n",
      "Iteration 47, loss = 0.56355755\n",
      "Iteration 48, loss = 0.56420232\n",
      "Iteration 49, loss = 0.56754748\n",
      "Iteration 50, loss = 0.56095780\n",
      "Iteration 51, loss = 0.56631332\n",
      "Iteration 52, loss = 0.56286424\n",
      "Iteration 53, loss = 0.56075500\n",
      "Iteration 54, loss = 0.56331972\n",
      "Iteration 55, loss = 0.56100978\n",
      "Iteration 56, loss = 0.56040109\n",
      "Iteration 57, loss = 0.57116889\n",
      "Iteration 58, loss = 0.56581595\n",
      "Iteration 59, loss = 0.56069807\n",
      "Iteration 60, loss = 0.55857673\n",
      "Iteration 61, loss = 0.56181668\n",
      "Iteration 62, loss = 0.56375990\n",
      "Iteration 63, loss = 0.56259562\n",
      "Iteration 64, loss = 0.55990311\n",
      "Iteration 65, loss = 0.55806171\n",
      "Iteration 66, loss = 0.55917396\n",
      "Iteration 67, loss = 0.55608796\n",
      "Iteration 68, loss = 0.55697924\n",
      "Iteration 69, loss = 0.56062097\n",
      "Iteration 70, loss = 0.55745377\n",
      "Iteration 71, loss = 0.56161703\n",
      "Iteration 72, loss = 0.56654908\n",
      "Iteration 73, loss = 0.55612359\n",
      "Iteration 74, loss = 0.55712627\n",
      "Iteration 75, loss = 0.55509534\n",
      "Iteration 76, loss = 0.55705335\n",
      "Iteration 77, loss = 0.55635766\n",
      "Iteration 78, loss = 0.55707536\n",
      "Iteration 79, loss = 0.56183804\n",
      "Iteration 80, loss = 0.55423030\n",
      "Iteration 81, loss = 0.55761135\n",
      "Iteration 82, loss = 0.55370015\n",
      "Iteration 83, loss = 0.55159869\n",
      "Iteration 84, loss = 0.56078797\n",
      "Iteration 85, loss = 0.55489891\n",
      "Iteration 86, loss = 0.55652089\n",
      "Iteration 87, loss = 0.55228570\n",
      "Iteration 88, loss = 0.55122149\n",
      "Iteration 89, loss = 0.55389222\n",
      "Iteration 90, loss = 0.55124813\n",
      "Iteration 91, loss = 0.55117906\n",
      "Iteration 92, loss = 0.55008151\n",
      "Iteration 93, loss = 0.55292930\n",
      "Iteration 94, loss = 0.55711724\n",
      "Iteration 95, loss = 0.55546066\n",
      "Iteration 96, loss = 0.54891994\n",
      "Iteration 97, loss = 0.54971776\n",
      "Iteration 98, loss = 0.55307358\n",
      "Iteration 99, loss = 0.55516203\n",
      "Iteration 100, loss = 0.55169655\n",
      "Iteration 101, loss = 0.54819093\n",
      "Iteration 102, loss = 0.55015639\n",
      "Iteration 103, loss = 0.55071455\n",
      "Iteration 104, loss = 0.54956937\n",
      "Iteration 105, loss = 0.54935935\n",
      "Iteration 106, loss = 0.54743699\n",
      "Iteration 107, loss = 0.54882904\n",
      "Iteration 108, loss = 0.54867190\n",
      "Iteration 109, loss = 0.55441107\n",
      "Iteration 110, loss = 0.55051993\n",
      "Iteration 111, loss = 0.54895971\n",
      "Iteration 112, loss = 0.54740203\n",
      "Iteration 113, loss = 0.54896695\n",
      "Iteration 114, loss = 0.55634729\n",
      "Iteration 115, loss = 0.55050308\n",
      "Iteration 116, loss = 0.55750478\n",
      "Iteration 117, loss = 0.55023671\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65956950\n",
      "Iteration 2, loss = 0.62303971\n",
      "Iteration 3, loss = 0.61947177\n",
      "Iteration 4, loss = 0.60721367\n",
      "Iteration 5, loss = 0.60708905\n",
      "Iteration 6, loss = 0.59948422\n",
      "Iteration 7, loss = 0.59711501\n",
      "Iteration 8, loss = 0.59939546\n",
      "Iteration 9, loss = 0.59840821\n",
      "Iteration 10, loss = 0.59440430\n",
      "Iteration 11, loss = 0.59245833\n",
      "Iteration 12, loss = 0.58833155\n",
      "Iteration 13, loss = 0.59468876\n",
      "Iteration 14, loss = 0.58832071\n",
      "Iteration 15, loss = 0.58838760\n",
      "Iteration 16, loss = 0.58701915\n",
      "Iteration 17, loss = 0.58747044\n",
      "Iteration 18, loss = 0.58573252\n",
      "Iteration 19, loss = 0.59079224\n",
      "Iteration 20, loss = 0.58610715\n",
      "Iteration 21, loss = 0.57987707\n",
      "Iteration 22, loss = 0.58032342\n",
      "Iteration 23, loss = 0.58150435\n",
      "Iteration 24, loss = 0.57638390\n",
      "Iteration 25, loss = 0.57545636\n",
      "Iteration 26, loss = 0.57411211\n",
      "Iteration 27, loss = 0.58084809\n",
      "Iteration 28, loss = 0.57518559\n",
      "Iteration 29, loss = 0.58337690\n",
      "Iteration 30, loss = 0.57617070\n",
      "Iteration 31, loss = 0.57430557\n",
      "Iteration 32, loss = 0.57747752\n",
      "Iteration 33, loss = 0.57038259\n",
      "Iteration 34, loss = 0.56960900\n",
      "Iteration 35, loss = 0.57637748\n",
      "Iteration 36, loss = 0.57078562\n",
      "Iteration 37, loss = 0.56897352\n",
      "Iteration 38, loss = 0.56961617\n",
      "Iteration 39, loss = 0.56876343\n",
      "Iteration 40, loss = 0.57072163\n",
      "Iteration 41, loss = 0.57179531\n",
      "Iteration 42, loss = 0.57461577\n",
      "Iteration 43, loss = 0.56995521\n",
      "Iteration 44, loss = 0.56699336\n",
      "Iteration 45, loss = 0.56713678\n",
      "Iteration 46, loss = 0.56766946\n",
      "Iteration 47, loss = 0.56655144\n",
      "Iteration 48, loss = 0.56476323\n",
      "Iteration 49, loss = 0.56422075\n",
      "Iteration 50, loss = 0.56260585\n",
      "Iteration 51, loss = 0.56932604\n",
      "Iteration 52, loss = 0.57207810\n",
      "Iteration 53, loss = 0.56835235\n",
      "Iteration 54, loss = 0.56403244\n",
      "Iteration 55, loss = 0.56425275\n",
      "Iteration 56, loss = 0.56376345\n",
      "Iteration 57, loss = 0.56172234\n",
      "Iteration 58, loss = 0.55927561\n",
      "Iteration 59, loss = 0.56654112\n",
      "Iteration 60, loss = 0.56127238\n",
      "Iteration 61, loss = 0.55872071\n",
      "Iteration 62, loss = 0.55815476\n",
      "Iteration 63, loss = 0.56043035\n",
      "Iteration 64, loss = 0.55924329\n",
      "Iteration 65, loss = 0.55907546\n",
      "Iteration 66, loss = 0.55760913\n",
      "Iteration 67, loss = 0.55881372\n",
      "Iteration 68, loss = 0.55828237\n",
      "Iteration 69, loss = 0.55746752\n",
      "Iteration 70, loss = 0.55494029\n",
      "Iteration 71, loss = 0.55745310\n",
      "Iteration 72, loss = 0.55932046\n",
      "Iteration 73, loss = 0.56035922\n",
      "Iteration 74, loss = 0.55805409\n",
      "Iteration 75, loss = 0.55837342\n",
      "Iteration 76, loss = 0.55762588\n",
      "Iteration 77, loss = 0.56044404\n",
      "Iteration 78, loss = 0.55429267\n",
      "Iteration 79, loss = 0.55718624\n",
      "Iteration 80, loss = 0.55365615\n",
      "Iteration 81, loss = 0.55483650\n",
      "Iteration 82, loss = 0.55328631\n",
      "Iteration 83, loss = 0.55335567\n",
      "Iteration 84, loss = 0.55823655\n",
      "Iteration 85, loss = 0.56027040\n",
      "Iteration 86, loss = 0.55740916\n",
      "Iteration 87, loss = 0.55376833\n",
      "Iteration 88, loss = 0.55100721\n",
      "Iteration 89, loss = 0.55658514\n",
      "Iteration 90, loss = 0.55360188\n",
      "Iteration 91, loss = 0.55457959\n",
      "Iteration 92, loss = 0.55215900\n",
      "Iteration 93, loss = 0.55281891\n",
      "Iteration 94, loss = 0.55854629\n",
      "Iteration 95, loss = 0.55238292\n",
      "Iteration 96, loss = 0.55671269\n",
      "Iteration 97, loss = 0.55054606\n",
      "Iteration 98, loss = 0.55733004\n",
      "Iteration 99, loss = 0.54926626\n",
      "Iteration 100, loss = 0.54912669\n",
      "Iteration 101, loss = 0.55566948\n",
      "Iteration 102, loss = 0.55159905\n",
      "Iteration 103, loss = 0.54864837\n",
      "Iteration 104, loss = 0.55798265\n",
      "Iteration 105, loss = 0.55018341\n",
      "Iteration 106, loss = 0.54940490\n",
      "Iteration 107, loss = 0.55142598\n",
      "Iteration 108, loss = 0.54832494\n",
      "Iteration 109, loss = 0.54808537\n",
      "Iteration 110, loss = 0.55817581\n",
      "Iteration 111, loss = 0.55519203\n",
      "Iteration 112, loss = 0.54839193\n",
      "Iteration 113, loss = 0.54747806\n",
      "Iteration 114, loss = 0.54553296\n",
      "Iteration 115, loss = 0.54964618\n",
      "Iteration 116, loss = 0.54892518\n",
      "Iteration 117, loss = 0.54524627\n",
      "Iteration 118, loss = 0.54533105\n",
      "Iteration 119, loss = 0.55091512\n",
      "Iteration 120, loss = 0.55179993\n",
      "Iteration 121, loss = 0.54837503\n",
      "Iteration 122, loss = 0.54588007\n",
      "Iteration 123, loss = 0.54565158\n",
      "Iteration 124, loss = 0.54526524\n",
      "Iteration 125, loss = 0.54397713\n",
      "Iteration 126, loss = 0.55023881\n",
      "Iteration 127, loss = 0.54884796\n",
      "Iteration 128, loss = 0.55368212\n",
      "Iteration 129, loss = 0.55174779\n",
      "Iteration 130, loss = 0.54835063\n",
      "Iteration 131, loss = 0.54529926\n",
      "Iteration 132, loss = 0.54703148\n",
      "Iteration 133, loss = 0.54976871\n",
      "Iteration 134, loss = 0.54656582\n",
      "Iteration 135, loss = 0.55338858\n",
      "Iteration 136, loss = 0.54493379\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67393757\n",
      "Iteration 2, loss = 0.62882103\n",
      "Iteration 3, loss = 0.61747395\n",
      "Iteration 4, loss = 0.60841351\n",
      "Iteration 5, loss = 0.60395018\n",
      "Iteration 6, loss = 0.60567040\n",
      "Iteration 7, loss = 0.59697832\n",
      "Iteration 8, loss = 0.59681320\n",
      "Iteration 9, loss = 0.60028719\n",
      "Iteration 10, loss = 0.60326136\n",
      "Iteration 11, loss = 0.59736258\n",
      "Iteration 12, loss = 0.59459932\n",
      "Iteration 13, loss = 0.60189400\n",
      "Iteration 14, loss = 0.59595493\n",
      "Iteration 15, loss = 0.58702881\n",
      "Iteration 16, loss = 0.59008438\n",
      "Iteration 17, loss = 0.59122839\n",
      "Iteration 18, loss = 0.58593064\n",
      "Iteration 19, loss = 0.58381354\n",
      "Iteration 20, loss = 0.59572545\n",
      "Iteration 21, loss = 0.58510699\n",
      "Iteration 22, loss = 0.58670449\n",
      "Iteration 23, loss = 0.59290460\n",
      "Iteration 24, loss = 0.57873450\n",
      "Iteration 25, loss = 0.58068590\n",
      "Iteration 26, loss = 0.57926933\n",
      "Iteration 27, loss = 0.57850023\n",
      "Iteration 28, loss = 0.57895431\n",
      "Iteration 29, loss = 0.57685307\n",
      "Iteration 30, loss = 0.57593276\n",
      "Iteration 31, loss = 0.57323472\n",
      "Iteration 32, loss = 0.57878244\n",
      "Iteration 33, loss = 0.57825306\n",
      "Iteration 34, loss = 0.57619091\n",
      "Iteration 35, loss = 0.58057856\n",
      "Iteration 36, loss = 0.57713741\n",
      "Iteration 37, loss = 0.57494392\n",
      "Iteration 38, loss = 0.57397416\n",
      "Iteration 39, loss = 0.57066812\n",
      "Iteration 40, loss = 0.58001561\n",
      "Iteration 41, loss = 0.57190761\n",
      "Iteration 42, loss = 0.57038232\n",
      "Iteration 43, loss = 0.57196801\n",
      "Iteration 44, loss = 0.57068372\n",
      "Iteration 45, loss = 0.56926775\n",
      "Iteration 46, loss = 0.57192663\n",
      "Iteration 47, loss = 0.57491033\n",
      "Iteration 48, loss = 0.56727781\n",
      "Iteration 49, loss = 0.56818623\n",
      "Iteration 50, loss = 0.57068376\n",
      "Iteration 51, loss = 0.56744338\n",
      "Iteration 52, loss = 0.56628331\n",
      "Iteration 53, loss = 0.56777781\n",
      "Iteration 54, loss = 0.56533157\n",
      "Iteration 55, loss = 0.56980715\n",
      "Iteration 56, loss = 0.56836233\n",
      "Iteration 57, loss = 0.57111328\n",
      "Iteration 58, loss = 0.57520631\n",
      "Iteration 59, loss = 0.56869892\n",
      "Iteration 60, loss = 0.56921007\n",
      "Iteration 61, loss = 0.56671376\n",
      "Iteration 62, loss = 0.56575958\n",
      "Iteration 63, loss = 0.56208804\n",
      "Iteration 64, loss = 0.56944761\n",
      "Iteration 65, loss = 0.56529552\n",
      "Iteration 66, loss = 0.56084838\n",
      "Iteration 67, loss = 0.56723246\n",
      "Iteration 68, loss = 0.56489292\n",
      "Iteration 69, loss = 0.56233444\n",
      "Iteration 70, loss = 0.56381261\n",
      "Iteration 71, loss = 0.56164894\n",
      "Iteration 72, loss = 0.56101740\n",
      "Iteration 73, loss = 0.56257917\n",
      "Iteration 74, loss = 0.56357397\n",
      "Iteration 75, loss = 0.56125397\n",
      "Iteration 76, loss = 0.55895775\n",
      "Iteration 77, loss = 0.55891206\n",
      "Iteration 78, loss = 0.56121835\n",
      "Iteration 79, loss = 0.56271064\n",
      "Iteration 80, loss = 0.56203911\n",
      "Iteration 81, loss = 0.56312039\n",
      "Iteration 82, loss = 0.55973399\n",
      "Iteration 83, loss = 0.55649364\n",
      "Iteration 84, loss = 0.55559329\n",
      "Iteration 85, loss = 0.55643740\n",
      "Iteration 86, loss = 0.56148358\n",
      "Iteration 87, loss = 0.56104998\n",
      "Iteration 88, loss = 0.55540345\n",
      "Iteration 89, loss = 0.55363101\n",
      "Iteration 90, loss = 0.55948402\n",
      "Iteration 91, loss = 0.55531046\n",
      "Iteration 92, loss = 0.55911920\n",
      "Iteration 93, loss = 0.56147788\n",
      "Iteration 94, loss = 0.55566057\n",
      "Iteration 95, loss = 0.55355838\n",
      "Iteration 96, loss = 0.55127770\n",
      "Iteration 97, loss = 0.55432146\n",
      "Iteration 98, loss = 0.55416892\n",
      "Iteration 99, loss = 0.55375284\n",
      "Iteration 100, loss = 0.55270707\n",
      "Iteration 101, loss = 0.55107710\n",
      "Iteration 102, loss = 0.55471890\n",
      "Iteration 103, loss = 0.55225129\n",
      "Iteration 104, loss = 0.55162898\n",
      "Iteration 105, loss = 0.55104998\n",
      "Iteration 106, loss = 0.54934507\n",
      "Iteration 107, loss = 0.55408025\n",
      "Iteration 108, loss = 0.55076446\n",
      "Iteration 109, loss = 0.55334082\n",
      "Iteration 110, loss = 0.55083798\n",
      "Iteration 111, loss = 0.55357795\n",
      "Iteration 112, loss = 0.56045784\n",
      "Iteration 113, loss = 0.55064092\n",
      "Iteration 114, loss = 0.54972089\n",
      "Iteration 115, loss = 0.55395193\n",
      "Iteration 116, loss = 0.56110306\n",
      "Iteration 117, loss = 0.55225685\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66471544\n",
      "Iteration 2, loss = 0.62356137\n",
      "Iteration 3, loss = 0.61400046\n",
      "Iteration 4, loss = 0.61195556\n",
      "Iteration 5, loss = 0.61165931\n",
      "Iteration 6, loss = 0.60603345\n",
      "Iteration 7, loss = 0.59735247\n",
      "Iteration 8, loss = 0.59424250\n",
      "Iteration 9, loss = 0.59531100\n",
      "Iteration 10, loss = 0.59306175\n",
      "Iteration 11, loss = 0.59049585\n",
      "Iteration 12, loss = 0.59134618\n",
      "Iteration 13, loss = 0.58883202\n",
      "Iteration 14, loss = 0.59073148\n",
      "Iteration 15, loss = 0.59211418\n",
      "Iteration 16, loss = 0.58720887\n",
      "Iteration 17, loss = 0.58929301\n",
      "Iteration 18, loss = 0.58570719\n",
      "Iteration 19, loss = 0.58303714\n",
      "Iteration 20, loss = 0.58658329\n",
      "Iteration 21, loss = 0.58727279\n",
      "Iteration 22, loss = 0.57961179\n",
      "Iteration 23, loss = 0.57859412\n",
      "Iteration 24, loss = 0.58368388\n",
      "Iteration 25, loss = 0.57921316\n",
      "Iteration 26, loss = 0.57763281\n",
      "Iteration 27, loss = 0.58043977\n",
      "Iteration 28, loss = 0.57374361\n",
      "Iteration 29, loss = 0.57426456\n",
      "Iteration 30, loss = 0.57620207\n",
      "Iteration 31, loss = 0.57595978\n",
      "Iteration 32, loss = 0.57284252\n",
      "Iteration 33, loss = 0.57558047\n",
      "Iteration 34, loss = 0.57529676\n",
      "Iteration 35, loss = 0.57692129\n",
      "Iteration 36, loss = 0.57421285\n",
      "Iteration 37, loss = 0.57015332\n",
      "Iteration 38, loss = 0.57480828\n",
      "Iteration 39, loss = 0.57540626\n",
      "Iteration 40, loss = 0.57057686\n",
      "Iteration 41, loss = 0.57074496\n",
      "Iteration 42, loss = 0.56878489\n",
      "Iteration 43, loss = 0.56810766\n",
      "Iteration 44, loss = 0.56776415\n",
      "Iteration 45, loss = 0.56960153\n",
      "Iteration 46, loss = 0.56765047\n",
      "Iteration 47, loss = 0.57549020\n",
      "Iteration 48, loss = 0.56779856\n",
      "Iteration 49, loss = 0.56664667\n",
      "Iteration 50, loss = 0.57229924\n",
      "Iteration 51, loss = 0.56663367\n",
      "Iteration 52, loss = 0.56350592\n",
      "Iteration 53, loss = 0.56655453\n",
      "Iteration 54, loss = 0.56653007\n",
      "Iteration 55, loss = 0.56246168\n",
      "Iteration 56, loss = 0.56830356\n",
      "Iteration 57, loss = 0.56603698\n",
      "Iteration 58, loss = 0.56578703\n",
      "Iteration 59, loss = 0.56204244\n",
      "Iteration 60, loss = 0.57300357\n",
      "Iteration 61, loss = 0.56146310\n",
      "Iteration 62, loss = 0.56214710\n",
      "Iteration 63, loss = 0.56307330\n",
      "Iteration 64, loss = 0.56514059\n",
      "Iteration 65, loss = 0.56295022\n",
      "Iteration 66, loss = 0.56319766\n",
      "Iteration 67, loss = 0.55909284\n",
      "Iteration 68, loss = 0.56105238\n",
      "Iteration 69, loss = 0.56277012\n",
      "Iteration 70, loss = 0.55862799\n",
      "Iteration 71, loss = 0.56068469\n",
      "Iteration 72, loss = 0.55663661\n",
      "Iteration 73, loss = 0.55711689\n",
      "Iteration 74, loss = 0.55734534\n",
      "Iteration 75, loss = 0.55825087\n",
      "Iteration 76, loss = 0.55958991\n",
      "Iteration 77, loss = 0.55691636\n",
      "Iteration 78, loss = 0.56094652\n",
      "Iteration 79, loss = 0.55720128\n",
      "Iteration 80, loss = 0.55590608\n",
      "Iteration 81, loss = 0.56386492\n",
      "Iteration 82, loss = 0.55917198\n",
      "Iteration 83, loss = 0.55452865\n",
      "Iteration 84, loss = 0.55516052\n",
      "Iteration 85, loss = 0.55950629\n",
      "Iteration 86, loss = 0.55415809\n",
      "Iteration 87, loss = 0.55295159\n",
      "Iteration 88, loss = 0.55879336\n",
      "Iteration 89, loss = 0.55581939\n",
      "Iteration 90, loss = 0.55319241\n",
      "Iteration 91, loss = 0.55345184\n",
      "Iteration 92, loss = 0.55607156\n",
      "Iteration 93, loss = 0.55339462\n",
      "Iteration 94, loss = 0.55674840\n",
      "Iteration 95, loss = 0.55829952\n",
      "Iteration 96, loss = 0.56025656\n",
      "Iteration 97, loss = 0.55466764\n",
      "Iteration 98, loss = 0.55142552\n",
      "Iteration 99, loss = 0.55911936\n",
      "Iteration 100, loss = 0.55211102\n",
      "Iteration 101, loss = 0.55129729\n",
      "Iteration 102, loss = 0.55200348\n",
      "Iteration 103, loss = 0.55197214\n",
      "Iteration 104, loss = 0.55270767\n",
      "Iteration 105, loss = 0.55523860\n",
      "Iteration 106, loss = 0.55464070\n",
      "Iteration 107, loss = 0.54877711\n",
      "Iteration 108, loss = 0.54865701\n",
      "Iteration 109, loss = 0.55008281\n",
      "Iteration 110, loss = 0.54873943\n",
      "Iteration 111, loss = 0.54835166\n",
      "Iteration 112, loss = 0.54975746\n",
      "Iteration 113, loss = 0.55286443\n",
      "Iteration 114, loss = 0.55795396\n",
      "Iteration 115, loss = 0.54871953\n",
      "Iteration 116, loss = 0.54861271\n",
      "Iteration 117, loss = 0.55105661\n",
      "Iteration 118, loss = 0.55274004\n",
      "Iteration 119, loss = 0.55115956\n",
      "Iteration 120, loss = 0.55338406\n",
      "Iteration 121, loss = 0.55322189\n",
      "Iteration 122, loss = 0.55400698\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67739445\n",
      "Iteration 2, loss = 0.62484025\n",
      "Iteration 3, loss = 0.61996712\n",
      "Iteration 4, loss = 0.60691122\n",
      "Iteration 5, loss = 0.60252729\n",
      "Iteration 6, loss = 0.60430232\n",
      "Iteration 7, loss = 0.60170129\n",
      "Iteration 8, loss = 0.59857976\n",
      "Iteration 9, loss = 0.59682646\n",
      "Iteration 10, loss = 0.59181433\n",
      "Iteration 11, loss = 0.59241104\n",
      "Iteration 12, loss = 0.58850088\n",
      "Iteration 13, loss = 0.58799823\n",
      "Iteration 14, loss = 0.58763338\n",
      "Iteration 15, loss = 0.59172043\n",
      "Iteration 16, loss = 0.59722927\n",
      "Iteration 17, loss = 0.59812801\n",
      "Iteration 18, loss = 0.58930330\n",
      "Iteration 19, loss = 0.58707292\n",
      "Iteration 20, loss = 0.58198038\n",
      "Iteration 21, loss = 0.58753216\n",
      "Iteration 22, loss = 0.58810292\n",
      "Iteration 23, loss = 0.58274854\n",
      "Iteration 24, loss = 0.58374811\n",
      "Iteration 25, loss = 0.58424478\n",
      "Iteration 26, loss = 0.58041981\n",
      "Iteration 27, loss = 0.57945249\n",
      "Iteration 28, loss = 0.57835120\n",
      "Iteration 29, loss = 0.57758720\n",
      "Iteration 30, loss = 0.57634393\n",
      "Iteration 31, loss = 0.58223044\n",
      "Iteration 32, loss = 0.57690467\n",
      "Iteration 33, loss = 0.57334883\n",
      "Iteration 34, loss = 0.57527712\n",
      "Iteration 35, loss = 0.57532960\n",
      "Iteration 36, loss = 0.58071123\n",
      "Iteration 37, loss = 0.57348978\n",
      "Iteration 38, loss = 0.57670165\n",
      "Iteration 39, loss = 0.57908336\n",
      "Iteration 40, loss = 0.57809923\n",
      "Iteration 41, loss = 0.57131665\n",
      "Iteration 42, loss = 0.57156230\n",
      "Iteration 43, loss = 0.57072778\n",
      "Iteration 44, loss = 0.58021698\n",
      "Iteration 45, loss = 0.57250399\n",
      "Iteration 46, loss = 0.57244788\n",
      "Iteration 47, loss = 0.56863025\n",
      "Iteration 48, loss = 0.57705598\n",
      "Iteration 49, loss = 0.57345233\n",
      "Iteration 50, loss = 0.56662469\n",
      "Iteration 51, loss = 0.56726601\n",
      "Iteration 52, loss = 0.56440967\n",
      "Iteration 53, loss = 0.56425819\n",
      "Iteration 54, loss = 0.56919459\n",
      "Iteration 55, loss = 0.56456259\n",
      "Iteration 56, loss = 0.56684907\n",
      "Iteration 57, loss = 0.56647398\n",
      "Iteration 58, loss = 0.56924310\n",
      "Iteration 59, loss = 0.56960104\n",
      "Iteration 60, loss = 0.56912191\n",
      "Iteration 61, loss = 0.56520785\n",
      "Iteration 62, loss = 0.56808934\n",
      "Iteration 63, loss = 0.56321892\n",
      "Iteration 64, loss = 0.56116271\n",
      "Iteration 65, loss = 0.56579541\n",
      "Iteration 66, loss = 0.56189811\n",
      "Iteration 67, loss = 0.56390123\n",
      "Iteration 68, loss = 0.56623694\n",
      "Iteration 69, loss = 0.56699044\n",
      "Iteration 70, loss = 0.56179456\n",
      "Iteration 71, loss = 0.56216313\n",
      "Iteration 72, loss = 0.56877653\n",
      "Iteration 73, loss = 0.56297392\n",
      "Iteration 74, loss = 0.55872605\n",
      "Iteration 75, loss = 0.55841358\n",
      "Iteration 76, loss = 0.56151677\n",
      "Iteration 77, loss = 0.56731913\n",
      "Iteration 78, loss = 0.56038929\n",
      "Iteration 79, loss = 0.56009008\n",
      "Iteration 80, loss = 0.55664023\n",
      "Iteration 81, loss = 0.55753888\n",
      "Iteration 82, loss = 0.55790145\n",
      "Iteration 83, loss = 0.56102887\n",
      "Iteration 84, loss = 0.56057931\n",
      "Iteration 85, loss = 0.55912652\n",
      "Iteration 86, loss = 0.55538877\n",
      "Iteration 87, loss = 0.55378029\n",
      "Iteration 88, loss = 0.55915904\n",
      "Iteration 89, loss = 0.55670681\n",
      "Iteration 90, loss = 0.55399301\n",
      "Iteration 91, loss = 0.55546478\n",
      "Iteration 92, loss = 0.55271496\n",
      "Iteration 93, loss = 0.55511747\n",
      "Iteration 94, loss = 0.55286472\n",
      "Iteration 95, loss = 0.55303199\n",
      "Iteration 96, loss = 0.55655476\n",
      "Iteration 97, loss = 0.55443423\n",
      "Iteration 98, loss = 0.55270856\n",
      "Iteration 99, loss = 0.55128716\n",
      "Iteration 100, loss = 0.55674283\n",
      "Iteration 101, loss = 0.55366414\n",
      "Iteration 102, loss = 0.54972142\n",
      "Iteration 103, loss = 0.55237473\n",
      "Iteration 104, loss = 0.55505583\n",
      "Iteration 105, loss = 0.55195009\n",
      "Iteration 106, loss = 0.55092577\n",
      "Iteration 107, loss = 0.55175718\n",
      "Iteration 108, loss = 0.55132191\n",
      "Iteration 109, loss = 0.55134249\n",
      "Iteration 110, loss = 0.54854531\n",
      "Iteration 111, loss = 0.54907263\n",
      "Iteration 112, loss = 0.55234635\n",
      "Iteration 113, loss = 0.54882455\n",
      "Iteration 114, loss = 0.54745747\n",
      "Iteration 115, loss = 0.54966494\n",
      "Iteration 116, loss = 0.55052924\n",
      "Iteration 117, loss = 0.55536106\n",
      "Iteration 118, loss = 0.54831509\n",
      "Iteration 119, loss = 0.55315896\n",
      "Iteration 120, loss = 0.54972847\n",
      "Iteration 121, loss = 0.54569397\n",
      "Iteration 122, loss = 0.54575088\n",
      "Iteration 123, loss = 0.54486529\n",
      "Iteration 124, loss = 0.54513478\n",
      "Iteration 125, loss = 0.54416323\n",
      "Iteration 126, loss = 0.55064109\n",
      "Iteration 127, loss = 0.54660068\n",
      "Iteration 128, loss = 0.55077401\n",
      "Iteration 129, loss = 0.54884691\n",
      "Iteration 130, loss = 0.55048596\n",
      "Iteration 131, loss = 0.54456425\n",
      "Iteration 132, loss = 0.54341014\n",
      "Iteration 133, loss = 0.54488784\n",
      "Iteration 134, loss = 0.54593423\n",
      "Iteration 135, loss = 0.54211222\n",
      "Iteration 136, loss = 0.54476907\n",
      "Iteration 137, loss = 0.54640036\n",
      "Iteration 138, loss = 0.54758239\n",
      "Iteration 139, loss = 0.54416872\n",
      "Iteration 140, loss = 0.54724512\n",
      "Iteration 141, loss = 0.54372841\n",
      "Iteration 142, loss = 0.54046573\n",
      "Iteration 143, loss = 0.54235689\n",
      "Iteration 144, loss = 0.54414668\n",
      "Iteration 145, loss = 0.54676762\n",
      "Iteration 146, loss = 0.54530699\n",
      "Iteration 147, loss = 0.54656911\n",
      "Iteration 148, loss = 0.54759880\n",
      "Iteration 149, loss = 0.54472729\n",
      "Iteration 150, loss = 0.54189291\n",
      "Iteration 151, loss = 0.54374886\n",
      "Iteration 152, loss = 0.54476417\n",
      "Iteration 153, loss = 0.54133843\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66152046\n",
      "Iteration 2, loss = 0.62474441\n",
      "Iteration 3, loss = 0.62669995\n",
      "Iteration 4, loss = 0.61585753\n",
      "Iteration 5, loss = 0.60575378\n",
      "Iteration 6, loss = 0.60045585\n",
      "Iteration 7, loss = 0.60063335\n",
      "Iteration 8, loss = 0.59460933\n",
      "Iteration 9, loss = 0.59846938\n",
      "Iteration 10, loss = 0.59165382\n",
      "Iteration 11, loss = 0.59156377\n",
      "Iteration 12, loss = 0.61190424\n",
      "Iteration 13, loss = 0.59060321\n",
      "Iteration 14, loss = 0.59551402\n",
      "Iteration 15, loss = 0.58671762\n",
      "Iteration 16, loss = 0.58353337\n",
      "Iteration 17, loss = 0.58833158\n",
      "Iteration 18, loss = 0.58304897\n",
      "Iteration 19, loss = 0.58596982\n",
      "Iteration 20, loss = 0.57854295\n",
      "Iteration 21, loss = 0.58552783\n",
      "Iteration 22, loss = 0.58560657\n",
      "Iteration 23, loss = 0.57674052\n",
      "Iteration 24, loss = 0.57741508\n",
      "Iteration 25, loss = 0.57822340\n",
      "Iteration 26, loss = 0.57539008\n",
      "Iteration 27, loss = 0.57575086\n",
      "Iteration 28, loss = 0.57615514\n",
      "Iteration 29, loss = 0.57739375\n",
      "Iteration 30, loss = 0.57433517\n",
      "Iteration 31, loss = 0.57190329\n",
      "Iteration 32, loss = 0.57528643\n",
      "Iteration 33, loss = 0.57270073\n",
      "Iteration 34, loss = 0.57045339\n",
      "Iteration 35, loss = 0.57521703\n",
      "Iteration 36, loss = 0.56870885\n",
      "Iteration 37, loss = 0.56862378\n",
      "Iteration 38, loss = 0.56743838\n",
      "Iteration 39, loss = 0.56841118\n",
      "Iteration 40, loss = 0.56666800\n",
      "Iteration 41, loss = 0.56716172\n",
      "Iteration 42, loss = 0.56731511\n",
      "Iteration 43, loss = 0.56814867\n",
      "Iteration 44, loss = 0.56887633\n",
      "Iteration 45, loss = 0.56612127\n",
      "Iteration 46, loss = 0.56586714\n",
      "Iteration 47, loss = 0.56643520\n",
      "Iteration 48, loss = 0.56831044\n",
      "Iteration 49, loss = 0.56779410\n",
      "Iteration 50, loss = 0.56628494\n",
      "Iteration 51, loss = 0.56789755\n",
      "Iteration 52, loss = 0.56741102\n",
      "Iteration 53, loss = 0.56269627\n",
      "Iteration 54, loss = 0.56275869\n",
      "Iteration 55, loss = 0.56339552\n",
      "Iteration 56, loss = 0.56304883\n",
      "Iteration 57, loss = 0.56403893\n",
      "Iteration 58, loss = 0.56230648\n",
      "Iteration 59, loss = 0.56241228\n",
      "Iteration 60, loss = 0.56144492\n",
      "Iteration 61, loss = 0.55978464\n",
      "Iteration 62, loss = 0.56087140\n",
      "Iteration 63, loss = 0.56069828\n",
      "Iteration 64, loss = 0.56031258\n",
      "Iteration 65, loss = 0.56850126\n",
      "Iteration 66, loss = 0.56069074\n",
      "Iteration 67, loss = 0.55802994\n",
      "Iteration 68, loss = 0.55843348\n",
      "Iteration 69, loss = 0.55532575\n",
      "Iteration 70, loss = 0.55554943\n",
      "Iteration 71, loss = 0.55816525\n",
      "Iteration 72, loss = 0.56069366\n",
      "Iteration 73, loss = 0.55832399\n",
      "Iteration 74, loss = 0.55853187\n",
      "Iteration 75, loss = 0.55985781\n",
      "Iteration 76, loss = 0.55511543\n",
      "Iteration 77, loss = 0.55882078\n",
      "Iteration 78, loss = 0.55657380\n",
      "Iteration 79, loss = 0.55469045\n",
      "Iteration 80, loss = 0.55384261\n",
      "Iteration 81, loss = 0.55941534\n",
      "Iteration 82, loss = 0.55772104\n",
      "Iteration 83, loss = 0.55576836\n",
      "Iteration 84, loss = 0.55308673\n",
      "Iteration 85, loss = 0.55632531\n",
      "Iteration 86, loss = 0.55988585\n",
      "Iteration 87, loss = 0.55413629\n",
      "Iteration 88, loss = 0.55556304\n",
      "Iteration 89, loss = 0.55102346\n",
      "Iteration 90, loss = 0.55878743\n",
      "Iteration 91, loss = 0.55572909\n",
      "Iteration 92, loss = 0.55234918\n",
      "Iteration 93, loss = 0.55357000\n",
      "Iteration 94, loss = 0.55752640\n",
      "Iteration 95, loss = 0.55000918\n",
      "Iteration 96, loss = 0.55059686\n",
      "Iteration 97, loss = 0.55080683\n",
      "Iteration 98, loss = 0.55944782\n",
      "Iteration 99, loss = 0.55308367\n",
      "Iteration 100, loss = 0.55201816\n",
      "Iteration 101, loss = 0.54960262\n",
      "Iteration 102, loss = 0.55033418\n",
      "Iteration 103, loss = 0.54912309\n",
      "Iteration 104, loss = 0.54850235\n",
      "Iteration 105, loss = 0.54911967\n",
      "Iteration 106, loss = 0.55625956\n",
      "Iteration 107, loss = 0.55570459\n",
      "Iteration 108, loss = 0.54981236\n",
      "Iteration 109, loss = 0.55416814\n",
      "Iteration 110, loss = 0.55152585\n",
      "Iteration 111, loss = 0.55069107\n",
      "Iteration 112, loss = 0.54864745\n",
      "Iteration 113, loss = 0.55422546\n",
      "Iteration 114, loss = 0.55366037\n",
      "Iteration 115, loss = 0.55046917\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68416608\n",
      "Iteration 2, loss = 0.63276344\n",
      "Iteration 3, loss = 0.62212860\n",
      "Iteration 4, loss = 0.60874903\n",
      "Iteration 5, loss = 0.60530414\n",
      "Iteration 6, loss = 0.60135318\n",
      "Iteration 7, loss = 0.60866833\n",
      "Iteration 8, loss = 0.60214076\n",
      "Iteration 9, loss = 0.59327113\n",
      "Iteration 10, loss = 0.59258994\n",
      "Iteration 11, loss = 0.59123689\n",
      "Iteration 12, loss = 0.58988695\n",
      "Iteration 13, loss = 0.58786712\n",
      "Iteration 14, loss = 0.58609533\n",
      "Iteration 15, loss = 0.58480496\n",
      "Iteration 16, loss = 0.58464322\n",
      "Iteration 17, loss = 0.59444689\n",
      "Iteration 18, loss = 0.58655193\n",
      "Iteration 19, loss = 0.58276478\n",
      "Iteration 20, loss = 0.58145970\n",
      "Iteration 21, loss = 0.58451473\n",
      "Iteration 22, loss = 0.58099496\n",
      "Iteration 23, loss = 0.57978936\n",
      "Iteration 24, loss = 0.58043074\n",
      "Iteration 25, loss = 0.57866347\n",
      "Iteration 26, loss = 0.57666297\n",
      "Iteration 27, loss = 0.57525108\n",
      "Iteration 28, loss = 0.57530327\n",
      "Iteration 29, loss = 0.57275595\n",
      "Iteration 30, loss = 0.58696834\n",
      "Iteration 31, loss = 0.57659634\n",
      "Iteration 32, loss = 0.57581209\n",
      "Iteration 33, loss = 0.57329804\n",
      "Iteration 34, loss = 0.57581857\n",
      "Iteration 35, loss = 0.57262247\n",
      "Iteration 36, loss = 0.57331437\n",
      "Iteration 37, loss = 0.57368419\n",
      "Iteration 38, loss = 0.57220272\n",
      "Iteration 39, loss = 0.57022395\n",
      "Iteration 40, loss = 0.58300577\n",
      "Iteration 41, loss = 0.57083438\n",
      "Iteration 42, loss = 0.57041625\n",
      "Iteration 43, loss = 0.56697702\n",
      "Iteration 44, loss = 0.56762436\n",
      "Iteration 45, loss = 0.57091422\n",
      "Iteration 46, loss = 0.56676450\n",
      "Iteration 47, loss = 0.56748811\n",
      "Iteration 48, loss = 0.56695622\n",
      "Iteration 49, loss = 0.56488752\n",
      "Iteration 50, loss = 0.56568969\n",
      "Iteration 51, loss = 0.56493703\n",
      "Iteration 52, loss = 0.56607431\n",
      "Iteration 53, loss = 0.56581984\n",
      "Iteration 54, loss = 0.56879156\n",
      "Iteration 55, loss = 0.56577153\n",
      "Iteration 56, loss = 0.56317305\n",
      "Iteration 57, loss = 0.56333152\n",
      "Iteration 58, loss = 0.56796814\n",
      "Iteration 59, loss = 0.57979414\n",
      "Iteration 60, loss = 0.56352330\n",
      "Iteration 61, loss = 0.56201791\n",
      "Iteration 62, loss = 0.56160616\n",
      "Iteration 63, loss = 0.56181299\n",
      "Iteration 64, loss = 0.55921317\n",
      "Iteration 65, loss = 0.56078708\n",
      "Iteration 66, loss = 0.56345960\n",
      "Iteration 67, loss = 0.56891167\n",
      "Iteration 68, loss = 0.56282663\n",
      "Iteration 69, loss = 0.56507184\n",
      "Iteration 70, loss = 0.56129484\n",
      "Iteration 71, loss = 0.55849426\n",
      "Iteration 72, loss = 0.55826548\n",
      "Iteration 73, loss = 0.55867776\n",
      "Iteration 74, loss = 0.55888341\n",
      "Iteration 75, loss = 0.55779933\n",
      "Iteration 76, loss = 0.55948985\n",
      "Iteration 77, loss = 0.55820734\n",
      "Iteration 78, loss = 0.55721995\n",
      "Iteration 79, loss = 0.56276480\n",
      "Iteration 80, loss = 0.55616473\n",
      "Iteration 81, loss = 0.55793822\n",
      "Iteration 82, loss = 0.55741825\n",
      "Iteration 83, loss = 0.56422096\n",
      "Iteration 84, loss = 0.55825058\n",
      "Iteration 85, loss = 0.55866914\n",
      "Iteration 86, loss = 0.55392319\n",
      "Iteration 87, loss = 0.55334707\n",
      "Iteration 88, loss = 0.55441401\n",
      "Iteration 89, loss = 0.55597758\n",
      "Iteration 90, loss = 0.55744789\n",
      "Iteration 91, loss = 0.55765364\n",
      "Iteration 92, loss = 0.55745005\n",
      "Iteration 93, loss = 0.55846049\n",
      "Iteration 94, loss = 0.55696892\n",
      "Iteration 95, loss = 0.55591390\n",
      "Iteration 96, loss = 0.55470693\n",
      "Iteration 97, loss = 0.55925472\n",
      "Iteration 98, loss = 0.55450928\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66089102\n",
      "Iteration 2, loss = 0.63149618\n",
      "Iteration 3, loss = 0.61316551\n",
      "Iteration 4, loss = 0.60801080\n",
      "Iteration 5, loss = 0.60467563\n",
      "Iteration 6, loss = 0.60485212\n",
      "Iteration 7, loss = 0.59805623\n",
      "Iteration 8, loss = 0.59590335\n",
      "Iteration 9, loss = 0.59655810\n",
      "Iteration 10, loss = 0.59847361\n",
      "Iteration 11, loss = 0.59143137\n",
      "Iteration 12, loss = 0.59659900\n",
      "Iteration 13, loss = 0.59494273\n",
      "Iteration 14, loss = 0.58634773\n",
      "Iteration 15, loss = 0.58874735\n",
      "Iteration 16, loss = 0.59147587\n",
      "Iteration 17, loss = 0.58467784\n",
      "Iteration 18, loss = 0.58212401\n",
      "Iteration 19, loss = 0.58142922\n",
      "Iteration 20, loss = 0.58746441\n",
      "Iteration 21, loss = 0.58512331\n",
      "Iteration 22, loss = 0.58809193\n",
      "Iteration 23, loss = 0.57922325\n",
      "Iteration 24, loss = 0.58106225\n",
      "Iteration 25, loss = 0.57792010\n",
      "Iteration 26, loss = 0.58950640\n",
      "Iteration 27, loss = 0.57928647\n",
      "Iteration 28, loss = 0.57974706\n",
      "Iteration 29, loss = 0.58252965\n",
      "Iteration 30, loss = 0.57672303\n",
      "Iteration 31, loss = 0.57979782\n",
      "Iteration 32, loss = 0.57851624\n",
      "Iteration 33, loss = 0.57545737\n",
      "Iteration 34, loss = 0.57778665\n",
      "Iteration 35, loss = 0.57227728\n",
      "Iteration 36, loss = 0.57153163\n",
      "Iteration 37, loss = 0.57139085\n",
      "Iteration 38, loss = 0.57794748\n",
      "Iteration 39, loss = 0.57341800\n",
      "Iteration 40, loss = 0.57331269\n",
      "Iteration 41, loss = 0.57353431\n",
      "Iteration 42, loss = 0.57138301\n",
      "Iteration 43, loss = 0.57093302\n",
      "Iteration 44, loss = 0.57376280\n",
      "Iteration 45, loss = 0.56978098\n",
      "Iteration 46, loss = 0.56719159\n",
      "Iteration 47, loss = 0.56777292\n",
      "Iteration 48, loss = 0.56592705\n",
      "Iteration 49, loss = 0.56722345\n",
      "Iteration 50, loss = 0.56758055\n",
      "Iteration 51, loss = 0.56453311\n",
      "Iteration 52, loss = 0.56760843\n",
      "Iteration 53, loss = 0.56421636\n",
      "Iteration 54, loss = 0.56795610\n",
      "Iteration 55, loss = 0.56828973\n",
      "Iteration 56, loss = 0.56196269\n",
      "Iteration 57, loss = 0.56898135\n",
      "Iteration 58, loss = 0.56432608\n",
      "Iteration 59, loss = 0.56605016\n",
      "Iteration 60, loss = 0.56470611\n",
      "Iteration 61, loss = 0.56147439\n",
      "Iteration 62, loss = 0.56170343\n",
      "Iteration 63, loss = 0.56055631\n",
      "Iteration 64, loss = 0.56243430\n",
      "Iteration 65, loss = 0.56475463\n",
      "Iteration 66, loss = 0.56028415\n",
      "Iteration 67, loss = 0.56198362\n",
      "Iteration 68, loss = 0.55964161\n",
      "Iteration 69, loss = 0.55848029\n",
      "Iteration 70, loss = 0.55974686\n",
      "Iteration 71, loss = 0.56053929\n",
      "Iteration 72, loss = 0.56423883\n",
      "Iteration 73, loss = 0.56060691\n",
      "Iteration 74, loss = 0.55791356\n",
      "Iteration 75, loss = 0.55824220\n",
      "Iteration 76, loss = 0.55950724\n",
      "Iteration 77, loss = 0.55795661\n",
      "Iteration 78, loss = 0.56121669\n",
      "Iteration 79, loss = 0.55830260\n",
      "Iteration 80, loss = 0.55656350\n",
      "Iteration 81, loss = 0.55726705\n",
      "Iteration 82, loss = 0.55621567\n",
      "Iteration 83, loss = 0.55627594\n",
      "Iteration 84, loss = 0.55384778\n",
      "Iteration 85, loss = 0.55552567\n",
      "Iteration 86, loss = 0.55810318\n",
      "Iteration 87, loss = 0.55828266\n",
      "Iteration 88, loss = 0.55731919\n",
      "Iteration 89, loss = 0.55550178\n",
      "Iteration 90, loss = 0.56194745\n",
      "Iteration 91, loss = 0.55409658\n",
      "Iteration 92, loss = 0.55748025\n",
      "Iteration 93, loss = 0.56833811\n",
      "Iteration 94, loss = 0.55791911\n",
      "Iteration 95, loss = 0.55442523\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67836506\n",
      "Iteration 2, loss = 0.61638015\n",
      "Iteration 3, loss = 0.60513003\n",
      "Iteration 4, loss = 0.59660134\n",
      "Iteration 5, loss = 0.59662323\n",
      "Iteration 6, loss = 0.58872669\n",
      "Iteration 7, loss = 0.59734089\n",
      "Iteration 8, loss = 0.59228544\n",
      "Iteration 9, loss = 0.58569867\n",
      "Iteration 10, loss = 0.59317187\n",
      "Iteration 11, loss = 0.58668516\n",
      "Iteration 12, loss = 0.59154122\n",
      "Iteration 13, loss = 0.58084208\n",
      "Iteration 14, loss = 0.58099648\n",
      "Iteration 15, loss = 0.57806334\n",
      "Iteration 16, loss = 0.58344400\n",
      "Iteration 17, loss = 0.58093010\n",
      "Iteration 18, loss = 0.57841019\n",
      "Iteration 19, loss = 0.57599131\n",
      "Iteration 20, loss = 0.57610930\n",
      "Iteration 21, loss = 0.57646289\n",
      "Iteration 22, loss = 0.57287812\n",
      "Iteration 23, loss = 0.57332759\n",
      "Iteration 24, loss = 0.57530423\n",
      "Iteration 25, loss = 0.57182259\n",
      "Iteration 26, loss = 0.57204470\n",
      "Iteration 27, loss = 0.57132173\n",
      "Iteration 28, loss = 0.57577769\n",
      "Iteration 29, loss = 0.56678845\n",
      "Iteration 30, loss = 0.57222611\n",
      "Iteration 31, loss = 0.57242522\n",
      "Iteration 32, loss = 0.56898125\n",
      "Iteration 33, loss = 0.57404855\n",
      "Iteration 34, loss = 0.56509620\n",
      "Iteration 35, loss = 0.56716138\n",
      "Iteration 36, loss = 0.56538115\n",
      "Iteration 37, loss = 0.56399090\n",
      "Iteration 38, loss = 0.56822709\n",
      "Iteration 39, loss = 0.56387440\n",
      "Iteration 40, loss = 0.56129506\n",
      "Iteration 41, loss = 0.56278459\n",
      "Iteration 42, loss = 0.56262730\n",
      "Iteration 43, loss = 0.56446054\n",
      "Iteration 44, loss = 0.55977685\n",
      "Iteration 45, loss = 0.56068796\n",
      "Iteration 46, loss = 0.56601026\n",
      "Iteration 47, loss = 0.57638212\n",
      "Iteration 48, loss = 0.56370731\n",
      "Iteration 49, loss = 0.56834972\n",
      "Iteration 50, loss = 0.55825410\n",
      "Iteration 51, loss = 0.56082334\n",
      "Iteration 52, loss = 0.55785770\n",
      "Iteration 53, loss = 0.56117698\n",
      "Iteration 54, loss = 0.55888989\n",
      "Iteration 55, loss = 0.55660471\n",
      "Iteration 56, loss = 0.55677359\n",
      "Iteration 57, loss = 0.55654033\n",
      "Iteration 58, loss = 0.55456058\n",
      "Iteration 59, loss = 0.55788482\n",
      "Iteration 60, loss = 0.55674381\n",
      "Iteration 61, loss = 0.55792114\n",
      "Iteration 62, loss = 0.55374823\n",
      "Iteration 63, loss = 0.55084030\n",
      "Iteration 64, loss = 0.55212083\n",
      "Iteration 65, loss = 0.55255684\n",
      "Iteration 66, loss = 0.55292435\n",
      "Iteration 67, loss = 0.55368723\n",
      "Iteration 68, loss = 0.55154418\n",
      "Iteration 69, loss = 0.55136275\n",
      "Iteration 70, loss = 0.55314539\n",
      "Iteration 71, loss = 0.55417398\n",
      "Iteration 72, loss = 0.54909891\n",
      "Iteration 73, loss = 0.55108135\n",
      "Iteration 74, loss = 0.55944002\n",
      "Iteration 75, loss = 0.55489275\n",
      "Iteration 76, loss = 0.54880351\n",
      "Iteration 77, loss = 0.54873355\n",
      "Iteration 78, loss = 0.54822618\n",
      "Iteration 79, loss = 0.55083150\n",
      "Iteration 80, loss = 0.54728939\n",
      "Iteration 81, loss = 0.54337397\n",
      "Iteration 82, loss = 0.54505531\n",
      "Iteration 83, loss = 0.55103122\n",
      "Iteration 84, loss = 0.54837003\n",
      "Iteration 85, loss = 0.54630625\n",
      "Iteration 86, loss = 0.54955560\n",
      "Iteration 87, loss = 0.54465636\n",
      "Iteration 88, loss = 0.54432264\n",
      "Iteration 89, loss = 0.54316560\n",
      "Iteration 90, loss = 0.54536309\n",
      "Iteration 91, loss = 0.54520327\n",
      "Iteration 92, loss = 0.54162513\n",
      "Iteration 93, loss = 0.54611348\n",
      "Iteration 94, loss = 0.54165857\n",
      "Iteration 95, loss = 0.54799826\n",
      "Iteration 96, loss = 0.54039389\n",
      "Iteration 97, loss = 0.54350517\n",
      "Iteration 98, loss = 0.54902416\n",
      "Iteration 99, loss = 0.54342228\n",
      "Iteration 100, loss = 0.53962510\n",
      "Iteration 101, loss = 0.54830019\n",
      "Iteration 102, loss = 0.54201223\n",
      "Iteration 103, loss = 0.54056191\n",
      "Iteration 104, loss = 0.54065740\n",
      "Iteration 105, loss = 0.54115530\n",
      "Iteration 106, loss = 0.53996391\n",
      "Iteration 107, loss = 0.54092830\n",
      "Iteration 108, loss = 0.54255066\n",
      "Iteration 109, loss = 0.54016163\n",
      "Iteration 110, loss = 0.54109129\n",
      "Iteration 111, loss = 0.53652868\n",
      "Iteration 112, loss = 0.53778681\n",
      "Iteration 113, loss = 0.54112775\n",
      "Iteration 114, loss = 0.54228474\n",
      "Iteration 115, loss = 0.54164026\n",
      "Iteration 116, loss = 0.54610285\n",
      "Iteration 117, loss = 0.54071465\n",
      "Iteration 118, loss = 0.53771675\n",
      "Iteration 119, loss = 0.53570445\n",
      "Iteration 120, loss = 0.53617269\n",
      "Iteration 121, loss = 0.53655755\n",
      "Iteration 122, loss = 0.53916881\n",
      "Iteration 123, loss = 0.53546508\n",
      "Iteration 124, loss = 0.53940525\n",
      "Iteration 125, loss = 0.53843719\n",
      "Iteration 126, loss = 0.53740418\n",
      "Iteration 127, loss = 0.54030944\n",
      "Iteration 128, loss = 0.53949079\n",
      "Iteration 129, loss = 0.54354313\n",
      "Iteration 130, loss = 0.53749286\n",
      "Iteration 131, loss = 0.54158816\n",
      "Iteration 132, loss = 0.53601784\n",
      "Iteration 133, loss = 0.53571780\n",
      "Iteration 134, loss = 0.53349313\n",
      "Iteration 135, loss = 0.53693376\n",
      "Iteration 136, loss = 0.53959871\n",
      "Iteration 137, loss = 0.53433133\n",
      "Iteration 138, loss = 0.53539778\n",
      "Iteration 139, loss = 0.53390805\n",
      "Iteration 140, loss = 0.53577520\n",
      "Iteration 141, loss = 0.53833404\n",
      "Iteration 142, loss = 0.53703263\n",
      "Iteration 143, loss = 0.53799706\n",
      "Iteration 144, loss = 0.53369831\n",
      "Iteration 145, loss = 0.53489490\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[0.705   0.71125 0.7225  0.7125  0.7025  0.73    0.72125 0.71375 0.74375\n",
      " 0.705  ]\n",
      "0.71675\n"
     ]
    }
   ],
   "source": [
    "summoner_cv_scores = cross_val_score(\n",
    "    estimator = pipeline_neuralnetwork,\n",
    "    X = X_train,\n",
    "    y = y_train,\n",
    "    cv = kfold\n",
    ")\n",
    "\n",
    "print(summoner_cv_scores)\n",
    "print(np.mean(summoner_cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['summonerLevel', 'championLevel','championPoints', \n",
    "             'Assassin', 'Fighter', 'Mage', 'Marksman', 'Support', 'Tank',\n",
    "            'info.attack', 'info.defense', 'info.magic', 'info.difficulty',\n",
    "            'stats.hpregenperlevel',\t'stats.mpregen', 'stats.mpregenperlevel',\t'stats.crit',\t'stats.critperlevel',\n",
    "            'stats.attackdamage', 'stats.attackdamageperlevel', 'stats.attackspeedperlevel',\t'stats.attackspeed',\n",
    "            'final_gold', 'final_xp', 'final_abilityhaste', 'final_abilitypower', 'final_armor', 'final_armorpen',\n",
    "            'final_armorpenpercent', 'final_atkdmg', 'final_bns_armorpenpercent', 'final_bns_magicpenpercent', 'final_ccreduction',\n",
    "            'final_cdreduction', 'final_remaining_health', 'final_health', 'final_healthrgn', 'final_lifesteal', 'final_mppen',\n",
    "            'final_mgpenpercent', 'final_mgres', 'final_ms', 'final_omnivamp', 'final_physicalvamp', 'final_power', 'final_powermax',\n",
    "            'final_powerregen', 'final_spellvamp', 'final_currentgold', 'final_magicdmgdone', 'final_magicdmgdonetochamps', 'final_magicdmgtaken',\n",
    "            'final_physdmgdone', 'final_physdmgdonetochamps', 'final_physdmgtaken', 'final_dmgdone', 'final_dmgdonetochamps', 'final_dmgtaken', \n",
    "            'final_truedmgdone', 'final_truedmgdonetochamps', 'final_truedmgtaken', 'final_goldpersec', 'final_jungleminionskilled', 'final_lvl',\n",
    "            'final_minionskilled', 'final_jungleminionskilled', 'final_jungleminionskilled', 'final_jungleminionskilled', 'final_enemycontrolled'                  \n",
    "             ]\n",
    "\n",
    "X_test = pd.merge(X_test_original, team_positions, how='inner', on=['matchId', 'participantId'])\n",
    "X_test = pd.merge(X_test, champion_data, how='inner', left_on='championId', right_on='key')\n",
    "X_test = pd.merge(X_test, champion_types, how='inner', left_on='championId', right_on='key')\n",
    "X_test = pd.merge(X_test, test_last_frame_values, how='inner', on=['matchId', 'participantId'])\n",
    "X_test = pd.merge(X_test, champion_mastery, how='left', on=['summonerId', 'championId']).fillna(0)\n",
    "\n",
    "X_test = X_test.sort_values(['matchId', 'participantId'], ascending = [True, True]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "convert_team_values(X_test, variables)\n",
    "\n",
    "X_test_perlane = X_test.groupby(['matchId', 'teamPosition'])[['final_gold']].sum().pivot_table(values='final_gold', index='matchId', columns='teamPosition').reset_index().drop(columns=0)\n",
    "\n",
    "for lane in ['BOTTOM', 'JUNGLE', 'MIDDLE', 'TOP', 'UTILITY']:\n",
    "  X_test_perlane[f'{lane}'] = np.where(X_test_perlane[f'{lane}'] >= 0, 1, -1)\n",
    "\n",
    "X_test = (\n",
    "    X_test\n",
    "    .groupby(['matchId'])[variables]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "X_test = pd.merge(X_test, X_test_perlane, how='inner', on='matchId').reset_index(drop = True)\n",
    "X_test = pd.merge(X_test, testing_events, how='inner', on='matchId').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline_neuralnetwork.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matchId</th>\n",
       "      <th>winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8000</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8001</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8002</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8003</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8004</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   matchId  winner\n",
       "0     8000     100\n",
       "1     8001     200\n",
       "2     8002     200\n",
       "3     8003     200\n",
       "4     8004     200"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['winner'] = y_pred\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('../data/submission_neural_network_hidden_layers4x100_alpha005_activtahn_2023_03_30.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt at gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_original\n",
    "\n",
    "X_train = pd.get_dummies(X_train, prefix='Champion')\n",
    "champions_encoded = list(X_train.drop(columns=['matchId', 'teamId', 'participantId', 'summonerId', 'summonerLevel', 'championId']).columns.values)\n",
    "\n",
    "variables = ['summonerLevel', 'championLevel','championPoints', \n",
    "            # 'Assassin', 'Fighter', 'Mage', 'Marksman', 'Support', 'Tank',\n",
    "            # 'info.attack', 'info.defense', 'info.magic', 'info.difficulty',\n",
    "            # 'stats.hpregenperlevel',\t'stats.mpregen', 'stats.mpregenperlevel',\t'stats.crit',\t'stats.critperlevel',\n",
    "            # 'stats.attackdamage', 'stats.attackdamageperlevel', 'stats.attackspeedperlevel',\t'stats.attackspeed',\n",
    "            'final_gold', 'final_xp', 'final_abilityhaste', 'final_abilitypower', 'final_armor', 'final_armorpen',\n",
    "            'final_armorpenpercent', 'final_atkdmg', 'final_bns_armorpenpercent', 'final_bns_magicpenpercent', 'final_ccreduction',\n",
    "            'final_cdreduction', 'final_remaining_health', 'final_health', 'final_healthrgn', 'final_lifesteal', 'final_mppen',\n",
    "            'final_mgpenpercent', 'final_mgres', 'final_ms', 'final_omnivamp', 'final_physicalvamp', 'final_power', 'final_powermax',\n",
    "            'final_powerregen', 'final_spellvamp', 'final_currentgold', 'final_magicdmgdone', 'final_magicdmgdonetochamps', 'final_magicdmgtaken',\n",
    "            'final_physdmgdone', 'final_physdmgdonetochamps', 'final_physdmgtaken', 'final_dmgdone', 'final_dmgdonetochamps', 'final_dmgtaken', \n",
    "            'final_truedmgdone', 'final_truedmgdonetochamps', 'final_truedmgtaken', 'final_goldpersec', 'final_jungleminionskilled', 'final_lvl',\n",
    "            'final_minionskilled', 'final_jungleminionskilled', 'final_jungleminionskilled', 'final_jungleminionskilled', 'final_enemycontrolled'                  \n",
    "             ] + champions_encoded\n",
    "\n",
    "X_train = pd.merge(X_train, team_positions, how='inner', on=['matchId', 'participantId'])\n",
    "X_train = pd.merge(X_train, champion_data, how='inner', left_on='championId', right_on='key')\n",
    "X_train = pd.merge(X_train, champion_types, how='inner', left_on='championId', right_on='key')\n",
    "X_train = pd.merge(X_train, train_last_frame_values, how='inner', on=['matchId', 'participantId'])\n",
    "X_train = pd.merge(X_train, champion_mastery, how='left', on=['summonerId', 'championId']).fillna(0)\n",
    "\n",
    "X_train = X_train.sort_values(['matchId', 'participantId'], ascending = [True, True]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "convert_team_values(X_train, variables)\n",
    "\n",
    "X_train_perlane = X_train.groupby(['matchId', 'teamPosition'])[['final_gold']].sum().pivot_table(values='final_gold', index='matchId', columns='teamPosition').reset_index().drop(columns=0)\n",
    "\n",
    "for lane in ['BOTTOM', 'JUNGLE', 'MIDDLE', 'TOP', 'UTILITY']:\n",
    "  X_train_perlane[f'{lane}'] = np.where(X_train_perlane[f'{lane}'] >= 0, 1, -1)\n",
    "\n",
    "X_train = (\n",
    "    X_train\n",
    "    .groupby(['matchId'])[variables]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "X_train = pd.merge(X_train, X_train_perlane, how='inner', on='matchId').reset_index(drop = True)\n",
    "X_train = pd.merge(X_train, training_events, how='inner', on='matchId').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;gb&#x27;,\n",
       "                 GradientBoostingClassifier(learning_rate=0.01,\n",
       "                                            n_estimators=1000))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;gb&#x27;,\n",
       "                 GradientBoostingClassifier(learning_rate=0.01,\n",
       "                                            n_estimators=1000))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(learning_rate=0.01, n_estimators=1000)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('gb',\n",
       "                 GradientBoostingClassifier(learning_rate=0.01,\n",
       "                                            n_estimators=1000))])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr = Pipeline(\n",
    "    steps = [\n",
    "        #('scaler', MinMaxScaler()),\n",
    "        ('gb', GradientBoostingClassifier(n_estimators = 1000, learning_rate=0.01))\n",
    "    ]\n",
    ")\n",
    "\n",
    "gbr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76125"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(\n",
    "    y_true = y_train,\n",
    "    y_pred = gbr.predict(X_train)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         100       0.76      0.77      0.77      4071\n",
      "         200       0.76      0.75      0.76      3929\n",
      "\n",
      "    accuracy                           0.76      8000\n",
      "   macro avg       0.76      0.76      0.76      8000\n",
      "weighted avg       0.76      0.76      0.76      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, gbr.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>final_gold</td>\n",
       "      <td>0.609183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>elite_monsters_killed</td>\n",
       "      <td>0.045338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>final_xp</td>\n",
       "      <td>0.044460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>championLevel</td>\n",
       "      <td>0.033303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Champion_AurelionSol</td>\n",
       "      <td>0.018480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>final_health</td>\n",
       "      <td>0.014638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>final_dmgdone</td>\n",
       "      <td>0.013148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>summonerLevel</td>\n",
       "      <td>0.012556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>BOTTOM</td>\n",
       "      <td>0.010200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Champion_Annie</td>\n",
       "      <td>0.009200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  variable  importance\n",
       "10              final_gold    0.609183\n",
       "227  elite_monsters_killed    0.045338\n",
       "11                final_xp    0.044460\n",
       "2            championLevel    0.033303\n",
       "67    Champion_AurelionSol    0.018480\n",
       "23            final_health    0.014638\n",
       "43           final_dmgdone    0.013148\n",
       "1            summonerLevel    0.012556\n",
       "219                 BOTTOM    0.010200\n",
       "64          Champion_Annie    0.009200"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances = pd.DataFrame({\n",
    "    'variable': gbr.feature_names_in_,\n",
    "    'importance': gbr['gb'].feature_importances_\n",
    "})\n",
    "\n",
    "importances.sort_values('importance', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test_original\n",
    "\n",
    "X_test = pd.get_dummies(X_test, prefix='Champion')\n",
    "champions_encoded = list(X_test.drop(columns=['matchId', 'teamId', 'participantId', 'summonerId', 'summonerLevel', 'championId']).columns.values)\n",
    "\n",
    "variables = ['summonerLevel', 'championLevel','championPoints', \n",
    "            # 'Assassin', 'Fighter', 'Mage', 'Marksman', 'Support', 'Tank',\n",
    "            # 'info.attack', 'info.defense', 'info.magic', 'info.difficulty',\n",
    "            # 'stats.hpregenperlevel',\t'stats.mpregen', 'stats.mpregenperlevel',\t'stats.crit',\t'stats.critperlevel',\n",
    "            # 'stats.attackdamage', 'stats.attackdamageperlevel', 'stats.attackspeedperlevel',\t'stats.attackspeed',\n",
    "            'final_gold', 'final_xp', 'final_abilityhaste', 'final_abilitypower', 'final_armor', 'final_armorpen',\n",
    "            'final_armorpenpercent', 'final_atkdmg', 'final_bns_armorpenpercent', 'final_bns_magicpenpercent', 'final_ccreduction',\n",
    "            'final_cdreduction', 'final_remaining_health', 'final_health', 'final_healthrgn', 'final_lifesteal', 'final_mppen',\n",
    "            'final_mgpenpercent', 'final_mgres', 'final_ms', 'final_omnivamp', 'final_physicalvamp', 'final_power', 'final_powermax',\n",
    "            'final_powerregen', 'final_spellvamp', 'final_currentgold', 'final_magicdmgdone', 'final_magicdmgdonetochamps', 'final_magicdmgtaken',\n",
    "            'final_physdmgdone', 'final_physdmgdonetochamps', 'final_physdmgtaken', 'final_dmgdone', 'final_dmgdonetochamps', 'final_dmgtaken', \n",
    "            'final_truedmgdone', 'final_truedmgdonetochamps', 'final_truedmgtaken', 'final_goldpersec', 'final_jungleminionskilled', 'final_lvl',\n",
    "            'final_minionskilled', 'final_jungleminionskilled', 'final_jungleminionskilled', 'final_jungleminionskilled', 'final_enemycontrolled'                  \n",
    "             ] + champions_encoded\n",
    "\n",
    "X_test = pd.merge(X_test, team_positions, how='inner', on=['matchId', 'participantId'])\n",
    "X_test = pd.merge(X_test, champion_data, how='inner', left_on='championId', right_on='key')\n",
    "X_test = pd.merge(X_test, champion_types, how='inner', left_on='championId', right_on='key')\n",
    "X_test = pd.merge(X_test, test_last_frame_values, how='inner', on=['matchId', 'participantId'])\n",
    "X_test = pd.merge(X_test, champion_mastery, how='left', on=['summonerId', 'championId']).fillna(0)\n",
    "\n",
    "X_test = X_test.sort_values(['matchId', 'participantId'], ascending = [True, True]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "convert_team_values(X_test, variables)\n",
    "\n",
    "X_test_perlane = X_test.groupby(['matchId', 'teamPosition'])[['final_gold']].sum().pivot_table(values='final_gold', index='matchId', columns='teamPosition').reset_index().drop(columns=0)\n",
    "\n",
    "for lane in ['BOTTOM', 'JUNGLE', 'MIDDLE', 'TOP', 'UTILITY']:\n",
    "  X_test_perlane[f'{lane}'] = np.where(X_test_perlane[f'{lane}'] >= 0, 1, -1)\n",
    "\n",
    "X_test = (\n",
    "    X_test\n",
    "    .groupby(['matchId'])[variables]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "X_test = pd.merge(X_test, X_test_perlane, how='inner', on='matchId').reset_index(drop = True)\n",
    "X_test = pd.merge(X_test, testing_events, how='inner', on='matchId').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gbr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matchId</th>\n",
       "      <th>winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8000</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8001</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8002</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8003</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8004</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   matchId  winner\n",
       "0     8000     100\n",
       "1     8001     200\n",
       "2     8002     200\n",
       "3     8003     200\n",
       "4     8004     200"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['winner'] = y_pred\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best submission so far 7.08 !\n",
    "#submission.to_csv('../data/submission_gradientboosting_n1000_learn001_2023_04_01.csv', index=False)\n",
    "\n",
    "# submission with this (all champs) is 7.06\n",
    "#submission.to_csv('../data/submission_gradientboosting_all_champions_n1000_learn001_2023_04_01.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
