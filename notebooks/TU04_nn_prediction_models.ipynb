{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle League of Legends competition - Neural Network Models\n",
    "\n",
    "## Team: Elden Ring"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://eldenring.wiki.fextralife.com/file/Elden-Ring/mirel_pastor_of_vow.jpg\" alt=\"PRAISE DOG\" style=\"width:806px;height:600px;\"/>\n",
    "\n",
    "#### PRAISE THE DOG!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Win at League of Legends?\n",
    "\n",
    "### Uninstall LoL and [install Dota 2](https://store.steampowered.com/app/570/Dota_2/), EZ. (just kidding, both games are great. Volvo pls gib patch.)\n",
    "\n",
    "<img src = \"https://static.wikia.nocookie.net/dota2_gamepedia/images/7/78/Keyart_phoenix.jpg/revision/latest/\" alt=\"SKREE CAW CAW IM A BIRD\" style=\"width:800px;height:497px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradually the list of files to read increased as I kept adding variables and complexities to models that could handle that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_original = pd.read_csv('../data/participants_train.csv')\n",
    "X_test_original = pd.read_csv('../data/participants_test.csv')\n",
    "y_train_original = pd.read_csv('../data/train_winners.csv')\n",
    "\n",
    "champion_mastery = pd.read_csv('../data/champion_mastery.csv')\n",
    "champion = pd.read_json('../data/champion.json')\n",
    "\n",
    "team_positions = pd.read_csv('../data/teamPositions.csv')\n",
    "\n",
    "train_last_frame_values = pd.read_csv('../data/train_last_frame_values.csv')\n",
    "test_last_frame_values = pd.read_csv('../data/test_last_frame_values.csv')\n",
    "\n",
    "training_events = pd.read_csv('../data/training_events.csv')\n",
    "testing_events = pd.read_csv('../data/testing_events.csv')\n",
    "\n",
    "submission = pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that converts values to negative (for the second team, teamId 200)\n",
    "# it leaves the first team values, teamId 100 intact\n",
    "\n",
    "def convert_team_values(df, col_names):\n",
    "        \n",
    "    for col in col_names:\n",
    "        df[col] = np.where(df['teamId'] == 200,\n",
    "                            -1* df[col],\n",
    "                                df[col])\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation needed on the dataframes (see the file prediction_models)\n",
    "vars = ['wards_placed', 'wards_killed', 'turretplates_destroyed', 'elite_monsters_killed']\n",
    "\n",
    "convert_team_values(training_events, vars)\n",
    "convert_team_values(testing_events, vars)\n",
    "\n",
    "training_events = training_events.groupby('matchId')[vars].sum()\n",
    "testing_events = testing_events.groupby('matchId')[vars].sum()\n",
    "\n",
    "champion_data = pd.json_normalize(champion['data'])\n",
    "champion_data['key'] = champion_data['key'].astype(int)\n",
    "\n",
    "champion_types= champion_data.explode('tags').pivot_table(values='id', index='key', columns='tags', aggfunc='count').fillna(0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be used later to measure the accuracy!\n",
    "kfold = KFold(n_splits = 10, shuffle = True, random_state = 42)\n",
    "\n",
    "# this is to extract the column that is needed for training\n",
    "y_train = y_train_original['winner']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Neural Networks to Improve the Logreg Predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['summonerLevel', 'championLevel','championPoints', \n",
    "             'Assassin', 'Fighter', 'Mage', 'Marksman', 'Support', 'Tank',\n",
    "            'info.attack', 'info.defense', 'info.magic', 'info.difficulty',\n",
    "            'stats.hpregenperlevel',\t'stats.mpregen', 'stats.mpregenperlevel',\t'stats.crit',\t'stats.critperlevel',\n",
    "            'stats.attackdamage', 'stats.attackdamageperlevel', 'stats.attackspeedperlevel',\t'stats.attackspeed',\n",
    "            'final_gold', 'final_xp', 'final_abilityhaste', 'final_abilitypower', 'final_armor', 'final_armorpen',\n",
    "            'final_armorpenpercent', 'final_atkdmg', 'final_bns_armorpenpercent', 'final_bns_magicpenpercent', 'final_ccreduction',\n",
    "            'final_cdreduction', 'final_remaining_health', 'final_health', 'final_healthrgn', 'final_lifesteal', 'final_mppen',\n",
    "            'final_mgpenpercent', 'final_mgres', 'final_ms', 'final_omnivamp', 'final_physicalvamp', 'final_power', 'final_powermax',\n",
    "            'final_powerregen', 'final_spellvamp', 'final_currentgold', 'final_magicdmgdone', 'final_magicdmgdonetochamps', 'final_magicdmgtaken',\n",
    "            'final_physdmgdone', 'final_physdmgdonetochamps', 'final_physdmgtaken', 'final_dmgdone', 'final_dmgdonetochamps', 'final_dmgtaken', \n",
    "            'final_truedmgdone', 'final_truedmgdonetochamps', 'final_truedmgtaken', 'final_goldpersec', 'final_jungleminionskilled', 'final_lvl',\n",
    "            'final_minionskilled', 'final_jungleminionskilled', 'final_jungleminionskilled', 'final_jungleminionskilled', 'final_enemycontrolled'                  \n",
    "             ]\n",
    "\n",
    "X_train = pd.merge(X_train_original, team_positions, how='inner', on=['matchId', 'participantId'])\n",
    "X_train = pd.merge(X_train, champion_data, how='inner', left_on='championId', right_on='key')\n",
    "X_train = pd.merge(X_train, champion_types, how='inner', left_on='championId', right_on='key')\n",
    "X_train = pd.merge(X_train, train_last_frame_values, how='inner', on=['matchId', 'participantId'])\n",
    "X_train = pd.merge(X_train, champion_mastery, how='left', on=['summonerId', 'championId']).fillna(0)\n",
    "\n",
    "X_train = X_train.sort_values(['matchId', 'participantId'], ascending = [True, True]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "convert_team_values(X_train, variables)\n",
    "\n",
    "X_train_perlane = X_train.groupby(['matchId', 'teamPosition'])[['final_gold']].sum().pivot_table(values='final_gold', index='matchId', columns='teamPosition').reset_index().drop(columns=0)\n",
    "\n",
    "for lane in ['BOTTOM', 'JUNGLE', 'MIDDLE', 'TOP', 'UTILITY']:\n",
    "  X_train_perlane[f'{lane}'] = np.where(X_train_perlane[f'{lane}'] >= 0, 1, -1)\n",
    "\n",
    "X_train = (\n",
    "    X_train\n",
    "    .groupby(['matchId'])[variables]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "X_train = pd.merge(X_train, X_train_perlane, how='inner', on='matchId').reset_index(drop = True)\n",
    "X_train = pd.merge(X_train, training_events, how='inner', on='matchId').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matchId</th>\n",
       "      <th>summonerLevel</th>\n",
       "      <th>championLevel</th>\n",
       "      <th>championPoints</th>\n",
       "      <th>Assassin</th>\n",
       "      <th>Fighter</th>\n",
       "      <th>Mage</th>\n",
       "      <th>Marksman</th>\n",
       "      <th>Support</th>\n",
       "      <th>Tank</th>\n",
       "      <th>...</th>\n",
       "      <th>final_enemycontrolled</th>\n",
       "      <th>BOTTOM</th>\n",
       "      <th>JUNGLE</th>\n",
       "      <th>MIDDLE</th>\n",
       "      <th>TOP</th>\n",
       "      <th>UTILITY</th>\n",
       "      <th>wards_placed</th>\n",
       "      <th>wards_killed</th>\n",
       "      <th>turretplates_destroyed</th>\n",
       "      <th>elite_monsters_killed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>682</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-605428.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>67664</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-64</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>628</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1356027.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-61783</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1049</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-273911.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-132630</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-1027</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-287667.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-39616</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1612</td>\n",
       "      <td>7.0</td>\n",
       "      <td>503668.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16629</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   matchId  summonerLevel  championLevel  championPoints  Assassin  Fighter  \\\n",
       "0        0            682            0.0       -605428.0      -1.0     -1.0   \n",
       "1        1            628            8.0       1356027.0       3.0      0.0   \n",
       "2        2           1049            1.0       -273911.0      -2.0      1.0   \n",
       "3        3          -1027           -3.0       -287667.0       1.0      1.0   \n",
       "4        4           1612            7.0        503668.0       0.0      0.0   \n",
       "\n",
       "   Mage  Marksman  Support  Tank  ...  final_enemycontrolled  BOTTOM  JUNGLE  \\\n",
       "0   2.0       0.0      0.0  -2.0  ...                  67664      -1      -1   \n",
       "1   0.0       0.0     -1.0  -1.0  ...                 -61783       1       1   \n",
       "2   0.0       0.0      0.0   0.0  ...                -132630       1      -1   \n",
       "3   1.0      -1.0     -1.0   0.0  ...                 -39616       1       1   \n",
       "4   2.0      -1.0      0.0  -1.0  ...                  16629      -1       1   \n",
       "\n",
       "   MIDDLE  TOP  UTILITY  wards_placed  wards_killed  turretplates_destroyed  \\\n",
       "0       1   -1       -1           -64            -1                      -2   \n",
       "1       1   -1        1             1             0                      -1   \n",
       "2      -1   -1       -1             4             1                      -2   \n",
       "3      -1   -1        1             4             0                       2   \n",
       "4      -1    1        1             4             0                      -1   \n",
       "\n",
       "   elite_monsters_killed  \n",
       "0                     -1  \n",
       "1                     -1  \n",
       "2                      1  \n",
       "3                      0  \n",
       "4                     -1  \n",
       "\n",
       "[5 rows x 79 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_neuralnetwork = Pipeline(\n",
    "    steps = [\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('nn', MLPClassifier(verbose = True,\n",
    "                             hidden_layer_sizes = (100, 100, 100, 100),\n",
    "                             activation = 'tanh',\n",
    "                             max_iter = 10000,\n",
    "                             alpha=0.05))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.65727970\n",
      "Iteration 2, loss = 0.61403113\n",
      "Iteration 3, loss = 0.61605887\n",
      "Iteration 4, loss = 0.62173213\n",
      "Iteration 5, loss = 0.60096988\n",
      "Iteration 6, loss = 0.59451160\n",
      "Iteration 7, loss = 0.59471611\n",
      "Iteration 8, loss = 0.59364611\n",
      "Iteration 9, loss = 0.59688439\n",
      "Iteration 10, loss = 0.59102111\n",
      "Iteration 11, loss = 0.58756731\n",
      "Iteration 12, loss = 0.59510892\n",
      "Iteration 13, loss = 0.58998631\n",
      "Iteration 14, loss = 0.58541963\n",
      "Iteration 15, loss = 0.58294796\n",
      "Iteration 16, loss = 0.58535731\n",
      "Iteration 17, loss = 0.58652887\n",
      "Iteration 18, loss = 0.58085443\n",
      "Iteration 19, loss = 0.57894982\n",
      "Iteration 20, loss = 0.57995711\n",
      "Iteration 21, loss = 0.58401510\n",
      "Iteration 22, loss = 0.57831234\n",
      "Iteration 23, loss = 0.57767067\n",
      "Iteration 24, loss = 0.57559296\n",
      "Iteration 25, loss = 0.57490748\n",
      "Iteration 26, loss = 0.57927736\n",
      "Iteration 27, loss = 0.57238270\n",
      "Iteration 28, loss = 0.57627896\n",
      "Iteration 29, loss = 0.57043315\n",
      "Iteration 30, loss = 0.57277950\n",
      "Iteration 31, loss = 0.57232347\n",
      "Iteration 32, loss = 0.57137035\n",
      "Iteration 33, loss = 0.56912744\n",
      "Iteration 34, loss = 0.56979738\n",
      "Iteration 35, loss = 0.56735406\n",
      "Iteration 36, loss = 0.57250630\n",
      "Iteration 37, loss = 0.56971830\n",
      "Iteration 38, loss = 0.56521054\n",
      "Iteration 39, loss = 0.56397337\n",
      "Iteration 40, loss = 0.56856375\n",
      "Iteration 41, loss = 0.56855097\n",
      "Iteration 42, loss = 0.56919491\n",
      "Iteration 43, loss = 0.56255578\n",
      "Iteration 44, loss = 0.56281944\n",
      "Iteration 45, loss = 0.56219425\n",
      "Iteration 46, loss = 0.56206831\n",
      "Iteration 47, loss = 0.56359943\n",
      "Iteration 48, loss = 0.55964437\n",
      "Iteration 49, loss = 0.56591664\n",
      "Iteration 50, loss = 0.56226395\n",
      "Iteration 51, loss = 0.56362619\n",
      "Iteration 52, loss = 0.56311092\n",
      "Iteration 53, loss = 0.56150268\n",
      "Iteration 54, loss = 0.55955133\n",
      "Iteration 55, loss = 0.55865879\n",
      "Iteration 56, loss = 0.55865635\n",
      "Iteration 57, loss = 0.55998611\n",
      "Iteration 58, loss = 0.55674632\n",
      "Iteration 59, loss = 0.55726655\n",
      "Iteration 60, loss = 0.56025150\n",
      "Iteration 61, loss = 0.56079259\n",
      "Iteration 62, loss = 0.55531378\n",
      "Iteration 63, loss = 0.55655701\n",
      "Iteration 64, loss = 0.55509697\n",
      "Iteration 65, loss = 0.55769097\n",
      "Iteration 66, loss = 0.55626904\n",
      "Iteration 67, loss = 0.55689444\n",
      "Iteration 68, loss = 0.55527227\n",
      "Iteration 69, loss = 0.55485178\n",
      "Iteration 70, loss = 0.55543053\n",
      "Iteration 71, loss = 0.55270882\n",
      "Iteration 72, loss = 0.55642321\n",
      "Iteration 73, loss = 0.55534472\n",
      "Iteration 74, loss = 0.55415309\n",
      "Iteration 75, loss = 0.55069828\n",
      "Iteration 76, loss = 0.55643783\n",
      "Iteration 77, loss = 0.55221719\n",
      "Iteration 78, loss = 0.55267116\n",
      "Iteration 79, loss = 0.55067626\n",
      "Iteration 80, loss = 0.55856754\n",
      "Iteration 81, loss = 0.55291606\n",
      "Iteration 82, loss = 0.55825937\n",
      "Iteration 83, loss = 0.55850565\n",
      "Iteration 84, loss = 0.54935006\n",
      "Iteration 85, loss = 0.55124309\n",
      "Iteration 86, loss = 0.55368370\n",
      "Iteration 87, loss = 0.55154217\n",
      "Iteration 88, loss = 0.54812199\n",
      "Iteration 89, loss = 0.54991892\n",
      "Iteration 90, loss = 0.55203006\n",
      "Iteration 91, loss = 0.54837790\n",
      "Iteration 92, loss = 0.55185539\n",
      "Iteration 93, loss = 0.54865712\n",
      "Iteration 94, loss = 0.55080639\n",
      "Iteration 95, loss = 0.55453370\n",
      "Iteration 96, loss = 0.55140287\n",
      "Iteration 97, loss = 0.54829998\n",
      "Iteration 98, loss = 0.55142862\n",
      "Iteration 99, loss = 0.55242164\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler()),\n",
       "                (&#x27;nn&#x27;,\n",
       "                 MLPClassifier(activation=&#x27;tanh&#x27;, alpha=0.05,\n",
       "                               hidden_layer_sizes=(100, 100, 100, 100),\n",
       "                               max_iter=10000, verbose=True))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler()),\n",
       "                (&#x27;nn&#x27;,\n",
       "                 MLPClassifier(activation=&#x27;tanh&#x27;, alpha=0.05,\n",
       "                               hidden_layer_sizes=(100, 100, 100, 100),\n",
       "                               max_iter=10000, verbose=True))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, alpha=0.05,\n",
       "              hidden_layer_sizes=(100, 100, 100, 100), max_iter=10000,\n",
       "              verbose=True)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('scaler', MinMaxScaler()),\n",
       "                ('nn',\n",
       "                 MLPClassifier(activation='tanh', alpha=0.05,\n",
       "                               hidden_layer_sizes=(100, 100, 100, 100),\n",
       "                               max_iter=10000, verbose=True))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_neuralnetwork.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.725625"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(\n",
    "    y_true = y_train,\n",
    "    y_pred = pipeline_neuralnetwork.predict(X_train)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.66646843\n",
      "Iteration 2, loss = 0.62959231\n",
      "Iteration 3, loss = 0.61642563\n",
      "Iteration 4, loss = 0.61104296\n",
      "Iteration 5, loss = 0.60275513\n",
      "Iteration 6, loss = 0.59776119\n",
      "Iteration 7, loss = 0.60205821\n",
      "Iteration 8, loss = 0.59823554\n",
      "Iteration 9, loss = 0.59391677\n",
      "Iteration 10, loss = 0.59442807\n",
      "Iteration 11, loss = 0.59049584\n",
      "Iteration 12, loss = 0.59265938\n",
      "Iteration 13, loss = 0.59102323\n",
      "Iteration 14, loss = 0.58738650\n",
      "Iteration 15, loss = 0.58705233\n",
      "Iteration 16, loss = 0.58641518\n",
      "Iteration 17, loss = 0.59130026\n",
      "Iteration 18, loss = 0.58164870\n",
      "Iteration 19, loss = 0.57932351\n",
      "Iteration 20, loss = 0.58228079\n",
      "Iteration 21, loss = 0.58372173\n",
      "Iteration 22, loss = 0.58742920\n",
      "Iteration 23, loss = 0.57940665\n",
      "Iteration 24, loss = 0.58405909\n",
      "Iteration 25, loss = 0.58878568\n",
      "Iteration 26, loss = 0.57689643\n",
      "Iteration 27, loss = 0.57845166\n",
      "Iteration 28, loss = 0.57799100\n",
      "Iteration 29, loss = 0.57608896\n",
      "Iteration 30, loss = 0.57440819\n",
      "Iteration 31, loss = 0.57395092\n",
      "Iteration 32, loss = 0.57544422\n",
      "Iteration 33, loss = 0.58279559\n",
      "Iteration 34, loss = 0.57382031\n",
      "Iteration 35, loss = 0.57062786\n",
      "Iteration 36, loss = 0.57118710\n",
      "Iteration 37, loss = 0.57003395\n",
      "Iteration 38, loss = 0.57527465\n",
      "Iteration 39, loss = 0.56952717\n",
      "Iteration 40, loss = 0.57128448\n",
      "Iteration 41, loss = 0.56938808\n",
      "Iteration 42, loss = 0.56965494\n",
      "Iteration 43, loss = 0.56695887\n",
      "Iteration 44, loss = 0.58151814\n",
      "Iteration 45, loss = 0.56906309\n",
      "Iteration 46, loss = 0.56733627\n",
      "Iteration 47, loss = 0.56622894\n",
      "Iteration 48, loss = 0.56411133\n",
      "Iteration 49, loss = 0.57169266\n",
      "Iteration 50, loss = 0.56598869\n",
      "Iteration 51, loss = 0.56564275\n",
      "Iteration 52, loss = 0.56313052\n",
      "Iteration 53, loss = 0.56132193\n",
      "Iteration 54, loss = 0.56869555\n",
      "Iteration 55, loss = 0.56345716\n",
      "Iteration 56, loss = 0.56385506\n",
      "Iteration 57, loss = 0.56167794\n",
      "Iteration 58, loss = 0.56160876\n",
      "Iteration 59, loss = 0.56186266\n",
      "Iteration 60, loss = 0.56606339\n",
      "Iteration 61, loss = 0.56870547\n",
      "Iteration 62, loss = 0.56423266\n",
      "Iteration 63, loss = 0.55822924\n",
      "Iteration 64, loss = 0.55905574\n",
      "Iteration 65, loss = 0.55995886\n",
      "Iteration 66, loss = 0.55949878\n",
      "Iteration 67, loss = 0.55760152\n",
      "Iteration 68, loss = 0.55614377\n",
      "Iteration 69, loss = 0.55937407\n",
      "Iteration 70, loss = 0.56766036\n",
      "Iteration 71, loss = 0.55811823\n",
      "Iteration 72, loss = 0.55505506\n",
      "Iteration 73, loss = 0.55731240\n",
      "Iteration 74, loss = 0.55903043\n",
      "Iteration 75, loss = 0.55596375\n",
      "Iteration 76, loss = 0.56216307\n",
      "Iteration 77, loss = 0.57025838\n",
      "Iteration 78, loss = 0.55723804\n",
      "Iteration 79, loss = 0.55677832\n",
      "Iteration 80, loss = 0.55889481\n",
      "Iteration 81, loss = 0.55510662\n",
      "Iteration 82, loss = 0.55798806\n",
      "Iteration 83, loss = 0.55361933\n",
      "Iteration 84, loss = 0.55404532\n",
      "Iteration 85, loss = 0.55381724\n",
      "Iteration 86, loss = 0.55505786\n",
      "Iteration 87, loss = 0.55400356\n",
      "Iteration 88, loss = 0.56220877\n",
      "Iteration 89, loss = 0.55371514\n",
      "Iteration 90, loss = 0.55021350\n",
      "Iteration 91, loss = 0.55236849\n",
      "Iteration 92, loss = 0.55497190\n",
      "Iteration 93, loss = 0.55826450\n",
      "Iteration 94, loss = 0.55131661\n",
      "Iteration 95, loss = 0.55202690\n",
      "Iteration 96, loss = 0.55280760\n",
      "Iteration 97, loss = 0.55073361\n",
      "Iteration 98, loss = 0.55069536\n",
      "Iteration 99, loss = 0.55089924\n",
      "Iteration 100, loss = 0.55032674\n",
      "Iteration 101, loss = 0.54850728\n",
      "Iteration 102, loss = 0.55176884\n",
      "Iteration 103, loss = 0.55382601\n",
      "Iteration 104, loss = 0.55280414\n",
      "Iteration 105, loss = 0.55109974\n",
      "Iteration 106, loss = 0.54884350\n",
      "Iteration 107, loss = 0.54796443\n",
      "Iteration 108, loss = 0.54829036\n",
      "Iteration 109, loss = 0.55323866\n",
      "Iteration 110, loss = 0.55156593\n",
      "Iteration 111, loss = 0.54885025\n",
      "Iteration 112, loss = 0.54808498\n",
      "Iteration 113, loss = 0.54768066\n",
      "Iteration 114, loss = 0.54662559\n",
      "Iteration 115, loss = 0.54643827\n",
      "Iteration 116, loss = 0.55027630\n",
      "Iteration 117, loss = 0.54901557\n",
      "Iteration 118, loss = 0.55188598\n",
      "Iteration 119, loss = 0.54781583\n",
      "Iteration 120, loss = 0.54951737\n",
      "Iteration 121, loss = 0.54718994\n",
      "Iteration 122, loss = 0.54757059\n",
      "Iteration 123, loss = 0.54804415\n",
      "Iteration 124, loss = 0.55090352\n",
      "Iteration 125, loss = 0.55041029\n",
      "Iteration 126, loss = 0.54613410\n",
      "Iteration 127, loss = 0.54632230\n",
      "Iteration 128, loss = 0.54748802\n",
      "Iteration 129, loss = 0.54494638\n",
      "Iteration 130, loss = 0.54999097\n",
      "Iteration 131, loss = 0.54432623\n",
      "Iteration 132, loss = 0.54338968\n",
      "Iteration 133, loss = 0.54244971\n",
      "Iteration 134, loss = 0.54681013\n",
      "Iteration 135, loss = 0.54421420\n",
      "Iteration 136, loss = 0.54573153\n",
      "Iteration 137, loss = 0.54860528\n",
      "Iteration 138, loss = 0.55356086\n",
      "Iteration 139, loss = 0.54264118\n",
      "Iteration 140, loss = 0.54397863\n",
      "Iteration 141, loss = 0.54486162\n",
      "Iteration 142, loss = 0.54625436\n",
      "Iteration 143, loss = 0.54214519\n",
      "Iteration 144, loss = 0.54327452\n",
      "Iteration 145, loss = 0.54112460\n",
      "Iteration 146, loss = 0.55080030\n",
      "Iteration 147, loss = 0.54498864\n",
      "Iteration 148, loss = 0.54305243\n",
      "Iteration 149, loss = 0.54281908\n",
      "Iteration 150, loss = 0.54542424\n",
      "Iteration 151, loss = 0.54710300\n",
      "Iteration 152, loss = 0.54322606\n",
      "Iteration 153, loss = 0.54763557\n",
      "Iteration 154, loss = 0.54273214\n",
      "Iteration 155, loss = 0.54303443\n",
      "Iteration 156, loss = 0.54145642\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64954290\n",
      "Iteration 2, loss = 0.61676386\n",
      "Iteration 3, loss = 0.61277751\n",
      "Iteration 4, loss = 0.60866291\n",
      "Iteration 5, loss = 0.61377980\n",
      "Iteration 6, loss = 0.60319062\n",
      "Iteration 7, loss = 0.59836861\n",
      "Iteration 8, loss = 0.59254300\n",
      "Iteration 9, loss = 0.59446977\n",
      "Iteration 10, loss = 0.59684340\n",
      "Iteration 11, loss = 0.59912110\n",
      "Iteration 12, loss = 0.59132684\n",
      "Iteration 13, loss = 0.58826859\n",
      "Iteration 14, loss = 0.58612482\n",
      "Iteration 15, loss = 0.58289305\n",
      "Iteration 16, loss = 0.58181119\n",
      "Iteration 17, loss = 0.59219399\n",
      "Iteration 18, loss = 0.58821480\n",
      "Iteration 19, loss = 0.58164058\n",
      "Iteration 20, loss = 0.58276901\n",
      "Iteration 21, loss = 0.57973557\n",
      "Iteration 22, loss = 0.58127284\n",
      "Iteration 23, loss = 0.58089198\n",
      "Iteration 24, loss = 0.57522748\n",
      "Iteration 25, loss = 0.57518793\n",
      "Iteration 26, loss = 0.57467340\n",
      "Iteration 27, loss = 0.57289607\n",
      "Iteration 28, loss = 0.57630946\n",
      "Iteration 29, loss = 0.57361259\n",
      "Iteration 30, loss = 0.58011018\n",
      "Iteration 31, loss = 0.57537495\n",
      "Iteration 32, loss = 0.57124438\n",
      "Iteration 33, loss = 0.57277157\n",
      "Iteration 34, loss = 0.57258156\n",
      "Iteration 35, loss = 0.57018272\n",
      "Iteration 36, loss = 0.57245124\n",
      "Iteration 37, loss = 0.56707240\n",
      "Iteration 38, loss = 0.56947382\n",
      "Iteration 39, loss = 0.57354152\n",
      "Iteration 40, loss = 0.56841397\n",
      "Iteration 41, loss = 0.57037100\n",
      "Iteration 42, loss = 0.56587772\n",
      "Iteration 43, loss = 0.56797139\n",
      "Iteration 44, loss = 0.56574057\n",
      "Iteration 45, loss = 0.57317130\n",
      "Iteration 46, loss = 0.56501885\n",
      "Iteration 47, loss = 0.56758594\n",
      "Iteration 48, loss = 0.56992022\n",
      "Iteration 49, loss = 0.56537375\n",
      "Iteration 50, loss = 0.56790310\n",
      "Iteration 51, loss = 0.56766697\n",
      "Iteration 52, loss = 0.56720305\n",
      "Iteration 53, loss = 0.56267920\n",
      "Iteration 54, loss = 0.56880547\n",
      "Iteration 55, loss = 0.56433882\n",
      "Iteration 56, loss = 0.56102844\n",
      "Iteration 57, loss = 0.56182063\n",
      "Iteration 58, loss = 0.55856260\n",
      "Iteration 59, loss = 0.55946954\n",
      "Iteration 60, loss = 0.55795996\n",
      "Iteration 61, loss = 0.56299042\n",
      "Iteration 62, loss = 0.56106593\n",
      "Iteration 63, loss = 0.55799967\n",
      "Iteration 64, loss = 0.56064595\n",
      "Iteration 65, loss = 0.55945673\n",
      "Iteration 66, loss = 0.56538314\n",
      "Iteration 67, loss = 0.56226955\n",
      "Iteration 68, loss = 0.55891107\n",
      "Iteration 69, loss = 0.56035756\n",
      "Iteration 70, loss = 0.55713847\n",
      "Iteration 71, loss = 0.55609964\n",
      "Iteration 72, loss = 0.56015386\n",
      "Iteration 73, loss = 0.55559557\n",
      "Iteration 74, loss = 0.55683882\n",
      "Iteration 75, loss = 0.55669688\n",
      "Iteration 76, loss = 0.55989137\n",
      "Iteration 77, loss = 0.55595735\n",
      "Iteration 78, loss = 0.55454112\n",
      "Iteration 79, loss = 0.55579077\n",
      "Iteration 80, loss = 0.55453787\n",
      "Iteration 81, loss = 0.55621049\n",
      "Iteration 82, loss = 0.55821748\n",
      "Iteration 83, loss = 0.55745420\n",
      "Iteration 84, loss = 0.55306854\n",
      "Iteration 85, loss = 0.55717168\n",
      "Iteration 86, loss = 0.55296446\n",
      "Iteration 87, loss = 0.55582520\n",
      "Iteration 88, loss = 0.55233095\n",
      "Iteration 89, loss = 0.55375026\n",
      "Iteration 90, loss = 0.55163467\n",
      "Iteration 91, loss = 0.55296800\n",
      "Iteration 92, loss = 0.55028087\n",
      "Iteration 93, loss = 0.55495052\n",
      "Iteration 94, loss = 0.55411666\n",
      "Iteration 95, loss = 0.56010674\n",
      "Iteration 96, loss = 0.56199155\n",
      "Iteration 97, loss = 0.55732334\n",
      "Iteration 98, loss = 0.55183829\n",
      "Iteration 99, loss = 0.55258641\n",
      "Iteration 100, loss = 0.55280839\n",
      "Iteration 101, loss = 0.55046645\n",
      "Iteration 102, loss = 0.54996667\n",
      "Iteration 103, loss = 0.55191188\n",
      "Iteration 104, loss = 0.54836248\n",
      "Iteration 105, loss = 0.54894179\n",
      "Iteration 106, loss = 0.55490067\n",
      "Iteration 107, loss = 0.54869639\n",
      "Iteration 108, loss = 0.55209354\n",
      "Iteration 109, loss = 0.54956681\n",
      "Iteration 110, loss = 0.54821384\n",
      "Iteration 111, loss = 0.54674369\n",
      "Iteration 112, loss = 0.55148878\n",
      "Iteration 113, loss = 0.55165248\n",
      "Iteration 114, loss = 0.55132107\n",
      "Iteration 115, loss = 0.54907100\n",
      "Iteration 116, loss = 0.54799852\n",
      "Iteration 117, loss = 0.55238649\n",
      "Iteration 118, loss = 0.55114587\n",
      "Iteration 119, loss = 0.55065578\n",
      "Iteration 120, loss = 0.55628674\n",
      "Iteration 121, loss = 0.55601661\n",
      "Iteration 122, loss = 0.55041720\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66488556\n",
      "Iteration 2, loss = 0.62340645\n",
      "Iteration 3, loss = 0.62849076\n",
      "Iteration 4, loss = 0.61194635\n",
      "Iteration 5, loss = 0.60642098\n",
      "Iteration 6, loss = 0.60069604\n",
      "Iteration 7, loss = 0.59952289\n",
      "Iteration 8, loss = 0.60649160\n",
      "Iteration 9, loss = 0.60605819\n",
      "Iteration 10, loss = 0.59763631\n",
      "Iteration 11, loss = 0.59167343\n",
      "Iteration 12, loss = 0.58990392\n",
      "Iteration 13, loss = 0.58732008\n",
      "Iteration 14, loss = 0.58871504\n",
      "Iteration 15, loss = 0.58929845\n",
      "Iteration 16, loss = 0.59331579\n",
      "Iteration 17, loss = 0.58558188\n",
      "Iteration 18, loss = 0.58221641\n",
      "Iteration 19, loss = 0.58774547\n",
      "Iteration 20, loss = 0.59153213\n",
      "Iteration 21, loss = 0.58114344\n",
      "Iteration 22, loss = 0.57967254\n",
      "Iteration 23, loss = 0.58089666\n",
      "Iteration 24, loss = 0.57872318\n",
      "Iteration 25, loss = 0.57796140\n",
      "Iteration 26, loss = 0.57922995\n",
      "Iteration 27, loss = 0.57480145\n",
      "Iteration 28, loss = 0.58635505\n",
      "Iteration 29, loss = 0.57887608\n",
      "Iteration 30, loss = 0.57648071\n",
      "Iteration 31, loss = 0.57381103\n",
      "Iteration 32, loss = 0.57397500\n",
      "Iteration 33, loss = 0.57387054\n",
      "Iteration 34, loss = 0.57726986\n",
      "Iteration 35, loss = 0.57351976\n",
      "Iteration 36, loss = 0.57938030\n",
      "Iteration 37, loss = 0.57845114\n",
      "Iteration 38, loss = 0.57344190\n",
      "Iteration 39, loss = 0.56968753\n",
      "Iteration 40, loss = 0.57500729\n",
      "Iteration 41, loss = 0.57153662\n",
      "Iteration 42, loss = 0.56978952\n",
      "Iteration 43, loss = 0.57507657\n",
      "Iteration 44, loss = 0.57496900\n",
      "Iteration 45, loss = 0.56707249\n",
      "Iteration 46, loss = 0.56554559\n",
      "Iteration 47, loss = 0.56606758\n",
      "Iteration 48, loss = 0.57362622\n",
      "Iteration 49, loss = 0.56437883\n",
      "Iteration 50, loss = 0.56531966\n",
      "Iteration 51, loss = 0.56459837\n",
      "Iteration 52, loss = 0.56614494\n",
      "Iteration 53, loss = 0.56297809\n",
      "Iteration 54, loss = 0.56719922\n",
      "Iteration 55, loss = 0.56867319\n",
      "Iteration 56, loss = 0.56846939\n",
      "Iteration 57, loss = 0.56250643\n",
      "Iteration 58, loss = 0.56222803\n",
      "Iteration 59, loss = 0.56250702\n",
      "Iteration 60, loss = 0.56437960\n",
      "Iteration 61, loss = 0.56209174\n",
      "Iteration 62, loss = 0.56327921\n",
      "Iteration 63, loss = 0.56216117\n",
      "Iteration 64, loss = 0.56976928\n",
      "Iteration 65, loss = 0.56299925\n",
      "Iteration 66, loss = 0.56133970\n",
      "Iteration 67, loss = 0.56490985\n",
      "Iteration 68, loss = 0.56590991\n",
      "Iteration 69, loss = 0.56281466\n",
      "Iteration 70, loss = 0.56073330\n",
      "Iteration 71, loss = 0.56088259\n",
      "Iteration 72, loss = 0.55800500\n",
      "Iteration 73, loss = 0.56055670\n",
      "Iteration 74, loss = 0.55791854\n",
      "Iteration 75, loss = 0.56116356\n",
      "Iteration 76, loss = 0.56676243\n",
      "Iteration 77, loss = 0.55896050\n",
      "Iteration 78, loss = 0.56414827\n",
      "Iteration 79, loss = 0.55649548\n",
      "Iteration 80, loss = 0.55802126\n",
      "Iteration 81, loss = 0.56159186\n",
      "Iteration 82, loss = 0.55983571\n",
      "Iteration 83, loss = 0.56074378\n",
      "Iteration 84, loss = 0.55625584\n",
      "Iteration 85, loss = 0.55617401\n",
      "Iteration 86, loss = 0.55888508\n",
      "Iteration 87, loss = 0.55458697\n",
      "Iteration 88, loss = 0.55521592\n",
      "Iteration 89, loss = 0.56034407\n",
      "Iteration 90, loss = 0.56284805\n",
      "Iteration 91, loss = 0.55412610\n",
      "Iteration 92, loss = 0.55493584\n",
      "Iteration 93, loss = 0.55608143\n",
      "Iteration 94, loss = 0.55235454\n",
      "Iteration 95, loss = 0.55261140\n",
      "Iteration 96, loss = 0.55104362\n",
      "Iteration 97, loss = 0.55099083\n",
      "Iteration 98, loss = 0.55238346\n",
      "Iteration 99, loss = 0.55580525\n",
      "Iteration 100, loss = 0.54987113\n",
      "Iteration 101, loss = 0.55148161\n",
      "Iteration 102, loss = 0.55757896\n",
      "Iteration 103, loss = 0.55272217\n",
      "Iteration 104, loss = 0.55184179\n",
      "Iteration 105, loss = 0.55408373\n",
      "Iteration 106, loss = 0.55656899\n",
      "Iteration 107, loss = 0.55644494\n",
      "Iteration 108, loss = 0.55624055\n",
      "Iteration 109, loss = 0.55670455\n",
      "Iteration 110, loss = 0.55255670\n",
      "Iteration 111, loss = 0.55169253\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68708988\n",
      "Iteration 2, loss = 0.63733576\n",
      "Iteration 3, loss = 0.61582621\n",
      "Iteration 4, loss = 0.61258652\n",
      "Iteration 5, loss = 0.61590785\n",
      "Iteration 6, loss = 0.60093918\n",
      "Iteration 7, loss = 0.60693817\n",
      "Iteration 8, loss = 0.59789847\n",
      "Iteration 9, loss = 0.59703570\n",
      "Iteration 10, loss = 0.59276349\n",
      "Iteration 11, loss = 0.59980484\n",
      "Iteration 12, loss = 0.59866714\n",
      "Iteration 13, loss = 0.59136814\n",
      "Iteration 14, loss = 0.59455208\n",
      "Iteration 15, loss = 0.59001250\n",
      "Iteration 16, loss = 0.58641104\n",
      "Iteration 17, loss = 0.58875718\n",
      "Iteration 18, loss = 0.58552044\n",
      "Iteration 19, loss = 0.58776227\n",
      "Iteration 20, loss = 0.58708913\n",
      "Iteration 21, loss = 0.59576844\n",
      "Iteration 22, loss = 0.58621782\n",
      "Iteration 23, loss = 0.57939014\n",
      "Iteration 24, loss = 0.58345721\n",
      "Iteration 25, loss = 0.57807436\n",
      "Iteration 26, loss = 0.58043837\n",
      "Iteration 27, loss = 0.57812808\n",
      "Iteration 28, loss = 0.58212457\n",
      "Iteration 29, loss = 0.57662362\n",
      "Iteration 30, loss = 0.57776416\n",
      "Iteration 31, loss = 0.57490627\n",
      "Iteration 32, loss = 0.57749100\n",
      "Iteration 33, loss = 0.57285314\n",
      "Iteration 34, loss = 0.57536897\n",
      "Iteration 35, loss = 0.57284208\n",
      "Iteration 36, loss = 0.57539149\n",
      "Iteration 37, loss = 0.57347690\n",
      "Iteration 38, loss = 0.57297289\n",
      "Iteration 39, loss = 0.57216652\n",
      "Iteration 40, loss = 0.57041012\n",
      "Iteration 41, loss = 0.57493184\n",
      "Iteration 42, loss = 0.57707890\n",
      "Iteration 43, loss = 0.57360777\n",
      "Iteration 44, loss = 0.57076121\n",
      "Iteration 45, loss = 0.57410431\n",
      "Iteration 46, loss = 0.57443318\n",
      "Iteration 47, loss = 0.57363676\n",
      "Iteration 48, loss = 0.56864849\n",
      "Iteration 49, loss = 0.56941150\n",
      "Iteration 50, loss = 0.56878551\n",
      "Iteration 51, loss = 0.56924011\n",
      "Iteration 52, loss = 0.56787784\n",
      "Iteration 53, loss = 0.56537503\n",
      "Iteration 54, loss = 0.56672653\n",
      "Iteration 55, loss = 0.56800519\n",
      "Iteration 56, loss = 0.56714742\n",
      "Iteration 57, loss = 0.56304491\n",
      "Iteration 58, loss = 0.56524415\n",
      "Iteration 59, loss = 0.56760832\n",
      "Iteration 60, loss = 0.56609823\n",
      "Iteration 61, loss = 0.56186202\n",
      "Iteration 62, loss = 0.56339492\n",
      "Iteration 63, loss = 0.56317380\n",
      "Iteration 64, loss = 0.56220251\n",
      "Iteration 65, loss = 0.55966246\n",
      "Iteration 66, loss = 0.56118505\n",
      "Iteration 67, loss = 0.55969289\n",
      "Iteration 68, loss = 0.56278047\n",
      "Iteration 69, loss = 0.55866132\n",
      "Iteration 70, loss = 0.56289537\n",
      "Iteration 71, loss = 0.55765500\n",
      "Iteration 72, loss = 0.55795254\n",
      "Iteration 73, loss = 0.56585481\n",
      "Iteration 74, loss = 0.55742495\n",
      "Iteration 75, loss = 0.55849606\n",
      "Iteration 76, loss = 0.55498322\n",
      "Iteration 77, loss = 0.55522902\n",
      "Iteration 78, loss = 0.56112605\n",
      "Iteration 79, loss = 0.55706505\n",
      "Iteration 80, loss = 0.55699036\n",
      "Iteration 81, loss = 0.55499738\n",
      "Iteration 82, loss = 0.55536853\n",
      "Iteration 83, loss = 0.55515759\n",
      "Iteration 84, loss = 0.55399469\n",
      "Iteration 85, loss = 0.55911894\n",
      "Iteration 86, loss = 0.56182063\n",
      "Iteration 87, loss = 0.55613142\n",
      "Iteration 88, loss = 0.55481163\n",
      "Iteration 89, loss = 0.55922792\n",
      "Iteration 90, loss = 0.55651666\n",
      "Iteration 91, loss = 0.55472577\n",
      "Iteration 92, loss = 0.56062435\n",
      "Iteration 93, loss = 0.55447571\n",
      "Iteration 94, loss = 0.55151410\n",
      "Iteration 95, loss = 0.55106102\n",
      "Iteration 96, loss = 0.55385762\n",
      "Iteration 97, loss = 0.55227175\n",
      "Iteration 98, loss = 0.55102080\n",
      "Iteration 99, loss = 0.55084030\n",
      "Iteration 100, loss = 0.55143198\n",
      "Iteration 101, loss = 0.55266022\n",
      "Iteration 102, loss = 0.55151460\n",
      "Iteration 103, loss = 0.55380557\n",
      "Iteration 104, loss = 0.54967644\n",
      "Iteration 105, loss = 0.54875577\n",
      "Iteration 106, loss = 0.54535546\n",
      "Iteration 107, loss = 0.55034073\n",
      "Iteration 108, loss = 0.54980273\n",
      "Iteration 109, loss = 0.55074636\n",
      "Iteration 110, loss = 0.55127748\n",
      "Iteration 111, loss = 0.54867129\n",
      "Iteration 112, loss = 0.55162528\n",
      "Iteration 113, loss = 0.54788915\n",
      "Iteration 114, loss = 0.54699030\n",
      "Iteration 115, loss = 0.55175602\n",
      "Iteration 116, loss = 0.54750061\n",
      "Iteration 117, loss = 0.54717468\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66407413\n",
      "Iteration 2, loss = 0.62879840\n",
      "Iteration 3, loss = 0.61581431\n",
      "Iteration 4, loss = 0.62008995\n",
      "Iteration 5, loss = 0.60844302\n",
      "Iteration 6, loss = 0.60076577\n",
      "Iteration 7, loss = 0.59749083\n",
      "Iteration 8, loss = 0.59658203\n",
      "Iteration 9, loss = 0.59874283\n",
      "Iteration 10, loss = 0.59793200\n",
      "Iteration 11, loss = 0.59363960\n",
      "Iteration 12, loss = 0.59296806\n",
      "Iteration 13, loss = 0.58654185\n",
      "Iteration 14, loss = 0.59370381\n",
      "Iteration 15, loss = 0.59793732\n",
      "Iteration 16, loss = 0.58858963\n",
      "Iteration 17, loss = 0.58265609\n",
      "Iteration 18, loss = 0.58904096\n",
      "Iteration 19, loss = 0.59020836\n",
      "Iteration 20, loss = 0.58576136\n",
      "Iteration 21, loss = 0.59018589\n",
      "Iteration 22, loss = 0.58143784\n",
      "Iteration 23, loss = 0.57915992\n",
      "Iteration 24, loss = 0.57713960\n",
      "Iteration 25, loss = 0.57901363\n",
      "Iteration 26, loss = 0.57728692\n",
      "Iteration 27, loss = 0.58672641\n",
      "Iteration 28, loss = 0.58044374\n",
      "Iteration 29, loss = 0.57793732\n",
      "Iteration 30, loss = 0.57562217\n",
      "Iteration 31, loss = 0.57275488\n",
      "Iteration 32, loss = 0.57625101\n",
      "Iteration 33, loss = 0.57319198\n",
      "Iteration 34, loss = 0.57320661\n",
      "Iteration 35, loss = 0.57236087\n",
      "Iteration 36, loss = 0.57382716\n",
      "Iteration 37, loss = 0.57243253\n",
      "Iteration 38, loss = 0.57374649\n",
      "Iteration 39, loss = 0.56988816\n",
      "Iteration 40, loss = 0.56693034\n",
      "Iteration 41, loss = 0.56883112\n",
      "Iteration 42, loss = 0.56864965\n",
      "Iteration 43, loss = 0.56747731\n",
      "Iteration 44, loss = 0.56893407\n",
      "Iteration 45, loss = 0.57242609\n",
      "Iteration 46, loss = 0.56884894\n",
      "Iteration 47, loss = 0.56909346\n",
      "Iteration 48, loss = 0.56587592\n",
      "Iteration 49, loss = 0.56496205\n",
      "Iteration 50, loss = 0.56405841\n",
      "Iteration 51, loss = 0.56483349\n",
      "Iteration 52, loss = 0.56631297\n",
      "Iteration 53, loss = 0.56412651\n",
      "Iteration 54, loss = 0.56215420\n",
      "Iteration 55, loss = 0.56205312\n",
      "Iteration 56, loss = 0.56498798\n",
      "Iteration 57, loss = 0.56357089\n",
      "Iteration 58, loss = 0.56412339\n",
      "Iteration 59, loss = 0.56393774\n",
      "Iteration 60, loss = 0.55918770\n",
      "Iteration 61, loss = 0.56073749\n",
      "Iteration 62, loss = 0.56170968\n",
      "Iteration 63, loss = 0.55983684\n",
      "Iteration 64, loss = 0.55807811\n",
      "Iteration 65, loss = 0.56089894\n",
      "Iteration 66, loss = 0.56736944\n",
      "Iteration 67, loss = 0.56021139\n",
      "Iteration 68, loss = 0.55796489\n",
      "Iteration 69, loss = 0.55857023\n",
      "Iteration 70, loss = 0.55953137\n",
      "Iteration 71, loss = 0.56043566\n",
      "Iteration 72, loss = 0.55951546\n",
      "Iteration 73, loss = 0.55800231\n",
      "Iteration 74, loss = 0.55746018\n",
      "Iteration 75, loss = 0.55611990\n",
      "Iteration 76, loss = 0.56075886\n",
      "Iteration 77, loss = 0.55741296\n",
      "Iteration 78, loss = 0.55706974\n",
      "Iteration 79, loss = 0.55944733\n",
      "Iteration 80, loss = 0.55828814\n",
      "Iteration 81, loss = 0.56295859\n",
      "Iteration 82, loss = 0.55506545\n",
      "Iteration 83, loss = 0.55574698\n",
      "Iteration 84, loss = 0.55394784\n",
      "Iteration 85, loss = 0.55586964\n",
      "Iteration 86, loss = 0.55239659\n",
      "Iteration 87, loss = 0.55455833\n",
      "Iteration 88, loss = 0.55349385\n",
      "Iteration 89, loss = 0.55560903\n",
      "Iteration 90, loss = 0.55349167\n",
      "Iteration 91, loss = 0.55252397\n",
      "Iteration 92, loss = 0.55232423\n",
      "Iteration 93, loss = 0.54942505\n",
      "Iteration 94, loss = 0.55079806\n",
      "Iteration 95, loss = 0.55462194\n",
      "Iteration 96, loss = 0.54975114\n",
      "Iteration 97, loss = 0.55395992\n",
      "Iteration 98, loss = 0.55301007\n",
      "Iteration 99, loss = 0.54998728\n",
      "Iteration 100, loss = 0.55119957\n",
      "Iteration 101, loss = 0.55419109\n",
      "Iteration 102, loss = 0.55002671\n",
      "Iteration 103, loss = 0.54734706\n",
      "Iteration 104, loss = 0.55371436\n",
      "Iteration 105, loss = 0.55054185\n",
      "Iteration 106, loss = 0.54955552\n",
      "Iteration 107, loss = 0.55083107\n",
      "Iteration 108, loss = 0.54910440\n",
      "Iteration 109, loss = 0.55007373\n",
      "Iteration 110, loss = 0.55223324\n",
      "Iteration 111, loss = 0.55159016\n",
      "Iteration 112, loss = 0.54992820\n",
      "Iteration 113, loss = 0.55096914\n",
      "Iteration 114, loss = 0.55166323\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67437132\n",
      "Iteration 2, loss = 0.63672240\n",
      "Iteration 3, loss = 0.61715088\n",
      "Iteration 4, loss = 0.60776070\n",
      "Iteration 5, loss = 0.60937170\n",
      "Iteration 6, loss = 0.60485486\n",
      "Iteration 7, loss = 0.60486244\n",
      "Iteration 8, loss = 0.59966894\n",
      "Iteration 9, loss = 0.60146627\n",
      "Iteration 10, loss = 0.59707860\n",
      "Iteration 11, loss = 0.59343953\n",
      "Iteration 12, loss = 0.60087082\n",
      "Iteration 13, loss = 0.59729069\n",
      "Iteration 14, loss = 0.59731288\n",
      "Iteration 15, loss = 0.58865530\n",
      "Iteration 16, loss = 0.58702167\n",
      "Iteration 17, loss = 0.59636468\n",
      "Iteration 18, loss = 0.58584138\n",
      "Iteration 19, loss = 0.58594425\n",
      "Iteration 20, loss = 0.59390345\n",
      "Iteration 21, loss = 0.58379404\n",
      "Iteration 22, loss = 0.57966628\n",
      "Iteration 23, loss = 0.57984780\n",
      "Iteration 24, loss = 0.58276528\n",
      "Iteration 25, loss = 0.58088077\n",
      "Iteration 26, loss = 0.57715173\n",
      "Iteration 27, loss = 0.58558278\n",
      "Iteration 28, loss = 0.58189665\n",
      "Iteration 29, loss = 0.58047916\n",
      "Iteration 30, loss = 0.58261732\n",
      "Iteration 31, loss = 0.57878187\n",
      "Iteration 32, loss = 0.57392155\n",
      "Iteration 33, loss = 0.57805994\n",
      "Iteration 34, loss = 0.57794789\n",
      "Iteration 35, loss = 0.57098395\n",
      "Iteration 36, loss = 0.57308121\n",
      "Iteration 37, loss = 0.56961793\n",
      "Iteration 38, loss = 0.56969187\n",
      "Iteration 39, loss = 0.57630382\n",
      "Iteration 40, loss = 0.57049885\n",
      "Iteration 41, loss = 0.57068770\n",
      "Iteration 42, loss = 0.56913866\n",
      "Iteration 43, loss = 0.57745461\n",
      "Iteration 44, loss = 0.57800546\n",
      "Iteration 45, loss = 0.56733749\n",
      "Iteration 46, loss = 0.56643869\n",
      "Iteration 47, loss = 0.56494489\n",
      "Iteration 48, loss = 0.56500978\n",
      "Iteration 49, loss = 0.57053110\n",
      "Iteration 50, loss = 0.57325597\n",
      "Iteration 51, loss = 0.57427111\n",
      "Iteration 52, loss = 0.57164310\n",
      "Iteration 53, loss = 0.56455298\n",
      "Iteration 54, loss = 0.56530879\n",
      "Iteration 55, loss = 0.56431971\n",
      "Iteration 56, loss = 0.56537212\n",
      "Iteration 57, loss = 0.56727723\n",
      "Iteration 58, loss = 0.56547160\n",
      "Iteration 59, loss = 0.56287842\n",
      "Iteration 60, loss = 0.56454315\n",
      "Iteration 61, loss = 0.56391710\n",
      "Iteration 62, loss = 0.56220050\n",
      "Iteration 63, loss = 0.56234331\n",
      "Iteration 64, loss = 0.56366976\n",
      "Iteration 65, loss = 0.56202956\n",
      "Iteration 66, loss = 0.55813908\n",
      "Iteration 67, loss = 0.55878494\n",
      "Iteration 68, loss = 0.56042415\n",
      "Iteration 69, loss = 0.56122311\n",
      "Iteration 70, loss = 0.56244335\n",
      "Iteration 71, loss = 0.55849092\n",
      "Iteration 72, loss = 0.55957209\n",
      "Iteration 73, loss = 0.55740694\n",
      "Iteration 74, loss = 0.55844004\n",
      "Iteration 75, loss = 0.55604613\n",
      "Iteration 76, loss = 0.55474568\n",
      "Iteration 77, loss = 0.55592015\n",
      "Iteration 78, loss = 0.55538187\n",
      "Iteration 79, loss = 0.55405941\n",
      "Iteration 80, loss = 0.55741230\n",
      "Iteration 81, loss = 0.56311732\n",
      "Iteration 82, loss = 0.56401102\n",
      "Iteration 83, loss = 0.55809599\n",
      "Iteration 84, loss = 0.55524462\n",
      "Iteration 85, loss = 0.55376569\n",
      "Iteration 86, loss = 0.55148251\n",
      "Iteration 87, loss = 0.55600501\n",
      "Iteration 88, loss = 0.55625761\n",
      "Iteration 89, loss = 0.55744300\n",
      "Iteration 90, loss = 0.55643526\n",
      "Iteration 91, loss = 0.55655542\n",
      "Iteration 92, loss = 0.55449815\n",
      "Iteration 93, loss = 0.54982623\n",
      "Iteration 94, loss = 0.55248828\n",
      "Iteration 95, loss = 0.55660229\n",
      "Iteration 96, loss = 0.55031218\n",
      "Iteration 97, loss = 0.55078239\n",
      "Iteration 98, loss = 0.55217322\n",
      "Iteration 99, loss = 0.55303069\n",
      "Iteration 100, loss = 0.54940325\n",
      "Iteration 101, loss = 0.55417991\n",
      "Iteration 102, loss = 0.55517150\n",
      "Iteration 103, loss = 0.55216593\n",
      "Iteration 104, loss = 0.54993862\n",
      "Iteration 105, loss = 0.54761827\n",
      "Iteration 106, loss = 0.54895560\n",
      "Iteration 107, loss = 0.54892850\n",
      "Iteration 108, loss = 0.54774761\n",
      "Iteration 109, loss = 0.55078943\n",
      "Iteration 110, loss = 0.55249667\n",
      "Iteration 111, loss = 0.54910945\n",
      "Iteration 112, loss = 0.54566034\n",
      "Iteration 113, loss = 0.54877767\n",
      "Iteration 114, loss = 0.55010714\n",
      "Iteration 115, loss = 0.54470286\n",
      "Iteration 116, loss = 0.54618067\n",
      "Iteration 117, loss = 0.54615172\n",
      "Iteration 118, loss = 0.55339437\n",
      "Iteration 119, loss = 0.54547819\n",
      "Iteration 120, loss = 0.54251419\n",
      "Iteration 121, loss = 0.54177643\n",
      "Iteration 122, loss = 0.54341481\n",
      "Iteration 123, loss = 0.54408403\n",
      "Iteration 124, loss = 0.54687753\n",
      "Iteration 125, loss = 0.54553744\n",
      "Iteration 126, loss = 0.54237111\n",
      "Iteration 127, loss = 0.54265007\n",
      "Iteration 128, loss = 0.54206347\n",
      "Iteration 129, loss = 0.54254468\n",
      "Iteration 130, loss = 0.54985728\n",
      "Iteration 131, loss = 0.54381896\n",
      "Iteration 132, loss = 0.54884337\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67554372\n",
      "Iteration 2, loss = 0.62035847\n",
      "Iteration 3, loss = 0.61212810\n",
      "Iteration 4, loss = 0.60304885\n",
      "Iteration 5, loss = 0.60188884\n",
      "Iteration 6, loss = 0.59856240\n",
      "Iteration 7, loss = 0.61220787\n",
      "Iteration 8, loss = 0.60302688\n",
      "Iteration 9, loss = 0.59098597\n",
      "Iteration 10, loss = 0.58899138\n",
      "Iteration 11, loss = 0.59185548\n",
      "Iteration 12, loss = 0.59163427\n",
      "Iteration 13, loss = 0.58692134\n",
      "Iteration 14, loss = 0.58838063\n",
      "Iteration 15, loss = 0.58561135\n",
      "Iteration 16, loss = 0.58549699\n",
      "Iteration 17, loss = 0.58461574\n",
      "Iteration 18, loss = 0.58360944\n",
      "Iteration 19, loss = 0.58121864\n",
      "Iteration 20, loss = 0.57721132\n",
      "Iteration 21, loss = 0.57760240\n",
      "Iteration 22, loss = 0.57442522\n",
      "Iteration 23, loss = 0.58307586\n",
      "Iteration 24, loss = 0.57623098\n",
      "Iteration 25, loss = 0.57624074\n",
      "Iteration 26, loss = 0.58262902\n",
      "Iteration 27, loss = 0.57703001\n",
      "Iteration 28, loss = 0.57579291\n",
      "Iteration 29, loss = 0.57177685\n",
      "Iteration 30, loss = 0.57292698\n",
      "Iteration 31, loss = 0.57793035\n",
      "Iteration 32, loss = 0.57186957\n",
      "Iteration 33, loss = 0.57195296\n",
      "Iteration 34, loss = 0.57144118\n",
      "Iteration 35, loss = 0.57993444\n",
      "Iteration 36, loss = 0.57068418\n",
      "Iteration 37, loss = 0.56887649\n",
      "Iteration 38, loss = 0.56562875\n",
      "Iteration 39, loss = 0.56660730\n",
      "Iteration 40, loss = 0.57470192\n",
      "Iteration 41, loss = 0.56604518\n",
      "Iteration 42, loss = 0.56644111\n",
      "Iteration 43, loss = 0.56513418\n",
      "Iteration 44, loss = 0.56874206\n",
      "Iteration 45, loss = 0.56842065\n",
      "Iteration 46, loss = 0.56655132\n",
      "Iteration 47, loss = 0.56292571\n",
      "Iteration 48, loss = 0.56815416\n",
      "Iteration 49, loss = 0.56300278\n",
      "Iteration 50, loss = 0.56472427\n",
      "Iteration 51, loss = 0.56652237\n",
      "Iteration 52, loss = 0.56030230\n",
      "Iteration 53, loss = 0.56338154\n",
      "Iteration 54, loss = 0.56220872\n",
      "Iteration 55, loss = 0.56509582\n",
      "Iteration 56, loss = 0.56023610\n",
      "Iteration 57, loss = 0.55940972\n",
      "Iteration 58, loss = 0.55720654\n",
      "Iteration 59, loss = 0.55972441\n",
      "Iteration 60, loss = 0.56085582\n",
      "Iteration 61, loss = 0.55989197\n",
      "Iteration 62, loss = 0.55655105\n",
      "Iteration 63, loss = 0.55982398\n",
      "Iteration 64, loss = 0.55976232\n",
      "Iteration 65, loss = 0.55706609\n",
      "Iteration 66, loss = 0.55817409\n",
      "Iteration 67, loss = 0.55973924\n",
      "Iteration 68, loss = 0.55675945\n",
      "Iteration 69, loss = 0.55408772\n",
      "Iteration 70, loss = 0.55583645\n",
      "Iteration 71, loss = 0.55506734\n",
      "Iteration 72, loss = 0.55534044\n",
      "Iteration 73, loss = 0.55679916\n",
      "Iteration 74, loss = 0.55945055\n",
      "Iteration 75, loss = 0.55841734\n",
      "Iteration 76, loss = 0.55585639\n",
      "Iteration 77, loss = 0.55781615\n",
      "Iteration 78, loss = 0.55391361\n",
      "Iteration 79, loss = 0.55490354\n",
      "Iteration 80, loss = 0.55253833\n",
      "Iteration 81, loss = 0.55120543\n",
      "Iteration 82, loss = 0.55336751\n",
      "Iteration 83, loss = 0.55178778\n",
      "Iteration 84, loss = 0.55599660\n",
      "Iteration 85, loss = 0.55357729\n",
      "Iteration 86, loss = 0.55378829\n",
      "Iteration 87, loss = 0.54978630\n",
      "Iteration 88, loss = 0.55533953\n",
      "Iteration 89, loss = 0.55787337\n",
      "Iteration 90, loss = 0.55397131\n",
      "Iteration 91, loss = 0.55252168\n",
      "Iteration 92, loss = 0.55004580\n",
      "Iteration 93, loss = 0.54928762\n",
      "Iteration 94, loss = 0.54962093\n",
      "Iteration 95, loss = 0.55008359\n",
      "Iteration 96, loss = 0.55198531\n",
      "Iteration 97, loss = 0.54666658\n",
      "Iteration 98, loss = 0.54831364\n",
      "Iteration 99, loss = 0.54571578\n",
      "Iteration 100, loss = 0.54941788\n",
      "Iteration 101, loss = 0.55136563\n",
      "Iteration 102, loss = 0.54751366\n",
      "Iteration 103, loss = 0.54751547\n",
      "Iteration 104, loss = 0.54488538\n",
      "Iteration 105, loss = 0.54713761\n",
      "Iteration 106, loss = 0.54685171\n",
      "Iteration 107, loss = 0.55080266\n",
      "Iteration 108, loss = 0.54596460\n",
      "Iteration 109, loss = 0.54630231\n",
      "Iteration 110, loss = 0.54385322\n",
      "Iteration 111, loss = 0.54683750\n",
      "Iteration 112, loss = 0.54507801\n",
      "Iteration 113, loss = 0.54990486\n",
      "Iteration 114, loss = 0.54115475\n",
      "Iteration 115, loss = 0.54654417\n",
      "Iteration 116, loss = 0.55046792\n",
      "Iteration 117, loss = 0.54671545\n",
      "Iteration 118, loss = 0.54335816\n",
      "Iteration 119, loss = 0.54310556\n",
      "Iteration 120, loss = 0.54490563\n",
      "Iteration 121, loss = 0.55173004\n",
      "Iteration 122, loss = 0.54589092\n",
      "Iteration 123, loss = 0.54521469\n",
      "Iteration 124, loss = 0.54584447\n",
      "Iteration 125, loss = 0.54902152\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67873805\n",
      "Iteration 2, loss = 0.62608836\n",
      "Iteration 3, loss = 0.63827840\n",
      "Iteration 4, loss = 0.62793453\n",
      "Iteration 5, loss = 0.61031009\n",
      "Iteration 6, loss = 0.60703469\n",
      "Iteration 7, loss = 0.60126874\n",
      "Iteration 8, loss = 0.59740829\n",
      "Iteration 9, loss = 0.59711766\n",
      "Iteration 10, loss = 0.60465815\n",
      "Iteration 11, loss = 0.59182063\n",
      "Iteration 12, loss = 0.59823262\n",
      "Iteration 13, loss = 0.59062721\n",
      "Iteration 14, loss = 0.59301807\n",
      "Iteration 15, loss = 0.58808583\n",
      "Iteration 16, loss = 0.58808783\n",
      "Iteration 17, loss = 0.58665009\n",
      "Iteration 18, loss = 0.58450608\n",
      "Iteration 19, loss = 0.58703448\n",
      "Iteration 20, loss = 0.58610479\n",
      "Iteration 21, loss = 0.58283441\n",
      "Iteration 22, loss = 0.58717812\n",
      "Iteration 23, loss = 0.59265965\n",
      "Iteration 24, loss = 0.58139205\n",
      "Iteration 25, loss = 0.58454405\n",
      "Iteration 26, loss = 0.58051129\n",
      "Iteration 27, loss = 0.57857595\n",
      "Iteration 28, loss = 0.57930610\n",
      "Iteration 29, loss = 0.57801296\n",
      "Iteration 30, loss = 0.57534963\n",
      "Iteration 31, loss = 0.57870537\n",
      "Iteration 32, loss = 0.57337712\n",
      "Iteration 33, loss = 0.57319682\n",
      "Iteration 34, loss = 0.57450753\n",
      "Iteration 35, loss = 0.57738492\n",
      "Iteration 36, loss = 0.57228883\n",
      "Iteration 37, loss = 0.57272337\n",
      "Iteration 38, loss = 0.58084430\n",
      "Iteration 39, loss = 0.57606532\n",
      "Iteration 40, loss = 0.57512618\n",
      "Iteration 41, loss = 0.57137879\n",
      "Iteration 42, loss = 0.57104270\n",
      "Iteration 43, loss = 0.57108297\n",
      "Iteration 44, loss = 0.57092962\n",
      "Iteration 45, loss = 0.57507147\n",
      "Iteration 46, loss = 0.57294941\n",
      "Iteration 47, loss = 0.56926003\n",
      "Iteration 48, loss = 0.56711722\n",
      "Iteration 49, loss = 0.57390957\n",
      "Iteration 50, loss = 0.57318472\n",
      "Iteration 51, loss = 0.56657640\n",
      "Iteration 52, loss = 0.57544475\n",
      "Iteration 53, loss = 0.57029245\n",
      "Iteration 54, loss = 0.56807960\n",
      "Iteration 55, loss = 0.56329498\n",
      "Iteration 56, loss = 0.56251646\n",
      "Iteration 57, loss = 0.56329410\n",
      "Iteration 58, loss = 0.56261211\n",
      "Iteration 59, loss = 0.56211163\n",
      "Iteration 60, loss = 0.56278598\n",
      "Iteration 61, loss = 0.56466733\n",
      "Iteration 62, loss = 0.56405301\n",
      "Iteration 63, loss = 0.56504724\n",
      "Iteration 64, loss = 0.56404106\n",
      "Iteration 65, loss = 0.56108810\n",
      "Iteration 66, loss = 0.56225194\n",
      "Iteration 67, loss = 0.56123287\n",
      "Iteration 68, loss = 0.56107291\n",
      "Iteration 69, loss = 0.56228536\n",
      "Iteration 70, loss = 0.56080872\n",
      "Iteration 71, loss = 0.56089816\n",
      "Iteration 72, loss = 0.56098921\n",
      "Iteration 73, loss = 0.55992086\n",
      "Iteration 74, loss = 0.55844783\n",
      "Iteration 75, loss = 0.56302604\n",
      "Iteration 76, loss = 0.55837141\n",
      "Iteration 77, loss = 0.56022345\n",
      "Iteration 78, loss = 0.56409683\n",
      "Iteration 79, loss = 0.56030765\n",
      "Iteration 80, loss = 0.56450469\n",
      "Iteration 81, loss = 0.56179106\n",
      "Iteration 82, loss = 0.55681682\n",
      "Iteration 83, loss = 0.55794909\n",
      "Iteration 84, loss = 0.55499674\n",
      "Iteration 85, loss = 0.55633037\n",
      "Iteration 86, loss = 0.55667698\n",
      "Iteration 87, loss = 0.55565543\n",
      "Iteration 88, loss = 0.55734184\n",
      "Iteration 89, loss = 0.56002365\n",
      "Iteration 90, loss = 0.55997525\n",
      "Iteration 91, loss = 0.55472082\n",
      "Iteration 92, loss = 0.55755360\n",
      "Iteration 93, loss = 0.55378138\n",
      "Iteration 94, loss = 0.55654980\n",
      "Iteration 95, loss = 0.55399632\n",
      "Iteration 96, loss = 0.55254224\n",
      "Iteration 97, loss = 0.55565618\n",
      "Iteration 98, loss = 0.55603204\n",
      "Iteration 99, loss = 0.55437880\n",
      "Iteration 100, loss = 0.55293055\n",
      "Iteration 101, loss = 0.56065523\n",
      "Iteration 102, loss = 0.55188126\n",
      "Iteration 103, loss = 0.55179350\n",
      "Iteration 104, loss = 0.55820567\n",
      "Iteration 105, loss = 0.55112979\n",
      "Iteration 106, loss = 0.55529270\n",
      "Iteration 107, loss = 0.55148086\n",
      "Iteration 108, loss = 0.55243128\n",
      "Iteration 109, loss = 0.55620922\n",
      "Iteration 110, loss = 0.55448921\n",
      "Iteration 111, loss = 0.54995229\n",
      "Iteration 112, loss = 0.55127320\n",
      "Iteration 113, loss = 0.55159409\n",
      "Iteration 114, loss = 0.55075305\n",
      "Iteration 115, loss = 0.54897937\n",
      "Iteration 116, loss = 0.54870379\n",
      "Iteration 117, loss = 0.54823154\n",
      "Iteration 118, loss = 0.54799977\n",
      "Iteration 119, loss = 0.54867290\n",
      "Iteration 120, loss = 0.54860631\n",
      "Iteration 121, loss = 0.55180776\n",
      "Iteration 122, loss = 0.54877591\n",
      "Iteration 123, loss = 0.54748180\n",
      "Iteration 124, loss = 0.54620933\n",
      "Iteration 125, loss = 0.55313352\n",
      "Iteration 126, loss = 0.55873163\n",
      "Iteration 127, loss = 0.54953089\n",
      "Iteration 128, loss = 0.54783572\n",
      "Iteration 129, loss = 0.54783792\n",
      "Iteration 130, loss = 0.54814452\n",
      "Iteration 131, loss = 0.54772450\n",
      "Iteration 132, loss = 0.55112800\n",
      "Iteration 133, loss = 0.54792154\n",
      "Iteration 134, loss = 0.54679341\n",
      "Iteration 135, loss = 0.54943227\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66093714\n",
      "Iteration 2, loss = 0.61996269\n",
      "Iteration 3, loss = 0.62074623\n",
      "Iteration 4, loss = 0.61771947\n",
      "Iteration 5, loss = 0.60346123\n",
      "Iteration 6, loss = 0.60273703\n",
      "Iteration 7, loss = 0.61487779\n",
      "Iteration 8, loss = 0.59691294\n",
      "Iteration 9, loss = 0.59542626\n",
      "Iteration 10, loss = 0.59532359\n",
      "Iteration 11, loss = 0.59577594\n",
      "Iteration 12, loss = 0.58907993\n",
      "Iteration 13, loss = 0.59032257\n",
      "Iteration 14, loss = 0.59079563\n",
      "Iteration 15, loss = 0.59060724\n",
      "Iteration 16, loss = 0.60569929\n",
      "Iteration 17, loss = 0.58926596\n",
      "Iteration 18, loss = 0.58670763\n",
      "Iteration 19, loss = 0.58258243\n",
      "Iteration 20, loss = 0.58625548\n",
      "Iteration 21, loss = 0.58440511\n",
      "Iteration 22, loss = 0.57852196\n",
      "Iteration 23, loss = 0.58412829\n",
      "Iteration 24, loss = 0.58487327\n",
      "Iteration 25, loss = 0.58197950\n",
      "Iteration 26, loss = 0.57584895\n",
      "Iteration 27, loss = 0.57824769\n",
      "Iteration 28, loss = 0.57572153\n",
      "Iteration 29, loss = 0.57597065\n",
      "Iteration 30, loss = 0.57399140\n",
      "Iteration 31, loss = 0.57224750\n",
      "Iteration 32, loss = 0.57334387\n",
      "Iteration 33, loss = 0.57379818\n",
      "Iteration 34, loss = 0.57385795\n",
      "Iteration 35, loss = 0.57333486\n",
      "Iteration 36, loss = 0.57541028\n",
      "Iteration 37, loss = 0.57111452\n",
      "Iteration 38, loss = 0.57027882\n",
      "Iteration 39, loss = 0.56837225\n",
      "Iteration 40, loss = 0.56810295\n",
      "Iteration 41, loss = 0.56652924\n",
      "Iteration 42, loss = 0.56699541\n",
      "Iteration 43, loss = 0.57190510\n",
      "Iteration 44, loss = 0.56570128\n",
      "Iteration 45, loss = 0.56891824\n",
      "Iteration 46, loss = 0.57439690\n",
      "Iteration 47, loss = 0.56899210\n",
      "Iteration 48, loss = 0.56595142\n",
      "Iteration 49, loss = 0.56785260\n",
      "Iteration 50, loss = 0.56746083\n",
      "Iteration 51, loss = 0.56879991\n",
      "Iteration 52, loss = 0.56851212\n",
      "Iteration 53, loss = 0.56306211\n",
      "Iteration 54, loss = 0.56447852\n",
      "Iteration 55, loss = 0.56508033\n",
      "Iteration 56, loss = 0.56479760\n",
      "Iteration 57, loss = 0.57004419\n",
      "Iteration 58, loss = 0.56656325\n",
      "Iteration 59, loss = 0.56342868\n",
      "Iteration 60, loss = 0.56985791\n",
      "Iteration 61, loss = 0.56294746\n",
      "Iteration 62, loss = 0.56306105\n",
      "Iteration 63, loss = 0.56547797\n",
      "Iteration 64, loss = 0.56029785\n",
      "Iteration 65, loss = 0.56271573\n",
      "Iteration 66, loss = 0.56533863\n",
      "Iteration 67, loss = 0.56171480\n",
      "Iteration 68, loss = 0.56254477\n",
      "Iteration 69, loss = 0.55920840\n",
      "Iteration 70, loss = 0.55981376\n",
      "Iteration 71, loss = 0.57443033\n",
      "Iteration 72, loss = 0.55980464\n",
      "Iteration 73, loss = 0.56050205\n",
      "Iteration 74, loss = 0.55980968\n",
      "Iteration 75, loss = 0.56067298\n",
      "Iteration 76, loss = 0.55916753\n",
      "Iteration 77, loss = 0.56221115\n",
      "Iteration 78, loss = 0.55747923\n",
      "Iteration 79, loss = 0.55656872\n",
      "Iteration 80, loss = 0.55535983\n",
      "Iteration 81, loss = 0.56393232\n",
      "Iteration 82, loss = 0.55659907\n",
      "Iteration 83, loss = 0.55790954\n",
      "Iteration 84, loss = 0.55502939\n",
      "Iteration 85, loss = 0.55626984\n",
      "Iteration 86, loss = 0.55526194\n",
      "Iteration 87, loss = 0.55422428\n",
      "Iteration 88, loss = 0.55329750\n",
      "Iteration 89, loss = 0.55786827\n",
      "Iteration 90, loss = 0.55555846\n",
      "Iteration 91, loss = 0.55430944\n",
      "Iteration 92, loss = 0.55381650\n",
      "Iteration 93, loss = 0.55381108\n",
      "Iteration 94, loss = 0.55232841\n",
      "Iteration 95, loss = 0.56009510\n",
      "Iteration 96, loss = 0.55891548\n",
      "Iteration 97, loss = 0.55421781\n",
      "Iteration 98, loss = 0.55387517\n",
      "Iteration 99, loss = 0.55631399\n",
      "Iteration 100, loss = 0.55269958\n",
      "Iteration 101, loss = 0.55310730\n",
      "Iteration 102, loss = 0.55954551\n",
      "Iteration 103, loss = 0.55116738\n",
      "Iteration 104, loss = 0.55853328\n",
      "Iteration 105, loss = 0.55038032\n",
      "Iteration 106, loss = 0.55142440\n",
      "Iteration 107, loss = 0.55012153\n",
      "Iteration 108, loss = 0.55096327\n",
      "Iteration 109, loss = 0.55348317\n",
      "Iteration 110, loss = 0.55383523\n",
      "Iteration 111, loss = 0.55431600\n",
      "Iteration 112, loss = 0.55026495\n",
      "Iteration 113, loss = 0.54910769\n",
      "Iteration 114, loss = 0.54911497\n",
      "Iteration 115, loss = 0.54960259\n",
      "Iteration 116, loss = 0.54989495\n",
      "Iteration 117, loss = 0.55033523\n",
      "Iteration 118, loss = 0.54830150\n",
      "Iteration 119, loss = 0.54889224\n",
      "Iteration 120, loss = 0.54947254\n",
      "Iteration 121, loss = 0.54810148\n",
      "Iteration 122, loss = 0.55044043\n",
      "Iteration 123, loss = 0.55792712\n",
      "Iteration 124, loss = 0.54724117\n",
      "Iteration 125, loss = 0.54917691\n",
      "Iteration 126, loss = 0.54698894\n",
      "Iteration 127, loss = 0.54960478\n",
      "Iteration 128, loss = 0.54921794\n",
      "Iteration 129, loss = 0.55058436\n",
      "Iteration 130, loss = 0.54756364\n",
      "Iteration 131, loss = 0.54842680\n",
      "Iteration 132, loss = 0.55032913\n",
      "Iteration 133, loss = 0.54931836\n",
      "Iteration 134, loss = 0.54366969\n",
      "Iteration 135, loss = 0.54536819\n",
      "Iteration 136, loss = 0.54615755\n",
      "Iteration 137, loss = 0.54812928\n",
      "Iteration 138, loss = 0.54416067\n",
      "Iteration 139, loss = 0.55113949\n",
      "Iteration 140, loss = 0.55061888\n",
      "Iteration 141, loss = 0.55326621\n",
      "Iteration 142, loss = 0.55156050\n",
      "Iteration 143, loss = 0.54803883\n",
      "Iteration 144, loss = 0.54497320\n",
      "Iteration 145, loss = 0.54851980\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66401886\n",
      "Iteration 2, loss = 0.61718932\n",
      "Iteration 3, loss = 0.60285388\n",
      "Iteration 4, loss = 0.59794497\n",
      "Iteration 5, loss = 0.59852948\n",
      "Iteration 6, loss = 0.59609084\n",
      "Iteration 7, loss = 0.59036685\n",
      "Iteration 8, loss = 0.59287343\n",
      "Iteration 9, loss = 0.58790303\n",
      "Iteration 10, loss = 0.58802884\n",
      "Iteration 11, loss = 0.58339045\n",
      "Iteration 12, loss = 0.58079444\n",
      "Iteration 13, loss = 0.58270345\n",
      "Iteration 14, loss = 0.58067977\n",
      "Iteration 15, loss = 0.57743075\n",
      "Iteration 16, loss = 0.58038020\n",
      "Iteration 17, loss = 0.58628986\n",
      "Iteration 18, loss = 0.57917890\n",
      "Iteration 19, loss = 0.57667010\n",
      "Iteration 20, loss = 0.57520080\n",
      "Iteration 21, loss = 0.57302264\n",
      "Iteration 22, loss = 0.57177051\n",
      "Iteration 23, loss = 0.57274509\n",
      "Iteration 24, loss = 0.56865472\n",
      "Iteration 25, loss = 0.57144064\n",
      "Iteration 26, loss = 0.57449969\n",
      "Iteration 27, loss = 0.56944953\n",
      "Iteration 28, loss = 0.56918772\n",
      "Iteration 29, loss = 0.57200281\n",
      "Iteration 30, loss = 0.57281221\n",
      "Iteration 31, loss = 0.56489185\n",
      "Iteration 32, loss = 0.56724962\n",
      "Iteration 33, loss = 0.56648145\n",
      "Iteration 34, loss = 0.56503280\n",
      "Iteration 35, loss = 0.56551106\n",
      "Iteration 36, loss = 0.56332081\n",
      "Iteration 37, loss = 0.56196764\n",
      "Iteration 38, loss = 0.56921639\n",
      "Iteration 39, loss = 0.56310369\n",
      "Iteration 40, loss = 0.56593095\n",
      "Iteration 41, loss = 0.56329174\n",
      "Iteration 42, loss = 0.56038828\n",
      "Iteration 43, loss = 0.56376119\n",
      "Iteration 44, loss = 0.55752990\n",
      "Iteration 45, loss = 0.56326977\n",
      "Iteration 46, loss = 0.56233597\n",
      "Iteration 47, loss = 0.56511866\n",
      "Iteration 48, loss = 0.55879477\n",
      "Iteration 49, loss = 0.55708519\n",
      "Iteration 50, loss = 0.55623095\n",
      "Iteration 51, loss = 0.55599115\n",
      "Iteration 52, loss = 0.55616014\n",
      "Iteration 53, loss = 0.55436846\n",
      "Iteration 54, loss = 0.55577085\n",
      "Iteration 55, loss = 0.55622208\n",
      "Iteration 56, loss = 0.55642178\n",
      "Iteration 57, loss = 0.55247663\n",
      "Iteration 58, loss = 0.55223015\n",
      "Iteration 59, loss = 0.55054107\n",
      "Iteration 60, loss = 0.55030569\n",
      "Iteration 61, loss = 0.55271583\n",
      "Iteration 62, loss = 0.55743825\n",
      "Iteration 63, loss = 0.55977547\n",
      "Iteration 64, loss = 0.55575031\n",
      "Iteration 65, loss = 0.55043157\n",
      "Iteration 66, loss = 0.55746141\n",
      "Iteration 67, loss = 0.55087848\n",
      "Iteration 68, loss = 0.55029526\n",
      "Iteration 69, loss = 0.55240160\n",
      "Iteration 70, loss = 0.54858830\n",
      "Iteration 71, loss = 0.55239696\n",
      "Iteration 72, loss = 0.55277477\n",
      "Iteration 73, loss = 0.54628086\n",
      "Iteration 74, loss = 0.54642690\n",
      "Iteration 75, loss = 0.54861574\n",
      "Iteration 76, loss = 0.54870043\n",
      "Iteration 77, loss = 0.54727900\n",
      "Iteration 78, loss = 0.54668428\n",
      "Iteration 79, loss = 0.55398408\n",
      "Iteration 80, loss = 0.55148848\n",
      "Iteration 81, loss = 0.55506496\n",
      "Iteration 82, loss = 0.54513277\n",
      "Iteration 83, loss = 0.54835025\n",
      "Iteration 84, loss = 0.54464455\n",
      "Iteration 85, loss = 0.54804155\n",
      "Iteration 86, loss = 0.54641884\n",
      "Iteration 87, loss = 0.54626667\n",
      "Iteration 88, loss = 0.54792593\n",
      "Iteration 89, loss = 0.54593931\n",
      "Iteration 90, loss = 0.55211557\n",
      "Iteration 91, loss = 0.54735885\n",
      "Iteration 92, loss = 0.54913885\n",
      "Iteration 93, loss = 0.55349753\n",
      "Iteration 94, loss = 0.54462173\n",
      "Iteration 95, loss = 0.54269504\n",
      "Iteration 96, loss = 0.54239217\n",
      "Iteration 97, loss = 0.54604799\n",
      "Iteration 98, loss = 0.54526879\n",
      "Iteration 99, loss = 0.54632465\n",
      "Iteration 100, loss = 0.54389285\n",
      "Iteration 101, loss = 0.54908217\n",
      "Iteration 102, loss = 0.54981960\n",
      "Iteration 103, loss = 0.54344051\n",
      "Iteration 104, loss = 0.54371592\n",
      "Iteration 105, loss = 0.54198531\n",
      "Iteration 106, loss = 0.54582254\n",
      "Iteration 107, loss = 0.54472068\n",
      "Iteration 108, loss = 0.54141016\n",
      "Iteration 109, loss = 0.54539763\n",
      "Iteration 110, loss = 0.54309304\n",
      "Iteration 111, loss = 0.54132391\n",
      "Iteration 112, loss = 0.54420887\n",
      "Iteration 113, loss = 0.54000450\n",
      "Iteration 114, loss = 0.54066300\n",
      "Iteration 115, loss = 0.53922547\n",
      "Iteration 116, loss = 0.54007905\n",
      "Iteration 117, loss = 0.54317758\n",
      "Iteration 118, loss = 0.54787169\n",
      "Iteration 119, loss = 0.54160538\n",
      "Iteration 120, loss = 0.54073609\n",
      "Iteration 121, loss = 0.54763749\n",
      "Iteration 122, loss = 0.55033166\n",
      "Iteration 123, loss = 0.54153885\n",
      "Iteration 124, loss = 0.53785573\n",
      "Iteration 125, loss = 0.54370865\n",
      "Iteration 126, loss = 0.54214324\n",
      "Iteration 127, loss = 0.53858297\n",
      "Iteration 128, loss = 0.53798590\n",
      "Iteration 129, loss = 0.54078450\n",
      "Iteration 130, loss = 0.53869418\n",
      "Iteration 131, loss = 0.54028514\n",
      "Iteration 132, loss = 0.54012328\n",
      "Iteration 133, loss = 0.53682590\n",
      "Iteration 134, loss = 0.53639029\n",
      "Iteration 135, loss = 0.53643999\n",
      "Iteration 136, loss = 0.53657185\n",
      "Iteration 137, loss = 0.53761961\n",
      "Iteration 138, loss = 0.53940283\n",
      "Iteration 139, loss = 0.53894213\n",
      "Iteration 140, loss = 0.53878747\n",
      "Iteration 141, loss = 0.53579094\n",
      "Iteration 142, loss = 0.53722735\n",
      "Iteration 143, loss = 0.53369285\n",
      "Iteration 144, loss = 0.53969425\n",
      "Iteration 145, loss = 0.53989198\n",
      "Iteration 146, loss = 0.53960440\n",
      "Iteration 147, loss = 0.54757053\n",
      "Iteration 148, loss = 0.53606767\n",
      "Iteration 149, loss = 0.53997596\n",
      "Iteration 150, loss = 0.53633875\n",
      "Iteration 151, loss = 0.54024150\n",
      "Iteration 152, loss = 0.54094023\n",
      "Iteration 153, loss = 0.53543086\n",
      "Iteration 154, loss = 0.54048051\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[0.70375 0.71625 0.7175  0.71125 0.70875 0.695   0.69625 0.7225  0.74875\n",
      " 0.69875]\n",
      "0.711875\n"
     ]
    }
   ],
   "source": [
    "summoner_cv_scores = cross_val_score(\n",
    "    estimator = pipeline_neuralnetwork,\n",
    "    X = X_train,\n",
    "    y = y_train,\n",
    "    cv = kfold\n",
    ")\n",
    "\n",
    "print(summoner_cv_scores)\n",
    "print(np.mean(summoner_cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['summonerLevel', 'championLevel','championPoints', \n",
    "             'Assassin', 'Fighter', 'Mage', 'Marksman', 'Support', 'Tank',\n",
    "            'info.attack', 'info.defense', 'info.magic', 'info.difficulty',\n",
    "            'stats.hpregenperlevel',\t'stats.mpregen', 'stats.mpregenperlevel',\t'stats.crit',\t'stats.critperlevel',\n",
    "            'stats.attackdamage', 'stats.attackdamageperlevel', 'stats.attackspeedperlevel',\t'stats.attackspeed',\n",
    "            'final_gold', 'final_xp', 'final_abilityhaste', 'final_abilitypower', 'final_armor', 'final_armorpen',\n",
    "            'final_armorpenpercent', 'final_atkdmg', 'final_bns_armorpenpercent', 'final_bns_magicpenpercent', 'final_ccreduction',\n",
    "            'final_cdreduction', 'final_remaining_health', 'final_health', 'final_healthrgn', 'final_lifesteal', 'final_mppen',\n",
    "            'final_mgpenpercent', 'final_mgres', 'final_ms', 'final_omnivamp', 'final_physicalvamp', 'final_power', 'final_powermax',\n",
    "            'final_powerregen', 'final_spellvamp', 'final_currentgold', 'final_magicdmgdone', 'final_magicdmgdonetochamps', 'final_magicdmgtaken',\n",
    "            'final_physdmgdone', 'final_physdmgdonetochamps', 'final_physdmgtaken', 'final_dmgdone', 'final_dmgdonetochamps', 'final_dmgtaken', \n",
    "            'final_truedmgdone', 'final_truedmgdonetochamps', 'final_truedmgtaken', 'final_goldpersec', 'final_jungleminionskilled', 'final_lvl',\n",
    "            'final_minionskilled', 'final_jungleminionskilled', 'final_jungleminionskilled', 'final_jungleminionskilled', 'final_enemycontrolled'                  \n",
    "             ]\n",
    "\n",
    "X_test = pd.merge(X_test_original, team_positions, how='inner', on=['matchId', 'participantId'])\n",
    "X_test = pd.merge(X_test, champion_data, how='inner', left_on='championId', right_on='key')\n",
    "X_test = pd.merge(X_test, champion_types, how='inner', left_on='championId', right_on='key')\n",
    "X_test = pd.merge(X_test, test_last_frame_values, how='inner', on=['matchId', 'participantId'])\n",
    "X_test = pd.merge(X_test, champion_mastery, how='left', on=['summonerId', 'championId']).fillna(0)\n",
    "\n",
    "X_test = X_test.sort_values(['matchId', 'participantId'], ascending = [True, True]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "convert_team_values(X_test, variables)\n",
    "\n",
    "X_test_perlane = X_test.groupby(['matchId', 'teamPosition'])[['final_gold']].sum().pivot_table(values='final_gold', index='matchId', columns='teamPosition').reset_index().drop(columns=0)\n",
    "\n",
    "for lane in ['BOTTOM', 'JUNGLE', 'MIDDLE', 'TOP', 'UTILITY']:\n",
    "  X_test_perlane[f'{lane}'] = np.where(X_test_perlane[f'{lane}'] >= 0, 1, -1)\n",
    "\n",
    "X_test = (\n",
    "    X_test\n",
    "    .groupby(['matchId'])[variables]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "X_test = pd.merge(X_test, X_test_perlane, how='inner', on='matchId').reset_index(drop = True)\n",
    "X_test = pd.merge(X_test, testing_events, how='inner', on='matchId').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline_neuralnetwork.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matchId</th>\n",
       "      <th>winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8000</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8001</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8002</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8003</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8004</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   matchId  winner\n",
       "0     8000     100\n",
       "1     8001     200\n",
       "2     8002     200\n",
       "3     8003     200\n",
       "4     8004     200"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['winner'] = y_pred\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission.to_csv('../data/submission_neural_network_hidden_layers4x100_alpha005_activtahn_2023_03_30.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt at Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matchId</th>\n",
       "      <th>teamId</th>\n",
       "      <th>participantId</th>\n",
       "      <th>summonerId</th>\n",
       "      <th>summonerLevel</th>\n",
       "      <th>championId</th>\n",
       "      <th>Champion_Aatrox</th>\n",
       "      <th>Champion_Ahri</th>\n",
       "      <th>Champion_Akali</th>\n",
       "      <th>Champion_Akshan</th>\n",
       "      <th>...</th>\n",
       "      <th>Champion_Yone</th>\n",
       "      <th>Champion_Yorick</th>\n",
       "      <th>Champion_Yuumi</th>\n",
       "      <th>Champion_Zac</th>\n",
       "      <th>Champion_Zed</th>\n",
       "      <th>Champion_Zeri</th>\n",
       "      <th>Champion_Ziggs</th>\n",
       "      <th>Champion_Zilean</th>\n",
       "      <th>Champion_Zoe</th>\n",
       "      <th>Champion_Zyra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>303</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>616</td>\n",
       "      <td>517</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>667</td>\n",
       "      <td>127</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>860</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>325</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79995</th>\n",
       "      <td>7999</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "      <td>13979</td>\n",
       "      <td>595</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79996</th>\n",
       "      <td>7999</td>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "      <td>39643</td>\n",
       "      <td>38</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79997</th>\n",
       "      <td>7999</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "      <td>5570</td>\n",
       "      <td>498</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79998</th>\n",
       "      <td>7999</td>\n",
       "      <td>200</td>\n",
       "      <td>9</td>\n",
       "      <td>10228</td>\n",
       "      <td>733</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79999</th>\n",
       "      <td>7999</td>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>1684</td>\n",
       "      <td>574</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80000 rows × 168 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       matchId  teamId  participantId  summonerId  summonerLevel  championId  \\\n",
       "0            0     100              1           0            303          82   \n",
       "1            0     100              2           1            616         517   \n",
       "2            0     100              3           2            667         127   \n",
       "3            0     100              4           3            860          51   \n",
       "4            0     100              5           4            325          25   \n",
       "...        ...     ...            ...         ...            ...         ...   \n",
       "79995     7999     200              6       13979            595          83   \n",
       "79996     7999     200              7       39643             38         106   \n",
       "79997     7999     200              8        5570            498          34   \n",
       "79998     7999     200              9       10228            733          29   \n",
       "79999     7999     200             10        1684            574          26   \n",
       "\n",
       "       Champion_Aatrox  Champion_Ahri  Champion_Akali  Champion_Akshan  ...  \\\n",
       "0                    0              0               0                0  ...   \n",
       "1                    0              0               0                0  ...   \n",
       "2                    0              0               0                0  ...   \n",
       "3                    0              0               0                0  ...   \n",
       "4                    0              0               0                0  ...   \n",
       "...                ...            ...             ...              ...  ...   \n",
       "79995                0              0               0                0  ...   \n",
       "79996                0              0               0                0  ...   \n",
       "79997                0              0               0                0  ...   \n",
       "79998                0              0               0                0  ...   \n",
       "79999                0              0               0                0  ...   \n",
       "\n",
       "       Champion_Yone  Champion_Yorick  Champion_Yuumi  Champion_Zac  \\\n",
       "0                  0                0               0             0   \n",
       "1                  0                0               0             0   \n",
       "2                  0                0               0             0   \n",
       "3                  0                0               0             0   \n",
       "4                  0                0               0             0   \n",
       "...              ...              ...             ...           ...   \n",
       "79995              0                1               0             0   \n",
       "79996              0                0               0             0   \n",
       "79997              0                0               0             0   \n",
       "79998              0                0               0             0   \n",
       "79999              0                0               0             0   \n",
       "\n",
       "       Champion_Zed  Champion_Zeri  Champion_Ziggs  Champion_Zilean  \\\n",
       "0                 0              0               0                0   \n",
       "1                 0              0               0                0   \n",
       "2                 0              0               0                0   \n",
       "3                 0              0               0                0   \n",
       "4                 0              0               0                0   \n",
       "...             ...            ...             ...              ...   \n",
       "79995             0              0               0                0   \n",
       "79996             0              0               0                0   \n",
       "79997             0              0               0                0   \n",
       "79998             0              0               0                0   \n",
       "79999             0              0               0                1   \n",
       "\n",
       "       Champion_Zoe  Champion_Zyra  \n",
       "0                 0              0  \n",
       "1                 0              0  \n",
       "2                 0              0  \n",
       "3                 0              0  \n",
       "4                 0              0  \n",
       "...             ...            ...  \n",
       "79995             0              0  \n",
       "79996             0              0  \n",
       "79997             0              0  \n",
       "79998             0              0  \n",
       "79999             0              0  \n",
       "\n",
       "[80000 rows x 168 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(X_train_original, prefix='Champion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_original\n",
    "\n",
    "X_train = pd.get_dummies(X_train, prefix='Champion')\n",
    "champions_encoded = list(X_train.drop(columns=['matchId', 'teamId', 'participantId', 'summonerId', 'summonerLevel', 'championId']).columns.values)\n",
    "\n",
    "variables = ['summonerLevel', 'championLevel','championPoints', \n",
    "            # 'Assassin', 'Fighter', 'Mage', 'Marksman', 'Support', 'Tank',\n",
    "            # 'info.attack', 'info.defense', 'info.magic', 'info.difficulty',\n",
    "            # 'stats.hpregenperlevel',\t'stats.mpregen', 'stats.mpregenperlevel',\t'stats.crit',\t'stats.critperlevel',\n",
    "            # 'stats.attackdamage', 'stats.attackdamageperlevel', 'stats.attackspeedperlevel',\t'stats.attackspeed',\n",
    "            'final_gold', 'final_xp', 'final_abilityhaste', 'final_abilitypower', 'final_armor', 'final_armorpen',\n",
    "            'final_armorpenpercent', 'final_atkdmg', 'final_bns_armorpenpercent', 'final_bns_magicpenpercent', 'final_ccreduction',\n",
    "            'final_cdreduction', 'final_remaining_health', 'final_health', 'final_healthrgn', 'final_lifesteal', 'final_mppen',\n",
    "            'final_mgpenpercent', 'final_mgres', 'final_ms', 'final_omnivamp', 'final_physicalvamp', 'final_power', 'final_powermax',\n",
    "            'final_powerregen', 'final_spellvamp', 'final_currentgold', 'final_magicdmgdone', 'final_magicdmgdonetochamps', 'final_magicdmgtaken',\n",
    "            'final_physdmgdone', 'final_physdmgdonetochamps', 'final_physdmgtaken', 'final_dmgdone', 'final_dmgdonetochamps', 'final_dmgtaken', \n",
    "            'final_truedmgdone', 'final_truedmgdonetochamps', 'final_truedmgtaken', 'final_goldpersec', 'final_jungleminionskilled', 'final_lvl',\n",
    "            'final_minionskilled', 'final_jungleminionskilled', 'final_jungleminionskilled', 'final_jungleminionskilled', 'final_enemycontrolled'                  \n",
    "             ] + champions_encoded\n",
    "\n",
    "X_train = pd.merge(X_train, team_positions, how='inner', on=['matchId', 'participantId'])\n",
    "\n",
    "X_train = pd.merge(X_train, champion_data, how='inner', left_on='championId', right_on='key')\n",
    "X_train = pd.merge(X_train, champion_types, how='inner', left_on='championId', right_on='key')\n",
    "X_train = pd.merge(X_train, train_last_frame_values, how='inner', on=['matchId', 'participantId'])\n",
    "X_train = pd.merge(X_train, champion_mastery, how='left', on=['summonerId', 'championId']).fillna(0)\n",
    "\n",
    "X_train = X_train.sort_values(['matchId', 'participantId'], ascending = [True, True]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "convert_team_values(X_train, variables)\n",
    "\n",
    "X_train_perlane = X_train.groupby(['matchId', 'teamPosition'])[['final_gold']].sum().pivot_table(values='final_gold', index='matchId', columns='teamPosition').reset_index().drop(columns=0)\n",
    "\n",
    "for lane in ['BOTTOM', 'JUNGLE', 'MIDDLE', 'TOP', 'UTILITY']:\n",
    "  X_train_perlane[f'{lane}'] = np.where(X_train_perlane[f'{lane}'] >= 0, 1, -1)\n",
    "\n",
    "X_train = (\n",
    "    X_train\n",
    "    .groupby(['matchId'])[variables]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "X_train = pd.merge(X_train, X_train_perlane, how='inner', on='matchId').reset_index(drop = True)\n",
    "X_train = pd.merge(X_train, training_events, how='inner', on='matchId').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;gb&#x27;,\n",
       "                 GradientBoostingClassifier(learning_rate=0.01,\n",
       "                                            n_estimators=1000))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;gb&#x27;,\n",
       "                 GradientBoostingClassifier(learning_rate=0.01,\n",
       "                                            n_estimators=1000))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(learning_rate=0.01, n_estimators=1000)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('gb',\n",
       "                 GradientBoostingClassifier(learning_rate=0.01,\n",
       "                                            n_estimators=1000))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr = Pipeline(\n",
    "    steps = [\n",
    "        #('scaler', MinMaxScaler()),\n",
    "        ('gb', GradientBoostingClassifier(n_estimators = 1000, learning_rate=0.01))\n",
    "    ]\n",
    ")\n",
    "\n",
    "gbr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76125"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(\n",
    "    y_true = y_train,\n",
    "    y_pred = gbr.predict(X_train)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         100       0.76      0.77      0.77      4071\n",
      "         200       0.76      0.75      0.76      3929\n",
      "\n",
      "    accuracy                           0.76      8000\n",
      "   macro avg       0.76      0.76      0.76      8000\n",
      "weighted avg       0.76      0.76      0.76      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, gbr.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>final_gold</td>\n",
       "      <td>0.610688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>elite_monsters_killed</td>\n",
       "      <td>0.045315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>final_xp</td>\n",
       "      <td>0.043786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>championLevel</td>\n",
       "      <td>0.033521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Champion_AurelionSol</td>\n",
       "      <td>0.019381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>final_health</td>\n",
       "      <td>0.015361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>final_dmgdone</td>\n",
       "      <td>0.013412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>summonerLevel</td>\n",
       "      <td>0.012571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>BOTTOM</td>\n",
       "      <td>0.010613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Champion_Annie</td>\n",
       "      <td>0.009574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>final_armor</td>\n",
       "      <td>0.008748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>final_remaining_health</td>\n",
       "      <td>0.007875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>final_dmgtaken</td>\n",
       "      <td>0.006308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>final_abilitypower</td>\n",
       "      <td>0.006154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>matchId</td>\n",
       "      <td>0.006082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>final_healthrgn</td>\n",
       "      <td>0.005780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>final_magicdmgdone</td>\n",
       "      <td>0.005772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>final_currentgold</td>\n",
       "      <td>0.005580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>final_minionskilled</td>\n",
       "      <td>0.005314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Champion_Karma</td>\n",
       "      <td>0.005261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   variable  importance\n",
       "4                final_gold    0.610688\n",
       "221   elite_monsters_killed    0.045315\n",
       "5                  final_xp    0.043786\n",
       "2             championLevel    0.033521\n",
       "61     Champion_AurelionSol    0.019381\n",
       "17             final_health    0.015361\n",
       "37            final_dmgdone    0.013412\n",
       "1             summonerLevel    0.012571\n",
       "213                  BOTTOM    0.010613\n",
       "58           Champion_Annie    0.009574\n",
       "8               final_armor    0.008748\n",
       "16   final_remaining_health    0.007875\n",
       "39           final_dmgtaken    0.006308\n",
       "7        final_abilitypower    0.006154\n",
       "0                   matchId    0.006082\n",
       "18          final_healthrgn    0.005780\n",
       "31       final_magicdmgdone    0.005772\n",
       "30        final_currentgold    0.005580\n",
       "46      final_minionskilled    0.005314\n",
       "105          Champion_Karma    0.005261"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances = pd.DataFrame({\n",
    "    'variable': gbr.feature_names_in_,\n",
    "    'importance': gbr['gb'].feature_importances_\n",
    "})\n",
    "\n",
    "importances.sort_values('importance', ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test_original\n",
    "\n",
    "X_test = pd.get_dummies(X_test, prefix='Champion')\n",
    "champions_encoded = list(X_test.drop(columns=['matchId', 'teamId', 'participantId', 'summonerId', 'summonerLevel', 'championId']).columns.values)\n",
    "\n",
    "variables = ['summonerLevel', 'championLevel','championPoints', \n",
    "            # 'Assassin', 'Fighter', 'Mage', 'Marksman', 'Support', 'Tank',\n",
    "            # 'info.attack', 'info.defense', 'info.magic', 'info.difficulty',\n",
    "            # 'stats.hpregenperlevel',\t'stats.mpregen', 'stats.mpregenperlevel',\t'stats.crit',\t'stats.critperlevel',\n",
    "            # 'stats.attackdamage', 'stats.attackdamageperlevel', 'stats.attackspeedperlevel',\t'stats.attackspeed',\n",
    "            'final_gold', 'final_xp', 'final_abilityhaste', 'final_abilitypower', 'final_armor', 'final_armorpen',\n",
    "            'final_armorpenpercent', 'final_atkdmg', 'final_bns_armorpenpercent', 'final_bns_magicpenpercent', 'final_ccreduction',\n",
    "            'final_cdreduction', 'final_remaining_health', 'final_health', 'final_healthrgn', 'final_lifesteal', 'final_mppen',\n",
    "            'final_mgpenpercent', 'final_mgres', 'final_ms', 'final_omnivamp', 'final_physicalvamp', 'final_power', 'final_powermax',\n",
    "            'final_powerregen', 'final_spellvamp', 'final_currentgold', 'final_magicdmgdone', 'final_magicdmgdonetochamps', 'final_magicdmgtaken',\n",
    "            'final_physdmgdone', 'final_physdmgdonetochamps', 'final_physdmgtaken', 'final_dmgdone', 'final_dmgdonetochamps', 'final_dmgtaken', \n",
    "            'final_truedmgdone', 'final_truedmgdonetochamps', 'final_truedmgtaken', 'final_goldpersec', 'final_jungleminionskilled', 'final_lvl',\n",
    "            'final_minionskilled', 'final_jungleminionskilled', 'final_jungleminionskilled', 'final_jungleminionskilled', 'final_enemycontrolled'                  \n",
    "             ] + champions_encoded\n",
    "\n",
    "X_test = pd.merge(X_test, team_positions, how='inner', on=['matchId', 'participantId'])\n",
    "X_test = pd.merge(X_test, champion_data, how='inner', left_on='championId', right_on='key')\n",
    "X_test = pd.merge(X_test, champion_types, how='inner', left_on='championId', right_on='key')\n",
    "X_test = pd.merge(X_test, test_last_frame_values, how='inner', on=['matchId', 'participantId'])\n",
    "X_test = pd.merge(X_test, champion_mastery, how='left', on=['summonerId', 'championId']).fillna(0)\n",
    "\n",
    "X_test = X_test.sort_values(['matchId', 'participantId'], ascending = [True, True]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "convert_team_values(X_test, variables)\n",
    "\n",
    "X_test_perlane = X_test.groupby(['matchId', 'teamPosition'])[['final_gold']].sum().pivot_table(values='final_gold', index='matchId', columns='teamPosition').reset_index().drop(columns=0)\n",
    "\n",
    "for lane in ['BOTTOM', 'JUNGLE', 'MIDDLE', 'TOP', 'UTILITY']:\n",
    "  X_test_perlane[f'{lane}'] = np.where(X_test_perlane[f'{lane}'] >= 0, 1, -1)\n",
    "\n",
    "X_test = (\n",
    "    X_test\n",
    "    .groupby(['matchId'])[variables]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "X_test = pd.merge(X_test, X_test_perlane, how='inner', on='matchId').reset_index(drop = True)\n",
    "X_test = pd.merge(X_test, testing_events, how='inner', on='matchId').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gbr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matchId</th>\n",
       "      <th>winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8000</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8001</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8002</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8003</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8004</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   matchId  winner\n",
       "0     8000     100\n",
       "1     8001     200\n",
       "2     8002     200\n",
       "3     8003     200\n",
       "4     8004     200"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['winner'] = y_pred\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best submission so far 7.08 on kaggle !\n",
    "#submission.to_csv('../data/submission_gradientboosting_n1000_learn001_2023_04_01.csv', index=False)\n",
    "\n",
    "# submission with this (all champs) is 7.06\n",
    "#submission.to_csv('../data/submission_gradientboosting_all_champions_n1000_learn001_2023_04_01.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding winrates and ban rates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was once competition was over but I was curious to see how much the predictions could have been imporved with this added information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_patch = pd.read_csv('../data/match_patch.csv')\n",
    "#champion_wr_stats = pd.read_csv('../data/champion_wr_stats.csv')\n",
    "champion_wr_stats = pd.read_csv('../data/champion_wr_stats_all.csv')\n",
    "X_train_original = pd.read_csv('../data/participants_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "champion_wr_stats['Name'] = champion_wr_stats['Name'].str.lower()\n",
    "\n",
    "# unfortunately can't match on 'teamPosition' and 'Role' due to some missing\n",
    "champion_wr_stats = champion_wr_stats.drop(columns=['Trend','Tier', 'Role']).groupby(['Name', 'Patch_Version']).max().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_original\n",
    "\n",
    "X_train['championName'] = X_train['championName'].str.lower()\n",
    "# additional name changes to match\n",
    "X_train['championName'] = X_train['championName'].str.replace('monkeyking', 'wukong')\n",
    "X_train['championName'] = X_train['championName'].str.replace('renata', 'renataglasc')\n",
    "X_train['championName'] = X_train['championName'].str.replace('drmundo', 'dr.mundo')\n",
    "\n",
    "variables = ['summonerLevel', 'championLevel','championPoints', \n",
    "            'Assassin', 'Fighter', 'Mage', 'Marksman', 'Support', 'Tank',\n",
    "            'info.attack', 'info.defense', 'info.magic', 'info.difficulty',\n",
    "            'stats.hpregenperlevel',\t'stats.mpregen', 'stats.mpregenperlevel',\t'stats.crit',\t'stats.critperlevel',\n",
    "            'stats.attackdamage', 'stats.attackdamageperlevel', 'stats.attackspeedperlevel',\t'stats.attackspeed',\n",
    "            'final_gold', 'final_xp', 'final_abilityhaste', 'final_abilitypower', 'final_armor', 'final_armorpen',\n",
    "            'final_armorpenpercent', 'final_atkdmg', 'final_bns_armorpenpercent', 'final_bns_magicpenpercent', 'final_ccreduction',\n",
    "            'final_cdreduction', 'final_remaining_health', 'final_health', 'final_healthrgn', 'final_lifesteal', 'final_mppen',\n",
    "            'final_mgpenpercent', 'final_mgres', 'final_ms', 'final_omnivamp', 'final_physicalvamp', 'final_power', 'final_powermax',\n",
    "            'final_powerregen', 'final_spellvamp', 'final_currentgold', 'final_magicdmgdone', 'final_magicdmgdonetochamps', 'final_magicdmgtaken',\n",
    "            'final_physdmgdone', 'final_physdmgdonetochamps', 'final_physdmgtaken', 'final_dmgdone', 'final_dmgdonetochamps', 'final_dmgtaken', \n",
    "            'final_truedmgdone', 'final_truedmgdonetochamps', 'final_truedmgtaken', 'final_goldpersec', 'final_jungleminionskilled', 'final_lvl',\n",
    "            'final_minionskilled', 'final_enemycontrolled',\n",
    "            'Score', 'Win %', 'Pick %', 'Ban %', 'KDA'                  \n",
    "             ]\n",
    "\n",
    "X_train = pd.merge(X_train, team_positions, how='inner', on=['matchId', 'participantId'])\n",
    "X_train = pd.merge(X_train, match_patch, how='inner', on='matchId')\n",
    "X_train = pd.merge(X_train, champion_wr_stats, how='inner', left_on=['championName', 'patch_version'], right_on=['Name', 'Patch_Version'])\n",
    "X_train = pd.merge(X_train, champion_data, how='inner', left_on='championId', right_on='key')\n",
    "X_train = pd.merge(X_train, champion_types, how='inner', left_on='championId', right_on='key')\n",
    "X_train = pd.merge(X_train, train_last_frame_values, how='inner', on=['matchId', 'participantId'])\n",
    "X_train = pd.merge(X_train, champion_mastery, how='left', on=['summonerId', 'championId']).fillna(0)\n",
    "\n",
    "X_train = X_train.sort_values(['matchId', 'participantId'], ascending = [True, True]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "convert_team_values(X_train, variables)\n",
    "\n",
    "X_train_perlane = X_train.groupby(['matchId', 'teamPosition'])[['final_gold']].sum().pivot_table(values='final_gold', index='matchId', columns='teamPosition').reset_index().drop(columns=0)\n",
    "\n",
    "for lane in ['BOTTOM', 'JUNGLE', 'MIDDLE', 'TOP', 'UTILITY']:\n",
    "  X_train_perlane[f'{lane}'] = np.where(X_train_perlane[f'{lane}'] >= 0, 1, -1)\n",
    "\n",
    "X_train = (\n",
    "    X_train\n",
    "    .groupby(['matchId'])[variables]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "X_train = pd.merge(X_train, X_train_perlane, how='inner', on='matchId').reset_index(drop = True)\n",
    "X_train = pd.merge(X_train, training_events, how='inner', on='matchId').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-14 {color: black;background-color: white;}#sk-container-id-14 pre{padding: 0;}#sk-container-id-14 div.sk-toggleable {background-color: white;}#sk-container-id-14 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-14 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-14 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-14 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-14 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-14 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-14 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-14 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-14 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-14 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-14 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-14 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-14 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-14 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-14 div.sk-item {position: relative;z-index: 1;}#sk-container-id-14 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-14 div.sk-item::before, #sk-container-id-14 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-14 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-14 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-14 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-14 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-14 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-14 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-14 div.sk-label-container {text-align: center;}#sk-container-id-14 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-14 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-14\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;gb&#x27;,\n",
       "                 GradientBoostingClassifier(learning_rate=0.01,\n",
       "                                            n_estimators=1000))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;gb&#x27;,\n",
       "                 GradientBoostingClassifier(learning_rate=0.01,\n",
       "                                            n_estimators=1000))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(learning_rate=0.01, n_estimators=1000)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('gb',\n",
       "                 GradientBoostingClassifier(learning_rate=0.01,\n",
       "                                            n_estimators=1000))])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr_wr = Pipeline(\n",
    "    steps = [\n",
    "        #('scaler', MinMaxScaler()),\n",
    "        ('gb', GradientBoostingClassifier(n_estimators = 1000, learning_rate=0.01))\n",
    "    ]\n",
    ")\n",
    "\n",
    "gbr_wr.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, attempting XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for xgboost need 1 and 0 instead of 100 and 200\n",
    "y_train_modified = y_train.copy()\n",
    "\n",
    "y_train_modified = y_train_modified.replace(100, 0)\n",
    "y_train_modified = y_train_modified.replace(200, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-25 {color: black;background-color: white;}#sk-container-id-25 pre{padding: 0;}#sk-container-id-25 div.sk-toggleable {background-color: white;}#sk-container-id-25 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-25 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-25 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-25 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-25 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-25 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-25 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-25 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-25 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-25 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-25 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-25 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-25 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-25 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-25 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-25 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-25 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-25 div.sk-item {position: relative;z-index: 1;}#sk-container-id-25 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-25 div.sk-item::before, #sk-container-id-25 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-25 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-25 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-25 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-25 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-25 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-25 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-25 div.sk-label-container {text-align: center;}#sk-container-id-25 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-25 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-25\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.001, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=1000, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" checked><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.001, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=1000, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.001, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=1000, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst = XGBClassifier(n_estimators=1000,\n",
    "                    max_depth=6,\n",
    "                    learning_rate=0.001,\n",
    "                    subsample=0.8,\n",
    "                    # reg_lambda=2,\n",
    "                    # reg_alpha=1,\n",
    "                    objective='binary:logistic')\n",
    "\n",
    "bst.fit(X_train, y_train_modified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.787125"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(\n",
    "    y_true = y_train_modified,\n",
    "    y_pred = bst.predict(X_train)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3232,  839],\n",
       "       [ 864, 3065]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train_modified, bst.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.79      0.79      4071\n",
      "           1       0.79      0.78      0.78      3929\n",
      "\n",
      "    accuracy                           0.79      8000\n",
      "   macro avg       0.79      0.79      0.79      8000\n",
      "weighted avg       0.79      0.79      0.79      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train_modified, bst.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importances = pd.DataFrame({\n",
    "#     'variable': bst.feature_names_in_,\n",
    "#     'importance': bst.feature_importances_\n",
    "# })\n",
    "\n",
    "# importances.sort_values('importance', ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_test = {\n",
    "# #  'max_depth':range(3,10,2),\n",
    "# #  'learning_rate': [0.05, 0.09, 0.1, 0.3]\n",
    "#    'n_estimators': [100, 300, 500, 700 ,1000]\n",
    "# }\n",
    "\n",
    "# gsearch = GridSearchCV(estimator = XGBClassifier(max_depth=9, learning_rate=0.1, objective= 'binary:logistic'),\n",
    "#                        param_grid = param_test,\n",
    "#                         scoring='roc_auc',\n",
    "#                         n_jobs=4, cv=5)\n",
    "# gsearch.fit(X_train,y_train_modified)\n",
    "\n",
    "# print( gsearch.best_params_, gsearch.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.763625"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(\n",
    "    y_true = y_train,\n",
    "    y_pred = gbr_wr.predict(X_train)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         100       0.76      0.78      0.77      4071\n",
      "         200       0.76      0.75      0.76      3929\n",
      "\n",
      "    accuracy                           0.76      8000\n",
      "   macro avg       0.76      0.76      0.76      8000\n",
      "weighted avg       0.76      0.76      0.76      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, gbr_wr.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>final_gold</td>\n",
       "      <td>0.592997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Win %</td>\n",
       "      <td>0.049038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>elite_monsters_killed</td>\n",
       "      <td>0.046754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>final_xp</td>\n",
       "      <td>0.046214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>championLevel</td>\n",
       "      <td>0.035903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>final_health</td>\n",
       "      <td>0.014880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>final_dmgdone</td>\n",
       "      <td>0.013568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>BOTTOM</td>\n",
       "      <td>0.010470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>summonerLevel</td>\n",
       "      <td>0.010141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>KDA</td>\n",
       "      <td>0.009496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>stats.attackdamageperlevel</td>\n",
       "      <td>0.008382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>stats.attackdamage</td>\n",
       "      <td>0.007678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>stats.mpregenperlevel</td>\n",
       "      <td>0.007561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>final_minionskilled</td>\n",
       "      <td>0.006509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Ban %</td>\n",
       "      <td>0.006504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>final_armor</td>\n",
       "      <td>0.006207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>final_remaining_health</td>\n",
       "      <td>0.006163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>matchId</td>\n",
       "      <td>0.005794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>final_dmgtaken</td>\n",
       "      <td>0.005664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>final_currentgold</td>\n",
       "      <td>0.005266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      variable  importance\n",
       "23                  final_gold    0.592997\n",
       "71                       Win %    0.049038\n",
       "83       elite_monsters_killed    0.046754\n",
       "24                    final_xp    0.046214\n",
       "2                championLevel    0.035903\n",
       "36                final_health    0.014880\n",
       "56               final_dmgdone    0.013568\n",
       "75                      BOTTOM    0.010470\n",
       "1                summonerLevel    0.010141\n",
       "74                         KDA    0.009496\n",
       "20  stats.attackdamageperlevel    0.008382\n",
       "19          stats.attackdamage    0.007678\n",
       "16       stats.mpregenperlevel    0.007561\n",
       "65         final_minionskilled    0.006509\n",
       "73                       Ban %    0.006504\n",
       "27                 final_armor    0.006207\n",
       "35      final_remaining_health    0.006163\n",
       "0                      matchId    0.005794\n",
       "58              final_dmgtaken    0.005664\n",
       "49           final_currentgold    0.005266"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances = pd.DataFrame({\n",
    "    'variable': gbr_wr.feature_names_in_,\n",
    "    'importance': gbr_wr['gb'].feature_importances_\n",
    "})\n",
    "\n",
    "importances.sort_values('importance', ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_original = pd.read_csv('../data/participants_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test_original\n",
    "\n",
    "X_test['championName'] = X_test['championName'].str.lower()\n",
    "# additional name changes to match\n",
    "X_test['championName'] = X_test['championName'].str.replace('monkeyking', 'wukong')\n",
    "X_test['championName'] = X_test['championName'].str.replace('renata', 'renataglasc')\n",
    "X_test['championName'] = X_test['championName'].str.replace('drmundo', 'dr.mundo')\n",
    "\n",
    "\n",
    "variables = ['summonerLevel', 'championLevel','championPoints', \n",
    "            'Assassin', 'Fighter', 'Mage', 'Marksman', 'Support', 'Tank',\n",
    "            'info.attack', 'info.defense', 'info.magic', 'info.difficulty',\n",
    "            'stats.hpregenperlevel',\t'stats.mpregen', 'stats.mpregenperlevel',\t'stats.crit',\t'stats.critperlevel',\n",
    "            'stats.attackdamage', 'stats.attackdamageperlevel', 'stats.attackspeedperlevel',\t'stats.attackspeed',\n",
    "            'final_gold', 'final_xp', 'final_abilityhaste', 'final_abilitypower', 'final_armor', 'final_armorpen',\n",
    "            'final_armorpenpercent', 'final_atkdmg', 'final_bns_armorpenpercent', 'final_bns_magicpenpercent', 'final_ccreduction',\n",
    "            'final_cdreduction', 'final_remaining_health', 'final_health', 'final_healthrgn', 'final_lifesteal', 'final_mppen',\n",
    "            'final_mgpenpercent', 'final_mgres', 'final_ms', 'final_omnivamp', 'final_physicalvamp', 'final_power', 'final_powermax',\n",
    "            'final_powerregen', 'final_spellvamp', 'final_currentgold', 'final_magicdmgdone', 'final_magicdmgdonetochamps', 'final_magicdmgtaken',\n",
    "            'final_physdmgdone', 'final_physdmgdonetochamps', 'final_physdmgtaken', 'final_dmgdone', 'final_dmgdonetochamps', 'final_dmgtaken', \n",
    "            'final_truedmgdone', 'final_truedmgdonetochamps', 'final_truedmgtaken', 'final_goldpersec', 'final_jungleminionskilled', 'final_lvl',\n",
    "            'final_minionskilled', 'final_enemycontrolled',\n",
    "            'Score', 'Win %', 'Pick %', 'Ban %', 'KDA'                     \n",
    "             ]\n",
    "\n",
    "X_test = pd.merge(X_test, team_positions, how='inner', on=['matchId', 'participantId'])\n",
    "X_test = pd.merge(X_test, match_patch, how='inner', on='matchId')\n",
    "X_test = pd.merge(X_test, champion_wr_stats, how='inner', left_on=['championName', 'patch_version'], right_on=['Name', 'Patch_Version'])\n",
    "X_test = pd.merge(X_test, champion_data, how='inner', left_on='championId', right_on='key')\n",
    "X_test = pd.merge(X_test, champion_types, how='inner', left_on='championId', right_on='key')\n",
    "X_test = pd.merge(X_test, test_last_frame_values, how='inner', on=['matchId', 'participantId'])\n",
    "X_test = pd.merge(X_test, champion_mastery, how='left', on=['summonerId', 'championId']).fillna(0)\n",
    "\n",
    "X_test = X_test.sort_values(['matchId', 'participantId'], ascending = [True, True]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "convert_team_values(X_test, variables)\n",
    "\n",
    "X_test_perlane = X_test.groupby(['matchId', 'teamPosition'])[['final_gold']].sum().pivot_table(values='final_gold', index='matchId', columns='teamPosition').reset_index().drop(columns=0)\n",
    "\n",
    "for lane in ['BOTTOM', 'JUNGLE', 'MIDDLE', 'TOP', 'UTILITY']:\n",
    "  X_test_perlane[f'{lane}'] = np.where(X_test_perlane[f'{lane}'] >= 0, 1, -1)\n",
    "\n",
    "X_test = (\n",
    "    X_test\n",
    "    .groupby(['matchId'])[variables]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "X_test = pd.merge(X_test, X_test_perlane, how='inner', on='matchId').reset_index(drop = True)\n",
    "X_test = pd.merge(X_test, testing_events, how='inner', on='matchId').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gbr_wr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_modified = bst.predict(X_test)\n",
    "\n",
    "y_pred_modified[y_pred_modified == 0] = 100\n",
    "y_pred_modified[y_pred_modified == 1] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matchId</th>\n",
       "      <th>winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8000</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8001</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8002</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8003</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8004</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   matchId  winner\n",
       "0     8000     100\n",
       "1     8001     200\n",
       "2     8002     200\n",
       "3     8003     200\n",
       "4     8004     200"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#submission['winner'] = y_pred\n",
    "submission['winner'] = y_pred_modified\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission.to_csv('../data/submission_gradientboosting_champwinrate_n1000_learn001_2023_04_15.csv', index=False)\n",
    "#submission.to_csv('../data/submission_gradientboosting_champwinrate_n1000_learn001_2023_04_30.csv', index=False)\n",
    "#submission.to_csv('../data/submission_gradientboosting_champ_all_winrate_n1000_learn001_2023_04_30.csv', index=False)\n",
    "#submission.to_csv('../data/submission_xgboost_n1000_mdepth10_learnrate_0001_2023_04_30.csv', index=False)\n",
    "#submission.to_csv('../data/submission_xgboost_n500_mdepth8_learnrate_00001_2023_04_30.csv', index=False)\n",
    "#submission.to_csv('../data/submission_xgboost_default_params_2023_04_30.csv', index=False)\n",
    "\n",
    "# this last prediction is the best one so far! 0.727 prediction on the private leaderboard; beats out the gradientboosting\n",
    "# submission.to_csv('../data/submission_xgboost_n1000_mdepth6_learnrate_0001_subsamp08_2023_04_30.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
